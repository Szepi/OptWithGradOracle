We thank the reviewers for their comments.

Reviewer 1:
1,3) Thanks, indeed, there are errors in Table 1, which we will correct, and we will also discuss the suggested reference.
2) While Theorem 1 focuses on choosing an optimal delta given p,q>0, Lemma 1 in the appendix actually gives a more general result, which can be applied even if any of $p,q,C_1,C_2$ are zero, and \delta can be chosen accordingly. Theorem 2 can be tweaked to hold for all q>0 by changing conditions (i) and (ii) after Theorem 2: namely, the right hand side of the conditions should be changed to \min(L,\frac{2z+1}{C_1^{2z-1}(z+1)^z}) where z=p/q. You are right that Theorem 2 does not hold for q=0 and we will modify the text appropriately to reflect this.


Reviewer 2:
We agree that a lot of work has been done to deal with the bias-variance tradeoff of gradient estimates, but--at the moment--they all lead to suboptimal error rates. This was the main motivation of our work: to clarify the fundamental limits achievable this way. Our definition of the gradient oracles aims at abstracting out the properties generally used about gradient estimators in the literature (and captures the properties of the numerous gradient estimation techniques used in the literature). Indeed, our upper bound is not particularly novel, by generalizing existing results to the framework we consider. Our main results are the lower bounds (and their relations to the upper bounds), which imply that the current gradient estimators and/or the proof techniques used to analyze the algorithms based on them cannot lead to optimal performance. This indicates that either some other properties of the gradient estimators have to be utilized or new gradient estimates are needed that exhibit a better bias-variance tradeoff, or completely different methods are necessary to achieve optimal rates. Concerning our specific construction in the lower bounds, we would like to point out that one can construct several other noise models than the proposed Gaussian one, for example, it is not hard to modify the argument to have bounded noise (scaled and shifted Bernoulli). However, irrespective of the actual noise of an estimator, the minimax nature of the lower bound shows that if a proof utilizes only the usual bias-variance bounds on the oracle, it cannot yield an optimal rate. Of course, our results would be significantly stronger if we could show that no better estimators are possible, that is, if the variance is bounded by delta^{-q}, the bias has to be at least delta^p, but so far we have not been able to show this (and it is known to be false for some special problems, see the response to Reviewer 3 below). Nevertheless, our results still correspond to the gradient estimates and upper bound proof techniques available in the literature (see also our analysis of the bias-variance tradeoff of different estimators in Appendix C).




Reviewer 3:
We stress again that the main novelty in our paper is the lower bounds, and the upper bounds are derived by generalizing existing results in the literature to our more general setup. To compare our work to Honorio's (2012), first notice that he considers a very specific problem (learning sparse Ising models), and also considers only upper bounds (using standard techniques). Furthermore, the special properties of his problem allow him to consider gradient estimates that do not show the bias-variance tradeoff we face (and which commonly appears in bandit convex optimization): Honorio optimizes some penalized log-likelihood function, where the hardest part is to estimate the gradient of the log-partition function. Since this is an integral, it is estimated through sampling, and he considers sampling methods that yield O(1/S) bias and variance, where S is the sample size used in the estimation (the bias term is defined somewhat differently than ours), that is, both the bias and the variance of the gradient estimator can be improved simultaneously at a fast rate. This is in sharp contrast to our assumptions which capture the phenomenon that reducing bias increases variance and vice versa. Of course, we could also use multiple samples to estimate the gradient, but averaging S samples from our oracle (at the same point) would not change the bias of our estimator, it would only reduce its variance. Thus, Honorio considers a different model with different assumptions on the gradient oracle, his results do not correspond to general bandit convex optimization, and he does not consider lower bounds (depending on his assumptions on the gradient oracle--these would involve the unknown constants B and V in his Definition 4, which could perhaps address the bias-variance tradeoff we consider). So, apart from high-level similarities, we do not see the connection of his paper to ours.
