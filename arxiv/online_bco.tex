%!TEX root =  bgo-arxiv.tex
% please do not delete or change the first line (needed by Csaba's editor)
In the \emph{online BCO} setting a learner sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the losses $f_1(X_1),\dots,f_n(X_n)$. More specifically, in round $t$, having observed\\ $f_1(X_1),\dots,f_{t-1}(X_{t-1})$ of the previous rounds, the learner chooses $X_t\in \cK$, after which it observed $f_t(X_t)$. The learner's goal is to minimize its expected regret $\EE{ \sum_{t=1}^n f_t(X_t) - \inf_{x\in \cK} \sum_{t=1}^n f_t(x) }$. 
This problem is also called online convex optimization with one-point feedback.
A slightly different problem is obtained if we allow the learner to choose multiple points in every round, at which points the function $f_t$ is observed. The loss is suffered at $X_t$. The points where the function is observed (``observation points'' for short) may or may not be tied to $X_t$. One possibility is that $X_t$ is one of the observation points.  
Another possibility is that $X_t$ is the average of the observation points (e.g., \citealt{AgDeXi10}). Yet another possibility is that there is no relationship between them. 

The oracle constructions from the previous section also apply to the online BCO setting, except that one cannot employ two-point feedback as the functions change in each round.  \todoc{Here we should actually introduce $Y$ etc.}
This rules out the controlled noise case, where one can employ two function valuations and cancel out the noise. Thus, for the online BCO setting, one should consider type-I (and II) oracles with $c_1(\delta) = C_1 \delta^p$ and $c_2(\delta) = C_2\delta^{-q}$ with $p=q=2$.
For these type of oracles, the results from Theorem \ref{thm:lb-convex} give the following result: 
\begin{theorem}\label{thm:aaa}
Let $\cF$ be the space of convex, $L$-smooth functions over a convex non-empty domain $\K$.
With a 
$(\delta^2,\delta^{-2})$ type-I oracle, no algorithm can achieve better regret than $\Omega(n^{2/3})$.
\end{theorem}

This optimality also implies that there is little hope to design gradient estimators with fundamentally better bias-variance tradeoff for this setting. Hence, by our earlier remark connecting regret and optimization error, we conclude that
 \textbf{
the currently used gradient estimators are not likely to lead to an algorithm that could  achieve the optimal
regret $O(n^{1/2})$} for online bandit convex optimization, established by \citet{BubeckDKP15,BuEl15}.

As an aside, we note that the algorithm from \cite{AgFoHsuKaRa13:SIAM} uses the ellipsoid method and achieves the  optimal
regret $O(n^{1/2})$. \todoc{Not for this setting! This paper should be mentioned in the previous section actually. For this setting, we  do not have \emph{any} algorithms that would achieve $O(n^{1/2})$.}
With a noisy gradient oracle, the next result shows that the regret is  $\O(n^{2/3})$ and this matches the lower bound in Theorem \ref{thm:aaa}.
\begin{theorem}
For zeroth order noisy optimization with smooth convex functions, the gradient estimators together with mirror descent (see Algorithm \ref{alg}) achieve $\O(n^{2/3})$ regret. \end{theorem}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bgo"
%%% End:
