%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
In the proof, we use \cref{lem:ub} to deal with noisy and biased gradient, which is essentially Theorem~C.4 of \cite{MahdaviPhd:2014}, and also identical to Theorem~6.3 of \cite{Bu:Convex14}, who cites \cite{Dekel:minibatch12} as the source. 
The lemma and its proof can be found below.

Proof of Theorem \ref{thm:ub}:

Since $f$ is convex, by Jensen's inequality and the bias condition in \cref{def:oracle2}, we get
\begin{align*}
 \E[f(\hat X_n) - \inf_{x \in \cK} f(x)] \le
 \dfrac{1}{n}\EE{ \sum_{t=1}^n f(X_t) - \inf_{x \in \cK}f(x) } 
 \le \dfrac{1}{n}\EE{ \sum_{t=1}^n \tilde{f}(X_t) - \inf_{x \in \cK}\tilde{f}(x) } +2C_1 \delta^p
 \,.
\end{align*}
Given $\overline{G}_t=\EE{G_t} = \nabla \tilde{f}(X_t)$, where $\tilde{f}\in \cF_{L,0}$ is convex and smooth,
the result immediately follows by applying \cref{lem:ub} to $\tilde{f}$. 
Substituting
%\todox{This may be confusing... since we are using type-II oracle, $\beta_t$ is actually 0.}
 $\beta_t = 0$, $\sigma^2_t = C_2 \delta^{-q}$, $a_t=a t^r$ for some $0<r<1$, respectively, we obtain
 \begin{align}
\E[f(\hat X_n) - \inf_{x \in \cK} f(x)]  
\le& \dfrac{1}{n}\left(\EE{f(X_1)-\inf_{x \in \cK}f(x)}+\dfrac{DL}{\alpha}  \right)\nonumber\\
&+\dfrac{Da}{\alpha} n^{r-1}+\dfrac{C_2 \delta^{-q}}{2a(1-r)}n^{-r}+ (2+\dfrac{2}{n})C_1\delta^p \,.
\label{eq:ubToBeOpt}
 \end{align}
 Choosing $r = \dfrac{p+q}{2p+q}$, and $a$, $\delta$ as stated in \cref{thm:ub}, the last $3$ terms in \eqref{eq:ubToBeOpt} are optimized to
 \[
 K_1 C_1^{q/(2p+q)} C_2^{p/(2p+q)} n ^{-p/(2p+q)} \,,
 \]
 where 
% $K_1 = 2^{\dfrac{3q}{2(p+q)}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{\dfrac{p+q^2+2pq}{(p+q)(2p+q)}}+2^{\dfrac{3q}{2(2p+q)}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{2-\dfrac{1}{2p+q}}$.
 $(K_1/3)^{2p+q} \le 2^{2q-p}\left(2+q/p \right)^p \left(D/\alpha\right)^{p}$.
 
 When $\tilde{f}\in \cF_{L,\mu, \cR}$ is $L$-smooth and $\mu$-strongly convex, similarly, we obtain
 \begin{align*}
 \MoveEqLeft
 \E[f(\hat X_n) - \inf_{x \in \cK} f(x)] -\dfrac{1}{n}\left(\EE{f(X_1)-\inf_{x \in \cK}f(x)}\right)\\
&\le (2+\dfrac{2}{n})C_1\delta^p+\dfrac{C_2 \delta^{-q}}{\alpha \mu n} \sum_{t=1}^{n-1}\dfrac{1}{t-\dfrac{2L}{\alpha \mu}}\\
&\le (2+\dfrac{2}{n})C_1\delta^p+\dfrac{C_2 }{\alpha \mu}\delta^{-q} \dfrac{\log n+1+\alpha \mu/(\alpha \mu-2L)}{n}\\
&\le K_2C_1^{\dfrac{q}{p+q}}C_2^{\dfrac{p}{p+q}} \left( \dfrac{\log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}}{n} \right)^{\dfrac{p}{p+q}}
 \end{align*}
The last step optimizes the bound via letting 
%$\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n}$, 
$\delta^{p+q} =  \dfrac{C_2\left( \log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)}$, 
where
%$K_2^{p+q}=\sqrt{2}^{2p+3q}D^{q/2}\alpha^{-p-q/2}\mu^{-p}$.
$K_2^{p+q}=2^{q}\alpha^{-p}\mu^{-p}$.


\begin{lemma}
\label{lem:ub}
Let $({\tilde{\cF}})_{t}$ be a filtration such that $X_t$ is ${\tilde{\cF}}_t$-measurable.
Let $\overline G_t = \EE{G_t|{\tilde{\cF}}_t}$ 
and assume that the nonnegative real-valued deterministic sequence $(\beta_t)_{1\le t\le n}$ is such that 
$\norm{\overline G_t - \nabla \tilde{f}(X_t)}_* \le \beta_t$ holds almost surely. 
Further, assume that $D=\sup_{x,y\in \cK} D_{\mathcal{R}}(x,y)$ and let $\eta_t = \frac{\alpha}{a_t+L}$ for some increasing 
sequence $(a_t)_{t=1}^{n-1}$ of numbers. Then, if $\tilde{f}$ is $L$-smooth,
\begin{align*}
\EE{ \sum_{t=1}^n \tilde{f}(X_t) - \tilde{f}(x) } 
\le 	 \EE{\tilde{f}(X_1)-\tilde{f}(x)}+ 
  \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t 
 +\frac{D(a_{n-1}+L)}{\alpha} +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,,
\end{align*}
where $\sigma_t^2 = \EE{ \norm{G_t-\overline G_t}_*^2}$ is the ``variance'' of $G_t$.

If ${\tilde{f}}$ is also $\mu$-strongly convex w.r.t. $\mathcal{R}$, which is $\alpha$-strongly convex, $\alpha > \dfrac{2L}{\mu}$. Let $\eta_t = \dfrac{2}{\mu t}$, $a_t = \dfrac{\alpha \mu}{2}t-L > 0$,
\begin{align*}
 \EE{ \sum_{t=1}^n \tilde{f}(X_t) - \tilde{f}(x) }  
\le 	 \EE{\tilde{f}(X_1)-\tilde{f}(x)}+ 
 \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t 
 +\sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,.
\end{align*}
\end{lemma}


To simplify notation, in the following we call the loss function $f$, not $\tilde{f}$ as stated in \cref{lem:ub}. Before the proof, we introduce a well known bound on the instantaneous linearized regret of Mirror Descent. 
\begin{lemma}
\label{lem:mdlinregret}
For any $x \in \cK$ and any $t \ge 1$,
\begin{align*}
\MoveEqLeft
\ip{G_t, X_{t+1}-x} 
\le \dfrac{1}{\eta_t} \left( \DR(x,X_t)-\DR(x,X_{t+1})-\DR(X_{t+1},X_t) \right)\,,
\end{align*}
where $X_{t+1}$ is selected as in \cref{alg}.
\end{lemma}
\begin{proof}
The point $X_{t+1}$ is the minimizer of 
$\Psi_{t+1}(x)=\eta_t \ip{G_t,x}+\DR(x,X_t)$ over $\cK$. The gradient of $\Psi_{t+1}(x)$ is
\[
\nabla \Psi_{t+1}(x) = \eta_t G_t + \nabla\mathcal{R}(x)-\nabla\mathcal{R}(X_t)\,.
\]
By the optimality condition, for any $x \in \cK$,
\[
\ip{\eta_t G_t + \nabla\mathcal{R}(x)-\nabla\mathcal{R}(X_t), x-X_{t+1}}\ge 0 \,,
\]
which is equivalent to the result by substituting the definition of Bregman divergence $\DR$.
\end{proof}

Proof of Lemma \ref{lem:ub}:

Denote $\hat{G}_t = \nabla f(X_t)$. 
From the smoothness and convexity of $f$ and using that $\mathcal{R}$ is strongly convex, we have
\begin{align}
\MoveEqLeft
f(X_{t+1}) - f(x) \nonumber\\
 \le& f(X_t) + \ip{ \hat{G}_t, X_{t+1}-X_t } + \frac{L}{2} \norm{X_{t+1}-X_t}^2 - \left\{ f(X_t) + \ip{ \hat{G}_t, x-X_t} \right\} \nonumber \\
 =& \ip{\hat{G}_t, X_{t+1} - x } +  \frac{L}{2} \norm{X_{t+1}-X_t}^2 \nonumber\\
 \le& \ip{\hat{G}_t,X_{t+1}-x} + \frac{L}{\alpha} D_{\mathcal{R}}(X_{t+1},X_t)\,. \label{eq:mdsmoothnoisy1}
\end{align}
Now write $\hat{G}_t = (\hat{G}_t-\overline G_t)  + \xi_t + G_t$ where $\xi_t = \overline G_t - G_t$ is the ``noise''.
Hence, 
\begin{align*}
\MoveEqLeft
\ip{\hat{G}_t,X_{t+1}-x} \\
\le& \beta_t \norm{X_{t+1}-x} + \ip{\xi_t,X_{t+1}-x} + \ip{ G_t, X_{t+1} - x }\\
\le &\beta_t \sqrt{ \frac{2D}{\alpha} } + \ip{\xi_t,X_{t+1}-x} + \ip{ G_t, X_{t+1} - x } \,.
\end{align*}
After we plug this into~\eqref{eq:mdsmoothnoisy1},
the plan is to take the conditional expectation of both sides w.r.t. $\F_{t}$.
As $X_t$ is $\F_{t}$-measurable, and by the definition of $\xi_t$ and $\overline G_t$, $\EE{ \xi_t|\F_{t}} = 0$, 
we have 
\begin{align*}
\MoveEqLeft
\EE{\ip{\xi_t, X_{t+1}-x}|\F_{t}} = \underbrace{\EE{\ip{\xi_t,X_{t}-x}|\F_{t}}}_{=0} + \EE{\ip{\xi_t,X_{t+1}-X_t}|\F_{t}} \,.
\end{align*}
We bound the second term inside the expectation from the Fenchel-Young inequality: 
\begin{align*}
\ip{\xi_t,X_{t+1}-X_t} \le \frac12 \left(\frac{\norm{\xi_t}_*^2}{a_t} + a_t \norm{X_{t+1}-X_t}^2\right)\,.
\end{align*}
Using again that $\mathcal{R}$ is $\alpha$-strongly convex, we get 
\[
\ip{\xi_t,X_{t+1}-X_t} \le \frac12 \left(\frac{\norm{\xi_t}_*^2}{a_t} + \frac{2a_t}{\alpha} D_{\mathcal{R}}(X_{t+1},X_t) \right)\,.
\]
Applying 
\cref{lem:mdlinregret} 
to bound $\ip{ G_t, X_{t+1} - x }$, and putting everything together gives
\begin{align*}
 \EE{f(X_{t+1}) - f(x) |\F_{t} }
\le & \quad
 \beta_t \sqrt{ \frac{2D}{\alpha} }+
\frac{1}{2a_t}  \EE{\norm{\xi_t}_*^2|\F_{t}}+
\frac{1}{\eta_t} \left(D_{\mathcal{R}}(x,X_t)-D_{\mathcal{R}}(x,X_{t+1})\right) \\&+
\underbrace{\left(\frac{a_t+L}{\alpha}-\frac{1}{\eta_t}\right) D_{\mathcal{R}}(X_{t+1},X_t)}_{=0}\numberthis \label{eq:ubProofWithNoise} \,.
\end{align*}
Given $\sigma_t^2 = \EE{ \norm{\xi_t}_*^2}$,
we will sum up these inequalities for $t=1,\dots,n-1$.
To bound the right-hand side of the result, we use
\begin{align*}
&\sum_{t=1}^{n-1} \frac{1}{\eta_t} \left(\DR(x,X_t)-\DR(x,X_{t+1})\right)
 \\
=& \DR(x,X_1) \frac{1}{\eta_1} + \DR(x,X_2) \left(\frac{1}{\eta_2}-\frac{1}{\eta_1}\right)
+ \ldots+\DR(x,X_{n-1}) \left(\frac{1}{\eta_{n-1}} -\frac{1}{\eta_{n-2}}\right)- \frac{1}{\eta_{n-1}} \DR(x,X_n)\\
 \le& D \frac{1}{\eta_1} + D \sum_{t=2}^{n-1} \left(\frac1{\eta_{t}}-\frac1{\eta_{t-1}}\right)\\
 = &\frac{D}{\eta_{n-1}}\,.
\end{align*}
The inequality results from the fact that $\{\eta_t\}$ is non-increasing.
Hence, by the tower rule,
\begin{align*}
 &\EE{ \sum_{t=1}^n f(X_t) - f(x) }  \\
\le& 
  \EE{f(X_1)-f(x)} + \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t +
	   \frac{D}{\eta_{n-1}} +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\\
=& 	  
  \EE{f(X_1)-f(x)} + \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t +
	   \frac{D(a_{n-1}+L)}{\alpha} +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,.
\end{align*}

When $f$ is $L$-smooth and $\mu$-strongly convex,  we can rewrite \eqref{eq:mdsmoothnoisy1} as
\begin{align*}
\MoveEqLeft
f(X_{t+1}) - f(x) \nonumber\\
 \le& f(X_t) + \ip{ \hat{G}_t, X_{t+1}-X_t } + \frac{L}{2} \norm{X_{t+1}-X_t}^2 - \left\{ f(X_t) + \ip{ \hat{G}_t, x-X_t}+\dfrac{\mu}{2}\DR(x, X_t) \right\} \nonumber \\
 =& \ip{\hat{G}_t, X_{t+1} - x } +  \frac{L}{2} \norm{X_{t+1}-X_t}^2-\dfrac{\mu}{2}\DR(x, X_t) \nonumber\\
 \le& \ip{\hat{G}_t,X_{t+1}-x} + \frac{L}{\alpha} D_{\mathcal{R}}(X_{t+1},X_t)-\dfrac{\mu}{2}\DR(x, X_t)\,.
\end{align*}
Again, \eqref{eq:ubProofWithNoise} can be written as
\begin{align*}
 \EE{f(X_{t+1}) - f(x) |\F_{t} }
\le & \quad
 \beta_t \sqrt{ \frac{2D}{\alpha} }+
\frac{1}{2a_t}  \EE{\norm{\xi_t}_*^2|\F_{t}}\\
&+\left(\dfrac{1}{\eta_t}-\dfrac{\mu}{2}  \right)\DR(x,X_t)-\dfrac{1}{\eta_t}\DR(x,X_{t+1})
+\left( \dfrac{L+a_t}{\alpha}-\dfrac{1}{\eta_t} \right)\DR(X_{t+1},X_t) \,.
\end{align*}
Given $\dfrac{1}{\eta_t}=\dfrac{\mu t}{2}=\dfrac{L+a_t}{\alpha}$, summing up theses inequalities for $t=1,2,\cdots, n-1$, we get
\begin{align*}
 \EE{ \sum_{t=1}^n f(X_t) - f(x) }  
\le& 
  \EE{f(X_1)-f(x)} + \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}-\dfrac{1}{\eta_{n-1}}\DR(x,X_{n})\\
\le& 	  
  \EE{f(X_1)-f(x)} + \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,.
\end{align*}
Proof is done.