%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Csaba at 2015-10-09 17:32:51 -0600


%% Saved with string encoding Unicode (UTF-8)

@article{liang2014zeroth,
  title={On Zeroth-Order Stochastic Convex Optimization via Random Walks},
  author={Liang, Tengyuan and Narayanan, Hariharan and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:1402.2667},
  year={2014}
}


@inproceedings{BubeckDKP15,
	Author = {Bubeck, S{\'{e}}bastien and Dekel, Ofer and Koren, Tomer and Peres, Yuval},
	Booktitle = {COLT},
	Date-Added = {2015-10-09 23:30:42 +0000},
	Date-Modified = {2015-10-09 23:32:49 +0000},
	Pages = {266--278},
	Title = {Bandit Convex Optimization: {\textbackslash}({\textbackslash}sqrt\{T\}{\textbackslash}) Regret in One Dimension},
	Year = {2015},
	Bdsk-Url-1 = {http://jmlr.org/proceedings/papers/v40/Bubeck15a.html}}

@article{atchade2014stochastic,
	Author = {Atchade, Yves F and Fort, Gersende and Moulines, Eric},
	Date-Added = {2015-10-09 16:20:22 +0000},
	Date-Modified = {2015-10-09 16:20:22 +0000},
	Journal = {arXiv preprint arXiv:1402.2365},
	Title = {On stochastic proximal gradient algorithms},
	Year = {2014}}

@book{NeYu83,
	Author = {Nemirovskii, Arkadii and ︡Yudin, David Borisovich},
	Date-Added = {2015-10-09 16:08:39 +0000},
	Date-Modified = {2015-10-09 16:09:16 +0000},
	Publisher = {Wiley},
	Series = {Wiley-Interscience series in discrete mathematics},
	Title = {Problem complexity and method efficiency in optimization},
	Year = 1983,
	Bdsk-Url-1 = {http://opac.inria.fr/record=b1091338}}

@book{demyanov1970approximate,
	Author = {Demyanov, Vladimir Fedorovich and Rubinov, Aleksandr Moiseevich},
	Date-Added = {2015-10-09 16:01:25 +0000},
	Date-Modified = {2015-10-09 16:01:25 +0000},
	Publisher = {Elsevier Publishing Company},
	Title = {Approximate methods in optimization problems},
	Volume = {32},
	Year = {1970}}

@book{glasserman1991gradient,
	Author = {Glasserman, Paul},
	Date-Added = {2015-10-09 15:59:10 +0000},
	Date-Modified = {2015-10-09 15:59:19 +0000},
	Publisher = {Springer},
	Title = {Gradient estimation via perturbation analysis},
	Year = {1991}}

@article{baes2013randomized,
	Author = {Baes, Michel and B{\"u}rgisser, Michael and Nemirovski, Arkadi},
	Date-Added = {2015-10-09 15:56:25 +0000},
	Date-Modified = {2015-10-09 15:56:33 +0000},
	Journal = {SIAM Journal on Optimization},
	Number = {2},
	Pages = {934--962},
	Title = {A randomized mirror-prox method for solving structured large-scale matrix saddle-point problems},
	Volume = {23},
	Year = {2013}}

@inproceedings{DvoGa15,
	Author = {Dvurechensky, Pavel and Gasnikov, Alexander},
	Booktitle = {ITaS 2015},
	Date-Added = {2015-10-09 14:49:50 +0000},
	Date-Modified = {2015-10-09 14:51:02 +0000},
	Title = {Stochastic Intermediate Gradient Method: Convex and Strongly Convex Cases},
	Url = {http://itas2015.iitp.ru/pdf/1570152405.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://itas2015.iitp.ru/pdf/1570152405.pdf}}

@inproceedings{SchRoBa11,
	Author = {Schmidt, Mark W. and Roux, Nicolas Le and Bach, Francis R.},
	Booktitle = {NIPS 24},
	Date-Added = {2015-10-09 06:50:27 +0000},
	Date-Modified = {2015-10-09 07:09:09 +0000},
	Pages = {1458--1466},
	Title = {Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization},
	Year = {2011},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/4452-convergence-rates-of-inexact-proximal-gradient-methods-for-convex-optimization}}

@article{dAsp08,
	Author = {d'Aspremont, Alexandre},
	Date-Added = {2015-10-09 06:33:22 +0000},
	Date-Modified = {2015-10-09 06:33:39 +0000},
	Journal = {SIAM Journal on Optimization},
	Pages = {1171--1183},
	Title = {Smooth Optimization with Approximate Gradient},
	Volume = {19},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1137/060676386}}

@techreport{Baes09,
	Address = {Switzerland},
	Author = {Baes, M.},
	Date-Added = {2015-10-09 06:28:32 +0000},
	Date-Modified = {2015-10-09 15:56:21 +0000},
	Institution = {IFOR Internal report, ETH Zurich},
	Title = {Estimate Sequence Methods: Extensions and Approximations},
	Year = {2009}}

@article{DeGliNe14,
	Author = {Devolder, Olivier and Glineur, Francois and Nesterov, Yurii},
	Date-Added = {2015-10-09 06:06:22 +0000},
	Date-Modified = {2015-10-09 06:06:22 +0000},
	Journal = {Mathematical Programming},
	Pages = {37--75},
	Title = {First-order methods of smooth convex optimization with inexact oracle},
	Volume = {146},
	Year = {2014}}

@incollection{HaLe14:SOC,
	Author = {Hazan, E. and Levy, K.},
	Booktitle = {NIPS},
	Date-Added = {2015-10-08 04:31:48 +0000},
	Date-Modified = {2015-10-08 04:32:22 +0000},
	Pages = {784--792},
	Title = {Bandit Convex Optimization: Towards Tight Bounds},
	Url = {http://papers.nips.cc/paper/5377-bandit-convex-optimization-towards-tight-bounds.pdf},
	Year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5377-bandit-convex-optimization-towards-tight-bounds.pdf}}

@book{bhatnagar-book,
	Author = {Bhatnagar, S and Prasad, H. L. and Prashanth, L. A.},
	Publisher = {Springer},
	Title = {Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods (Lecture Notes in Control and Information Sciences)},
	Volume = {434},
	Year = {2013}}

@book{kushcla,
	Address = {New York},
	Author = {Kushner, H. J. and Clark, D. S.},
	Publisher = {Springer Verlag},
	Title = {Stochastic Approximation Methods for Constrained and Unconstrained Systems},
	Year = 1978}

@article{katkul,
	Author = {Katkovnik, V. Ya and Kulchitsky, Yu},
	Journal = {Automation Remote Control},
	Pages = {1321--1326},
	Title = {Convergence of a class of random search algorithms},
	Volume = {8},
	Year = 1972}

@inproceedings{hazan2014bandit,
	Author = {Hazan, Elad and Levy, Kfir},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {784--792},
	Title = {Bandit convex optimization: Towards tight bounds},
	Year = {2014}}

@inproceedings{shamir2012complexity,
	Author = {Shamir, Ohad},
	Booktitle = {Proceedings of the Twenty Sixth Annual Conference on Computational Learning Theory},
	Title = {On the complexity of bandit and derivative-free stochastic convex optimization},
	Year = {2012}}

@article{raginsky2011information,
	Author = {Raginsky, Maxim and Rakhlin, Alexander},
	Journal = {IEEE Transactions on Information Theory},
	Number = {10},
	Pages = {7036--7056},
	Publisher = {IEEE},
	Title = {Information-based complexity, feedback and dynamics in convex programming},
	Volume = {57},
	Year = {2011}}

@article{nemirovski2009robust,
	Author = {Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
	Journal = {SIAM Journal on Optimization},
	Number = {4},
	Pages = {1574--1609},
	Publisher = {SIAM},
	Title = {Robust stochastic approximation approach to stochastic programming},
	Volume = {19},
	Year = {2009}}

@incollection{GhaLan15,
	Author = {Ghadimi, Saeed and Lan, Guanghui},
	Booktitle = {Handbook of Simulation Optimization},
	Date-Added = {2015-08-24 01:02:32 +0000},
	Date-Modified = {2015-08-24 01:02:35 +0000},
	Doi = {10.1007/978-1-4939-1384-8_7},
	Editor = {Fu, Michael C},
	Pages = {179--206},
	Publisher = {Springer New York},
	Series = {International Series in Operations Research \& Management Science},
	Title = {Stochastic Approximation Methods and Their Finite-Time Convergence Properties},
	Url = {http://dx.doi.org/10.1007/978-1-4939-1384-8_7},
	Volume = {216},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-1-4939-1384-8_7}}

@unpublished{BachPer15,
	Author = {Bach, Francis and Perchet, Vianney},
	Date-Added = {2015-08-24 00:24:55 +0000},
	Date-Modified = {2015-08-24 00:25:44 +0000},
	Note = {submitted},
	Title = {Highly-Smooth Zero-th Order Online Optimization},
	Year = {2015}}

@article{Chen88:LB-AoS,
	Abstract = {The problem is considered of estimating the point of global maximum of a function f belonging to a class F of functions on [ -1, 1 ], based on estimates of function values at points selected possibly during the experimentation. If p is odd and greater than 1, K is a positive constant and F contains enough functions with pth derivatives bounded by K, then we prove that, under additional weak regularity conditions, the lower rate of convergence is n-(p - 1)/(2p).},
	Author = {Chen, Hung},
	Date-Added = {2015-08-23 18:33:13 +0000},
	Date-Modified = {2015-08-23 18:33:24 +0000},
	Journal = {The Annals of Statistics},
	Number = {3},
	Pages = {1330--1334},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Lower Rate of Convergence for Locating a Maximum of a Function},
	Volume = {16},
	Year = {1988}}

@article{PoTsy90,
	Author = {Polyak, B.T. and Tsybakov, A.B.},
	Date-Added = {2015-08-23 18:21:06 +0000},
	Date-Modified = {2015-08-23 18:21:06 +0000},
	Journal = {Problems in Information Transmission},
	Pages = {126--133},
	Title = {Optimal orders of accuracy for search algorithms of stochastic optimization},
	Year = {1990}}

@article{Dip03:AoS,
	Abstract = {We propose a general class of randomized gradient estimates to be employed in a recursive search for the minimum of an unknown multivariate regression function. Here only two observations per iteration step are used. Special cases include random direction stochastic approximation (Kushner and Clark), simultaneous perturbation stochastic approximation (Spall) and a special kernel based stochastic approximation method (Polyak and Tsybakov). If the unknown regression is p-smooth (p ≥ 2) at the point of minimum, these methods achieve the optimal rate of convergence O(n-(p-1)/(2p)). For both the classical stochastic approximation scheme (Kiefer and Wolfowitz) and the averaging scheme (Ruppert and Polyak) the related asymptotic distributions are computed.},
	Author = {Dippon, J\"urgen},
	Date-Added = {2015-08-23 18:17:55 +0000},
	Date-Modified = {2015-08-23 18:18:06 +0000},
	Journal = {The Annals of Statistics},
	Number = {4},
	Pages = {1260--1281},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Accelerated Randomized Stochastic Optimization},
	Volume = {31},
	Year = {2003}}

@article{DuBaWa12,
	Author = {Duchi, John C. and Bartlett, Peter L. and Wainwright, Martin J.},
	Date-Added = {2015-08-23 16:22:12 +0000},
	Date-Modified = {2015-08-23 16:22:20 +0000},
	Doi = {10.1137/110831659},
	Eprint = {http://dx.doi.org/10.1137/110831659},
	Journal = {SIAM Journal on Optimization},
	Number = {2},
	Pages = {674-701},
	Title = {Randomized Smoothing for Stochastic Optimization},
	Url = {http://dx.doi.org/10.1137/110831659},
	Volume = {22},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1137/110831659}}

@article{LiNaRa14,
	Author = {Liang, Tengyuan and Narayanan, Hariharan and Rakhlin, Alexander},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/LiangNR14},
	Date-Added = {2015-08-23 04:44:57 +0000},
	Date-Modified = {2015-08-23 04:45:04 +0000},
	Journal = {CoRR},
	Timestamp = {Wed, 05 Mar 2014 14:43:44 +0100},
	Title = {On Zeroth-Order Stochastic Convex Optimization via Random Walks},
	Url = {http://arxiv.org/abs/1402.2667},
	Volume = {abs/1402.2667},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1402.2667}}

@techreport{Bu:Convex14,
	Abstract = {We construct a new map from a convex function to a distribution on its domain, with the
property that this distribution is a multi-scale exploration of the function.  We use this map to
solve a decade-old open problem in adversarial bandit convex optimization by showing that
the minimax regret for this problem is
e
O
(poly(
n
)
p
T
)
, where
n
is the dimension and
T
the
number of rounds.  This bound is obtained by studying the dual Bayesian maximin regret via
the information ratio analysis of Russo and Van Roy, and then using the multi-scale exploration
to solve the Bayesian problem.},
	Author = {Bubeck, Sebastien},
	Date-Added = {2015-08-23 03:29:39 +0000},
	Date-Modified = {2015-08-23 03:30:12 +0000},
	Institution = {Microsoft Research},
	Title = {Theory of convex optimization for machine learning},
	Year = {2014},
	Bdsk-Url-1 = {http://EconPapers.repec.org/RePEc:cor:louvco:2011001}}

@article{Dekel:minibatch12,
	Acmid = {2188391},
	Author = {Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
	Date-Added = {2015-08-23 03:28:48 +0000},
	Date-Modified = {2015-08-23 03:29:08 +0000},
	Issn = {1532-4435},
	Issue_Date = {January 2012},
	Journal = {J. Mach. Learn. Res.},
	Keywords = {convex optimization, distributed computing, online learning, regret bounds, stochastic optimization},
	Month = jan,
	Number = {1},
	Numpages = {38},
	Pages = {165--202},
	Publisher = {JMLR.org},
	Title = {Optimal Distributed Online Prediction Using Mini-batches},
	Url = {http://dl.acm.org/citation.cfm?id=2503308.2188391},
	Volume = {13},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2503308.2188391}}

@techreport{BuEl15,
	Abstract = {We construct a new map from a convex function to a distribution on its domain, with the
property that this distribution is a multi-scale exploration of the function.  We use this map to
solve a decade-old open problem in adversarial bandit convex optimization by showing that
the minimax regret for this problem is
e
O
(poly(
n
)
p
T
)
, where
n
is the dimension and
T
the
number of rounds.  This bound is obtained by studying the dual Bayesian maximin regret via
the information ratio analysis of Russo and Van Roy, and then using the multi-scale exploration
to solve the Bayesian problem.},
	Author = {Bubeck, Sebastien and Eldan, Ronen},
	Date-Added = {2015-08-23 03:11:32 +0000},
	Date-Modified = {2015-08-23 03:12:53 +0000},
	Institution = {Microsoft Research},
	Title = {Multi-scale exploration of convex functions and bandit convex optimization},
	Year = {2015},
	Bdsk-Url-1 = {http://EconPapers.repec.org/RePEc:cor:louvco:2011001}}

@inproceedings{Stich14,
	Author = {Stich, Sebastian Urban},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/conf/ppsn/Stich14},
	Booktitle = {Proceedings Parallel Problem Solving from Nature - {PPSN} {XIII}},
	Date-Added = {2015-08-23 03:09:17 +0000},
	Date-Modified = {2015-08-23 03:09:50 +0000},
	Doi = {10.1007/978-3-319-10762-2_13},
	Pages = {130--140},
	Timestamp = {Sun, 14 Sep 2014 14:57:45 +0200},
	Title = {On Low Complexity Acceleration Techniques for Randomized Optimization},
	Url = {http://dx.doi.org/10.1007/978-3-319-10762-2_13},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-319-10762-2_13}}

@article{ChenWild15,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150703332C},
	Archiveprefix = {arXiv},
	Author = {Chen, R. and Wild, S.},
	Date-Added = {2015-08-23 03:05:41 +0000},
	Date-Modified = {2015-08-23 03:06:04 +0000},
	Eprint = {1507.03332},
	Journal = {ArXiv e-prints},
	Keywords = {Mathematics - Optimization and Control},
	Month = jul,
	Primaryclass = {math.OC},
	Title = {Randomized Derivative-Free Optimization of Noisy Convex Functions},
	Year = 2015}

@article{duchi2015optimal,
	Author = {Duchi, John C and Jordan, Michael and Wainwright, Martin J and Wibisono, Andre},
	Date-Added = {2015-08-23 02:39:30 +0000},
	Date-Modified = {2015-08-23 02:39:42 +0000},
	Journal = {IEEE Transactions on Information Theory},
	Number = {5},
	Pages = {2788--2806},
	Publisher = {IEEE},
	Title = {Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations},
	Volume = {61},
	Year = {2015}}

@inproceedings{wibisono2012finite,
	Author = {Wibisono, Andre and Wainwright, Martin J and Jordan, Michael I and Duchi, John C},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2015-08-23 02:36:22 +0000},
	Date-Modified = {2015-08-23 02:36:22 +0000},
	Pages = {1439--1447},
	Title = {Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods},
	Year = {2012}}

@article{duchi2013optimal,
	Author = {Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
	Date-Added = {2015-08-23 02:33:38 +0000},
	Date-Modified = {2015-08-23 02:33:38 +0000},
	Journal = {arXiv preprint arXiv:1312.2139},
	Title = {Optimal rates for zero-order convex optimization: the power of two function evaluations},
	Year = {2013}}

@article{stich2013optimization,
	Author = {Stich, Sebastian U and Muller, CL and Gartner, B},
	Date-Added = {2015-08-23 02:33:01 +0000},
	Date-Modified = {2015-08-23 02:33:01 +0000},
	Journal = {SIAM Journal on Optimization},
	Number = {2},
	Pages = {1284--1309},
	Publisher = {SIAM},
	Title = {Optimization of convex functions with Random Pursuit},
	Volume = {23},
	Year = {2013}}

@article{bubeck2012regret,
	Author = {Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo},
	Date-Added = {2015-08-23 02:32:04 +0000},
	Date-Modified = {2015-08-23 02:32:04 +0000},
	Journal = {arXiv preprint arXiv:1204.5721},
	Title = {Regret analysis of stochastic and nonstochastic multi-armed bandit problems},
	Year = {2012}}

@article{garmanjani2012smoothing,
	Author = {Garmanjani, R and Vicente, LN},
	Date-Added = {2015-08-23 02:30:04 +0000},
	Date-Modified = {2015-08-23 02:30:04 +0000},
	Journal = {IMA Journal of Numerical Analysis},
	Pages = {drs027},
	Publisher = {Oxford University Press},
	Title = {Smoothing and worst-case complexity for direct-search methods in nonsmooth optimization},
	Year = {2012}}

@inproceedings{jamieson2012query,
	Author = {Jamieson, Kevin G and Nowak, Robert and Recht, Ben},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2015-08-23 02:29:14 +0000},
	Date-Modified = {2015-08-23 02:29:14 +0000},
	Pages = {2672--2680},
	Title = {Query complexity of derivative-free optimization},
	Year = {2012}}

@techreport{Ne11:TR,
	Abstract = {In this paper, we prove the complexity bounds for methods of Convex Optimization based only on computation of the function value. The search directions of our schemes are normally distributed random Gaussian vectors. It appears that such methods usually need at most n times more iterations than the standard gradient methods, where n is the dimension of the space of variables. This conclusion is true both for nonsmooth and smooth problems. For the later class, we present also an accelerated scheme with the expected rate of convergence O(n[ exp ]2 /k[ exp ]2), where k is the iteration counter. For Stochastic Optimization, we propose a zero-order scheme and justify its expected rate of convergence O(n/k[ exp ]1/2). We give also some bounds for the rate of convergence of the random gradient-free methods to stationary points of nonconvex functions, both for smooth and nonsmooth cases. Our theoretical results are supported by preliminary computational experiments.},
	Author = {Nesterov, Yurii},
	Date-Added = {2015-08-23 01:58:00 +0000},
	Date-Modified = {2015-08-23 01:58:10 +0000},
	Institution = {Universit{\'e} catholique de Louvain, Center for Operations Research and Econometrics (CORE)},
	Keywords = {convex optimization; stochastic optimization; derivative-free methods; random methods; complexity bounds},
	Number = {2011001},
	Title = {Random gradient-free minimization of convex functions},
	Type = {CORE Discussion Papers},
	Url = {http://EconPapers.repec.org/RePEc:cor:louvco:2011001},
	Year = {2011},
	Bdsk-Url-1 = {http://EconPapers.repec.org/RePEc:cor:louvco:2011001}}

@article{AgFoHsuKaRa13:SIAM,
	Author = {Agarwal, Alekh and Foster, Dean P. and Hsu, Daniel and Kakade, Sham M. and Rakhlin, Alexander},
	Date-Added = {2015-08-23 01:24:52 +0000},
	Date-Modified = {2015-08-23 01:25:08 +0000},
	Doi = {10.1137/110850827},
	Eprint = {http://dx.doi.org/10.1137/110850827},
	Journal = {SIAM Journal on Optimization},
	Number = {1},
	Pages = {213--240},
	Title = {Stochastic Convex Optimization with Bandit Feedback},
	Url = {http://dx.doi.org/10.1137/110850827},
	Volume = {23},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1137/110850827}}

@phdthesis{MahdaviPhd:2014,
	Author = {Mahdavi, Mehrdad},
	Date-Added = {2015-08-22 02:46:20 +0000},
	Date-Modified = {2015-08-22 02:46:59 +0000},
	School = {Michigan State University},
	Title = {Exploiting Smoothness in Statistical Learning, Sequential Prediction, and Stochastic Optimization},
	Year = {2014}}

@book{Nest04,
	Author = {Nesterov, Y.},
	Date-Added = {2015-08-21 22:29:05 +0000},
	Date-Modified = {2015-08-21 22:29:05 +0000},
	Publisher = {Kluwer Academic Publishers},
	Title = {Introductory Lectures on Convex Optimization},
	Year = {2004}}

@inproceedings{AgDeXi10,
	Author = {Agarwal, Alekh and Dekel, Ofer and Xiao, Lin},
	Booktitle = {COLT},
	Date-Added = {2015-05-26 03:46:20 +0000},
	Date-Modified = {2015-05-26 03:48:36 +0000},
	Pages = {28--40},
	Title = {Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback},
	Year = {2010},
	Bdsk-Url-1 = {http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=36}}

@inproceedings{AbHaRa08,
	Author = {Abernethy, Jacob and Hazan, Elad and Rakhlin, Alexander},
	Booktitle = {COLT},
	Date-Added = {2015-05-26 03:44:35 +0000},
	Date-Modified = {2015-05-26 03:45:24 +0000},
	Pages = {263--274},
	Title = {Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization},
	Year = {2008},
	Bdsk-Url-1 = {http://colt2008.cs.helsinki.fi/papers/127-Abernethy.pdf}}

@inproceedings{flaxman2005online,
	Author = {Flaxman, Abraham D and Kalai, Adam Tauman and McMahan, H Brendan},
	Booktitle = {Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms},
	Organization = {Society for Industrial and Applied Mathematics},
	Pages = {385--394},
	Title = {Online convex optimization in the bandit setting: gradient descent without a gradient},
	Year = {2005}}

@inproceedings{hazan2007adaptive,
	Author = {Hazan, Elad and Rakhlin, Alexander and Bartlett, Peter L},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {65--72},
	Title = {Adaptive online gradient descent},
	Year = {2007}}

@book{nesterov2004introductory,
	Author = {Nesterov, Yurii},
	Publisher = {Springer Science \& Business Media},
	Title = {Introductory lectures on convex optimization},
	Volume = {87},
	Year = {2004}}

@article{spall1992multivariate,
	Author = {Spall, James C},
	Journal = {Automatic Control, IEEE Transactions on},
	Number = {3},
	Pages = {332--341},
	Publisher = {IEEE},
	Title = {Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
	Volume = {37},
	Year = {1992}}

@article{spall1997one,
	Author = {Spall, James C},
	Journal = {Automatica},
	Number = {1},
	Pages = {109--112},
	Publisher = {Elsevier},
	Title = {A one-measurement form of simultaneous perturbation stochastic approximation},
	Volume = {33},
	Year = {1997}}
