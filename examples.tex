%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
The main application of the biased noisy gradient oracle based convex optimization of the previous section 
is to bandit convex optimization, which we briefly introduce now. Readers familiar with these problems may skip this description
and come back to it only to clarify our notation and terminology in case some confusion arises later.

In the \emph{online variant} of bandit convex optimization a learner sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the losses $f_1(X_1),\dots,f_n(X_n)$. More specifically, in round $t$, having observed $f_1(X_1),\dots,f_{t-1}(X_{t-1})$ of the previous rounds, the learner chooses $X_t\in \cK$, after which it observed $f_t(X_t)$. The learner's goal is to minimize its expected regret $\EE{ \sum_{t=1}^n f_t(X_t) - \inf_{x\in \cK} \sum_{t=1}^n f_t(x) }$. 
This problem is also called online convex optimization with one-point feedback.
A slightly different problem is obtained if we allow the learner to choose multiple points in every round, at which points the function $f_t$ is observed. The loss is suffered at $X_t$. The points where the function is observed (``observation points'' for short) may or may not be tied to $X_t$. One possibility is that $X_t$ is one of the observation points.  
Another possibility is that $X_t$ is the average of the observation points (e.g., \citealt{AgDeXi10}). Yet another possibility is that there is no relationship between them. \todoc{I commented out the specifics of  \cite{AgDeXi10} because it is better to discuss the oracles at one place later.}

\if0
For example, \cite{AgDeXi10} used an algorithm that queries at two points per round, and defined the incurred loss as the average of losses at the two observation points. In our oracle, it can be stated as in round $t$, the same oracle $\gamma_t$ responds to the same inputs $(X_t, \delta_t, f_t)$ with two different pairs $(G_{t,1}, Y_{t,1})$ and $(G_{t,2}, Y_{t,2})$. The accumulated regret can be written as 
$% \[
R_n = \sum_{t=1}^n \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2}) \right) -\inf_{x \in \cK}\sum_{t=1}^n f_t(x) \,.
$ %\]
Recalling that $Y_{t,1}$, $Y_{t,2}$ are in the $\delta$-vicinity of $X_t$, the relationship between $f_t(X_t)$ and $\dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)$ is then determined by the environment (i.e. the property of $f_t$). It is straightforward to bound $| \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)- f_t(X_t)|$ as a function of $\delta$. \todoc{How? When $f_t$ is Lipschitz, smooth, etc? So you mean, when $f_t$ is a smooth function?}
The common assumption for this setting is: The oracle is a stochastic mapping from $(X, \delta, f)$ to $(G, Y)$; The algorithm selects the point $X_t$ depending on $\left( X_1, G_{1,1}, G_{1,2}, \cdots, X_{t-1},G_{t-1,1}, G_{t-1,2}  \right)$. This two-point feedback can be easily extended to multi-point feedback, too.
\fi

In the \emph{optimization variant} of bandit (or ``zeroth order'') convex optimization, 
the algorithm sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the loss function at these points in noise.
In particular, in round $t$, the algorithm chooses $X_t$ based on the earlier observations $Z_1,\dots,Z_{t-1}\in \R$ and $X_1,\dots,X_{t-1}$, after which it observes $Z_t$, where $Z_t$ is the value of $f(X_t)$ corrupted by ``noise''.
Previous research considered several possible constraints connecting $Z_t$ and $f(X_t)$.
One simple assumption is that $Z_t-f(X_t)$ is an $\cF_t = \sigma(X_{1:t},Z_{1:t-1})$-adapted martingale difference sequence (with favourable tail properties). \todoc{Some readers might be put off by martingales..}
A specific case is when $Z_t - f(X_t) = \xi_t$, where $(\xi_t)$ is an i.i.d. sequence.
A stronger assumption, which is most appropriate in stochastic programming, 
is that $Z_t = F(X_t,U_t)$, where $U_t\in \R$, $\int F(X_t,u) dP_U(u) = f(X_t)$ with some distribution function $P_U$ over the reals and the algorithm has access to an oracle that can produce independent samples from $F$ (in which case, $(U_t)$ may be an i.i.d. sequence sampled from $P_U$).
This assumption is stronger because the algorithm controls the ``noise''. 
In particular, the algorithm may decide to reuse the same random sample from $F$ multiple times, 
in the hope of increasing accuracy, akin to the method of common random numbers from Monte Carlo algorithms.
\todoc{mention simulation optimization as the ``field''}
Again, it is also possible to consider multi-point feedback as in the online case.

However, what differs in optimization variant is that the value of the loss at the points sent to the oracle does not matter.
Hence, in this setting the distinction between one-point and multi-point feedback (as long as the number observations is fixed, independently of the dimension) is irrelevant: 
By grouping $K$ multiple consecutive observations, one can turn an $n$ rounds one-point feedback setup into an $n/K$-round $K$-point feedback setup. The reduction in the number of rounds, being a fixed constant factor, is negligible, as far as the convergence rates are considered.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\centering
\begin{tabular}{|c|c|c|}
\toprule
\textbf{Noise }$\bm{ \rightarrow}$ & \multirow{2}{*}{\textbf{Controlled }($\bm{\xi^+ = \xi^-}$)} & \multirow{2}{*}{\textbf{Uncontrolled }(see~\eqref{eq:noiseass})} \\ 
\textbf{Function } &&\\
$\bm{\downarrow}$ &&\\\midrule
\multirow{2}{*}{\textbf{Convex + Smooth}} & \multirow{2}{*}{$(C_1 \delta, C_2)$} & \multirow{2}{*}{$(C_1\delta, C_2/\delta^2)$}\\ 
 &&\\\midrule
\multirow{2}{*}{$\bm{f \in \C^3}$} & \multirow{2}{*}{$(C_1 \delta^2, C_2/\delta^2)$} & \multirow{2}{*}{$(C_1 \delta^2, C_2/\delta^2)$} \\ 
 &&\\\bottomrule
\end{tabular}
\caption{Gradient oracles for different function classes and noise categories. Each table entry specifies the pair $(c_1(\delta), c_2(\delta))$.
For the second row, $C_1 = \frac{B_3 \EE{ \norm{V}_* \norm{U}^3 }}{6}$ and $C_2 = 4 \EE{\norm{V}_*^2}\left( \sigma_\xi^2+\fspan(f)\right)$.
}
\label{tab:oracles}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%A major tool in bandit convex optimization is to design gradient estimators, which are then used in conjunction with variants of gradient descent. \todoc{zillions of references.}
A common popular idea is to bandit convex optimization is to use the bandit feedback to construct noisy (and biased) estimate of the gradient \cite{XXXXX}.
In particular, consider the following oracle (which, as will be discussed later 
 encompasses several simultaneous perturbation methods):
Given the inputs $x\in \K$,  $0<\delta\le 1$,  \todoc{We need an upper bound I believe. Add it earlier.}
the gradient estimate is
\begin{align}
%\hspace{-1.5ex}
\begin{split}
G &=  \dfrac{Z^+ - Z^-}{2\delta}\, V \,, \\
&  Z^{\pm} = f(X^{\pm}) + \xi^{\pm}, X^{\pm} = x \pm \delta U\,,
 \end{split}
%V \left(\dfrac{f(x+\delta U) + \xi^+ - (f(x-\delta U) + \xi^-)}{2\delta}\right).
 \label{eq:twosp}
\end{align}
where $U,V\in \R^d$, $\xi^{\pm}\in \R$ are random, jointly distributed random variables, $U,V$ chosen by the oracle
from some fixed distribution characterizing the oracle and $\xi^{\pm}$ being the noise of the returned feedback $Z^{\pm}$ at points $X^{\pm}$.
For the following proposition we consider $4=2\times 2$ cases.
First, the function is either assumed to be $L$-smooth and convex (i.e., the derivative of $f$ is $L$-Lipschitz w.r.t. $\norm{\cdot}_*$), or it is assumed to be three times continuously differentiable (in notation: $f\in C^3$).
The other two options are that either $\xi^+=\xi^-$, which we call the \emph{controlled noise} setting (see above for the motivation), or we make the alternate assumptions 
\begin{align}
\begin{split}
\E[\xi^+ |\, V] = \E[\xi^+ |\, V] =0 \text{ and }\\
\E [ (\xi^{+} - \xi^-)^{2} \mid V] \le \sigma_\xi^2 <\infty\,.
\end{split}
\label{eq:noiseass}
\end{align}

The following proposition provides conditions under which the bias-variance parameters $(c_1,c_2)$ can be bounded as shown in \cref{tab:oracles}:
\begin{proposition}
\label{prop:grad-spsa}
Consider a function $f:\D \to \R$, with $\K \subset \D^\circ \subset \R^d$.%
\footnote{Here, $\D^\circ$ denotes the interior of $\D$.}
For any $x \in \cK$, and $0< \delta \le 1$ let the oracle $\gamma$ return $G$ as specified in~\eqref{eq:twosp},
where $(U,V)$ are such that $x+\delta U \in \D$ a.s.,
$\E[V U\tr] = I$, while 
$\EE{\norm{V}_*^2}, \EE{ \norm{U}^3 }<+\infty$.
\if0
Consider the following function classes:\\ 
\textbf{1:} $f$ is convex, with a $L$-Lipschitz gradient.\\
\textbf{2:} $f \in \C^3$, i.e., $f$ is three times continuously differentiable and has bounded third derivative.

Let $\xi^+, \xi^-$ be  random and are either \\
\textbf{Controlled:} $\xi^+= \xi^-$ (or)  \\
\textbf{Uncontrolled:} $\E[\xi^+ \mid V] = \E[\xi^+ \mid V] =0$ and $\E \left( (\xi^{+} - \xi^-)^{2} \mid V\right) < \infty$.
\fi
Then, we have that $\gamma$ is a type-I oracle with $c_1(\delta)$ and $c_2(\delta)$ given by \cref{tab:oracles}.
\end{proposition}
\begin{proof}
See Appendix \ref{sec:appendix-grad}.
\end{proof}

\paragraph{Popular choices for $V$ and $U$:}\ \\
\begin{inparaenum}[$\bullet$]
 \item If we set $U_i$ to be independent, symmetric $\pm 1$-valued r.v.s and $V_i = 1/U_i$, then we recover the popular SPSA scheme proposed by \cite{spall1992multivariate}.
It is easy to see that $\EE{  V U\tr } = I$ holds in this case.
 When the norm $\norm{\cdot}$ is the $2$-norm, $C_1 = O(d^2)$ and $C_2 = O(d)$. If we set $\norm{\cdot}$ to be the max-norm, $C_1 = O(\sqrt{d})$ and $C_2 = O(d)$.\\
 \item If we set $V=U$, with $U$ chosen uniform at random on the surface of a sphere with radius $\sqrt{d}$, 
 then we recover the RDSA scheme proposed by  \citeauthor{kushcla} \citep[cf. pp.~58--60][]{kushcla}. 
 In particular, $(U_i)_i$ are identically distributed with $\EE{ U_i U_j } = 0$ if $i\ne j$ and $\EE{ U\tr U } = d$, hence $\EE{U_i^2} = 1$. Thus, if we choose $\norm{\cdot}$ to be the $2$-norm, $C_1 = O( d^2 )$ and $C_2 = O(d)$.
 \\
 \item If we set $V=U$, with $U$ the standard $d$-dimensional Gaussian with unit covariance matrix, we recover the smoothed functional (SF) scheme proposed by \cite{katkul}. 
Indeed, in this case, by definition, $\EE{VU\tr} = \EE{U U\tr } = I$.
When $\norm{\cdot}$ is the $2$-norm, $C_1 = O(d^2)$
%$\norm{U}^4 = (\sum_{i=1}^d U_i^2)^{2} = \sum_i U_i^4 + 2 \sum_{i<j} U_i^2 U_j^2$,
%hence $\EE{ \norm{U}^4} = O(d^2)$
 and $C_2 = O( d)$.
 This scheme can also be interpreted as a smoothing operation that  convolves the gradient of the function $f$ with a Gaussian density.
%  , followed by an integration by parts and the resulting integral can be estimated using samples (without access to the gradient of $f$).   
\end{inparaenum}

It is also possible to obtain a gradient oracle based on a single noisy observation (see \cref{prop:grad-onepoint}). \todoc{Add citations.}
However, as discussed, in the optimization setting one can also always group two consecutive observations
at the price of reducing the number of rounds by a factor of two only, which has a negligible effect on the rate of convergence.
\todoc{For now, I comment out the one-point feedback. I don't think the one-point feedback is essential and I rather work on something else than discuss the pros and cons of one-point feedback.}
\begin{proposition}
\label{prop:grad-onepoint}
For one-point feedback oracle, i.e.,
\begin{align*}
G = \dfrac{Z+\xi}{\delta}V, \quad Z=f(x+\delta U)\,,
\end{align*}
in addition to the conditions in Proposition \ref{prop:grad-spsa}, we further assume $\EE{V}=0$, and for $f \in \C^3$, more specifically, $V$ is a deterministic odd function of $U$ and $U$ symmetrically distributed. Then, $\gamma$ is a type-I oracle with $c_1(\delta)$ and $c_2(\delta)$ same as the uncontrolled noise case in \cref{tab:oracles}.
\end{proposition}

Type-II oracles can be obtained using the same construction as above, but with a different analysis:
\begin{proposition}
In the one-point feedback, let
\begin{align*}
V = n_W(U)\dfrac{\lvert \partial W\rvert}{\lvert W \rvert}\,,
\end{align*}
where $W$ is a convex body with surface $\partial W$, $U \in \partial W$ is uniformly distributed. $n_W(U)$ denotes the normal vector of $\partial W$ at $U$, and $\lvert \cdot \rvert$ denotes the volume. Then, $\gamma$ is a type-II oracle with $c_2(\delta) = C_2/\delta^2$.
\todox{$c_1$ depends on property of $f$.}
\end{proposition}
\begin{proof}
this will go elsewhere.
\end{proof}
\if0
\begin{proposition}\textbf{\textit{(One-point feedback)}}
 Letting 
 \[
 G =  V \dfrac{f(x+\delta U) + \xi}{\delta}\,,
 \]
 one obtains a gradient estimate using only one function evaluation.
 
For function class \textbf{1},  let $U$ be drawn from the surface of a unit ball centered at origin w.r.t. some norm $\norm{\cdot}$, $V=dU$, then $\gamma$ is a $(C_1\delta^2, C_2/\delta^2)$ Type-II oracle.
 
 For function class \textbf{2}, in addition to the conditions in Proposition \ref{prop:grad-spsa}, we further assume $V$ is symmetrically distributed, then $\gamma$ is also a $(C_1\delta^2, C_2/\delta^2)$ Type-I oracle.
 
\end{proposition}
As in the case of two-point feedback, one can recover SPSA, RDSA and SF variants by using appropriate choices for $V$ and $U$ See Appendix \ref{sec:appendix-grad} for a formal proof.
\fi
% If the function $f$ is assumed to be convex and smooth, then gradient estimates similar to \eqref{eq:twosp} can be constructed and this does not require higher order smoothness conditions as in Proposition \ref{prop:grad-spsa}. 
% \begin{proposition}
% \label{prop:grad-convex}
% Consider a function $f:\D \to \R$, with $\K \subset \D \subset \R^d$,  
% that is convex and has a $L$-Lipschitz continuous gradient. .
% For any $x \in \cK$, and $\delta >0$ such that $\B(x,\delta) \in \D$, let the oracle $\gamma$ return 
% \begin{align}
% % Y = x+\delta U \,, \quad
% G =  V \left(\dfrac{f(x+\delta U) + \xi^+ - (f(x-\delta U) + \xi^-)}{2\delta}\right),
%  \label{eq:twosp}
% \end{align}
% where $\xi^+, \xi^-$, $V$, $U$ are as in Proposition \ref{prop:grad-spsa}. 
% Then, we have that $\gamma$ is a type-I oracle with $c_1(\delta) = C_1 \delta$ and $c_2(\delta) = C_2 d/\delta^2$.
% \end{proposition}
% \begin{proof}
% See Appendix \ref{sec:appendix-grad}.
% \end{proof}


% A popular idea for estimating gradient using one-point feedback is the smoothed function approach, which was originally proposed in \citep{katkul}. The idea is to convolve the gradient of the objective function with a suitable density function and then, via an integration by parts arguments show that the resulting integral is an estimate of the gradient of the smoothed objective function. 
% This approach has been adopted in a stochastic convex optimization setup in \cite{duchi2015optimal}. 

%% \paragraph{Simultaneous perturbation methods:}
%The idea of simultaneous perturbation can work even for functions that are not differentiable, as shown in \cite{flaxman2005online}. 
%% follow this approach in the context of bandit convex optimization.
%Formally,  
%% a $(c_1,c_2)$ Type-II oracle when $\cF$ is a general class of functions (this does not even require differentiability).
%given any $f \in \cF$, $x \in \cK$, and $\delta >0$, the oracle returns\footnote{See also \cite[pp.~58-60]{kushcla} for an old reference that proposed a gradient estimate similar to \eqref{eq:flaxman}.}   
%\begin{align}
% Y = x+\delta u \in \cK', \quad
% G = \dfrac{d}{\delta}f(x+\delta u)u \in \R^d, \label{eq:flaxman}
%\end{align}
%where $u\in \R^d$ is a random unit vector, so the first condition of \cref{def:oracle2} immediately follows. 
%The variance of $G$ is bounded by $d^2C^2 \delta^{-2}$ for some constant $C = \sup_{y\in \cK'}f(y)$. 
%\todox[inline]{Fix this proposition statement to say Flaxman scheme is a type II oracle}
%\begin{proposition}
%Let $\tilde{f}(x) = \EE{f(x+\delta v)}$ denote the smoothed version of $f$. Then, we have
%$\EE{G} = \nabla \tilde{f}(x)$.
%\end{proposition}
%\begin{proof}
% See Lemma 1 in \citep{flaxman2005online}.
%\end{proof}
% As to the bias condition, it was proved that $\EE{G} = \nabla \tilde{f}(x)$, where $\tilde{f}$ is a smoothed version of $f$, i.e.,
%$\tilde{f}(x) = \EE{f(x+\delta v)}$,
%$v$ is a random vector in a unit ball. There are different ways to bound the bias depending on the property of $f$.
%If $f$ is $L_{lip}$-Lipschitz over $\cK'$, then we have
%\begin{align*}
%\MoveEqLeft
%\norm{\tilde{f}(x)-f(x)}_\infty \\
%=&\norm{\EE{f(x+\delta v)-f(x)}}_\infty
%\le L_{lip} \delta \,.
%\end{align*}
%If $f$ is convex, and $L_{smo}$-smooth, 
%\begin{align*}
%\MoveEqLeft
%\norm{\tilde{f}(x)-f(x)}_\infty \\
%\le& \norm{\EE{\ip{\nabla f(x), \delta v}+\dfrac{L_{smo}}{2}\delta^2\norm{v}^2}}_\infty\\
%\le &\dfrac{L_{smo}}{2}\delta^2 \,.
%\end{align*}
%Therefore, the estimator \eqref{eq:flaxman} can always fit the oracle setting by choosing $c_1(\delta) = C_1 \delta$ (or $C_1\delta^2$), $c_2(\delta) = C_2 \delta^{-2}$, for some constant $C_1$, $C_2$.

% \begin{remark}
%  Instead of picking $u$ randomly on the surface of a unit sphere, one can employ an random variable $u$ that satisfies $E[u u\tr] = I_d$, where $I_d$ is the $d$-dimensional identity matrix. A popular choice for $u$ that satisfies the aforementioned constraint is the $d$-dimensional standard Gaussian - a choice that has been explored in the context of zeroth order optimization in \citep{duchi2015optimal}. See \citep{bhatnagar-book} for an overview of gradient and Hessian estimation techniques using random perturbations.   
% \end{remark}

It may be noticed that the function domain $\D$ can be larger than or equal to the set $\K$, where the algorithm chooses $x$. This is to ensure that the oracle will not receive invalid inputs, i.e., queries where $f$ is not defined.
When the functions are defined over $\K$ only and $\K$ is bounded, the above constructions only work for $\delta$ small enough.
In this case, the best approach perhaps is to use Dikin ellipsoids to construct the oracles, as done by \citet{HaLe14:SOC}.
\todoc[inline]{I cut the text shorter, as I don't really want to go into this.}


The above constructions suggest that one should consider type-I (and II) oracles with $c_1(\delta) = C_1 \delta^p$ and $c_2(\delta) = C_2\delta^{-q}$ with $p=q=2$.
For this type of oracles, the results of the previous section give the following result: 
\begin{theorem}
Let $\cF$ be the space of convex, $L$-smooth functions over a convex non-empty domain $\K$.
Then no algorithm that obtains information about the unknown function using a 
$(\delta^2,\delta^{-2})$ type-I oracle\todoc{Can we show that no better oracles exist? Can we show that $p=q=2$ is the best the oracle defined above can get? Then this statement could be much strengthened.}
 can achieve better error than $\Omega(n^{-1/3})$.
Similarly, no such algorithm can achieve better regret than $\Omega(n^{2/3})$.
\end{theorem}
\begin{remark}
Note that this result implies that \textbf{
the currently used gradient estimators cannot be used by any algorithm to achieve the optimal
regret $O(n^{1/2})$}.  \todoc{Add citation for optimal regret.}
\end{remark}
For the noisy optimization setting, the $O(n^{-1/3})$ rate is known to be optimal for the smooth, convex case.
\todoc{Add citation.}
\begin{theorem}
For zeroth order noisy optimization with smooth convex functions, the gradient estimators together with the algorithm of \cref{thm:mirrordescent} achieves the optimal rate. \todoc{Is this a new result?}
\end{theorem}

%When the function is defined outside $\cK$, let $\mathcal{D} \supseteq \left\lbrace y \vert \norm{y-x} \le \delta, \forall x \in \cK \right\rbrace$, where $\delta=\sup_{t}\delta_t$. When the function is not defined outside $\cK$, the algorithm must project $X$ to a shrinked set $(1-\delta)\cK$ (\cite{flaxman2005online}), or use Dikin ellipsoid 
%\citep{HaLe14:SOC}.

% An alternative approach, popularly known as SPSA \citep{spall1997one}, employs independent Rademacher random variables $\Delta_i, i=1,\ldots,d$ to estimate the gradient as follows:
% % 
% % One point SPSA
% % Proposition: One-point SPSA is blah-blah 
% % The one-point SPSA given by \cite{spall1997one} is a $(c_1,c_2)$ Type-I oracle when $\cF$ contains only $3$-times continuously differentiable functions where  $c_1 = C_1 \delta^2$, $c_2 = C_2 \delta^{-2}$ for some constant $C_1$, $C_2$.
% For any input triple $(x, \delta, f)$, the oracle responds with
% \begin{align}
% Y = x+\delta \Delta \,, \quad
% G = \dfrac{f(Y)+\epsilon}{\delta}\begin{pmatrix}\Delta_{\cdot1}^{-1}\\\Delta_{\cdot2}^{-1}\\ \vdots\\ \Delta_{\cdot
%  d}^{-1}\end{pmatrix} \,,
%  \label{eq:onespsa}
% \end{align}
% where $\epsilon$ is zero-mean measurement noise. More general distributions could be used for the perturbations $\Delta$, as long as they zero mean and satisfy bounded second and inverse second moments.
% \todox{For one-point SPSA, $\Delta$ has to be symmetric around origin to eliminate the $f(x)$, $f'(x)$ terms}
% % , $\Delta=\left(\Delta_{\cdot1}, \Delta_{\cdot2}, \cdots, \Delta_{\cdot
% %  d}  \right)^\top$ is a vector of independent random variables. Each $\Delta_{\cdot i}$ is symmetrically distributed around $0$. $\EE{\Delta_{\cdot i}^{-1}}$ exists and is bounded.  The second and third moment of $\Delta$ are also bounded.
% 
% % 
% % Proposition: Two-point SPSA is $(c_1,c_2)$ Type-I oracle when $\cF$ contains only $3$-times continuously differentiable functions where  $c_1 = \cdot$, $c_2 = \dots$. \todoc{Make this precise, add proof.}
% % \todox[inline]{two-point SPSA}
% % The two-point SPSA (\cite{spall1992multivariate}) is quite similar. However, the oracle 
% Using two-point feedback, the original SPSA algorithm \citep{spall1992multivariate} based oracle    
% will respond to $(x,\delta,f)$ with
% \begin{align}
% Y^+ &= x+\delta \Delta \,,\quad
% Y^- = x-\delta \Delta \,, \\
% G =& \dfrac{\left(f(Y^+)+\epsilon^+\right) - \left(f(Y^-)+\epsilon^-\right)}{2\delta}\begin{pmatrix}\Delta_{\cdot1}^{-1}\\\Delta_{\cdot2}^{-1}\\ \vdots\\ \Delta_{\cdot
%  d}^{-1}\end{pmatrix} \,.
%  \label{eq:twospsa}
% \end{align}
% 
% \begin{proposition}
% \label{prop:grad-spsa}
%  Assume $f$ is $3$-times continuously differentiable. Then, for $G$ governed by either \eqref{eq:onespsa} or \eqref{eq:twospsa}, we have
%  \begin{align*}
% &\EE{G_{\cdot i}}= [f'(x)]_i +O(\delta^2) \,, \quad \text{ for } i=1,\ldots,d.
% \end{align*}
% \end{proposition}
% \begin{proof}
%  See Appendix \ref{sec:appendix-grad}.
% \end{proof}
% 
% From the foregoing, it is easy to see that, for $3$-times continuously differentiable functions, one-point and two-point SPSA are $(c_1,c_2)$ Type-I oracles with $c_1 = C_1 \delta^2$, $c_2 = C_2 \delta^{-2}$ for some constant $C_1$, $C_2$.

% Under this situation, using Taylor expansion again, the $f(x)$ and $f''(x)$ terms in \eqref{eq:spsaTaylorExp} can be canceled. As a result, we only need $\Delta_{\cdot i}$ to be zero-mean instead of symmetry.


