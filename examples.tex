%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
The main application of the biased noisy gradient oracle based convex optimization of the previous section 
is to bandit convex optimization, which we briefly introduce now. Readers familiar with these problems may skip this description
and come back to it only to clarify our notation and terminology in case some confusion arises later.

In the \emph{online variant} of bandit convex optimization a learner sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the losses $f_1(X_1),\dots,f_n(X_n)$. More specifically, in round $t$, having observed $f_1(X_1),\dots,f_{t-1}(X_{t-1})$ of the previous rounds, the learner chooses $X_t\in \cK$, after which it observed $f_t(X_t)$. The learner's goal is to minimize its expected regret $\EE{ \sum_{t=1}^n f_t(X_t) - \inf_{x\in \cK} \sum_{t=1}^n f_t(x) }$. 
This problem is also called online convex optimization with one-point feedback.
A slightly different problem is obtained if we allow the learner to choose multiple points in every round, at which points the function $f_t$ is observed. The loss is suffered at $X_t$. The points where the function is observed (``observation points'' for short) may or may not be tied to $X_t$. One possibility is that $X_t$ is one of the observation points.  
Another possibility is that $X_t$ is the average of the observation points. Yet another possibility is that there is no relationship between them. \todoc{Add refs, explain relationship between these problems.}

\todox[inline]{add the following paragraph about two-point online variant.}
For example, \cite{AgDeXi10} used an algorithm that queries at two points per round, and defined the incurred loss as the average of losses at two observation points. In our oracle, it can be stated as in round $t$, the same oracle $\gamma_t$ responds to the same inputs $(X_t, \delta_t, f_t)$ with two different pairs $(G_{t,1}, Y_{t,1})$ and $(G_{t,2}, Y_{t,2})$. The accumulated regret can be written as 
\[
R_n = \sum_{t=1}^n \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2}) \right) -\inf_{x \in \cK}\sum_{t=1}^n f_t(x) \,.
\]
Recalling that $Y_{t,1}$, $Y_{t,2}$ are in the $\delta$-vicinity of $X_t$, the relationship between $f_t(X_t)$ and $\dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)$ is then determined by the environment (i.e. the property of $f_t$). It is straightforward to bound $| \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)- f_t(X_t)|$ with $\delta$.
The common assumption for this setting is: The oracle is a stochastic mapping from $(X, \delta, f)$ to $(G, Y)$; The algorithm selects the point $X_t$ depending on $\left( X_1, G_{1,1}, G_{1,2}, \cdots, X_{t-1},G_{t-1,1}, G_{t-1,2}  \right)$. This two-point feedback can be easily extended to multi-point feedback, too.

In the \emph{optimization variant} of bandit (or ``zeroth order'') convex optimization, 
the algorithm sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the loss function at these points in noise.
In particular, in round $t$, the algorithm chooses $X_t$ based on the earlier observations $Z_1,\dots,Z_{t-1}\in \R$ and $X_1,\dots,X_{t-1}$, after which it observes $Z_t$, where $Z_t$ is the value of $f(X_t)$ corrupted by ``noise''.
Previous research considered several possible constraints connecting $Z_t$ and $f(X_t)$.
One simple assumption is that $Z_t-f(X_t)$ is an $\cF_t = \sigma(X_{1:t},Z_{1:t-1})$-adapted martingale difference sequence (with favourable tail properties). \todoc{Some readers might be put off by martingales..}
A specific case is when $Z_t - f(X_t) = \xi_t$, where $(\xi_t)$ is an i.i.d. sequence.
A stronger assumption, which is most appropriate in stochastic programming, 
is that $Z_t = F(X_t,U_t)$, where $U_t\in \R$, $\int F(X_t,u) dF(u) = f(X_t)$ with some distribution function $F$ over the reals and the algorithm has access to an oracle that can produce independent samples from $F$ (in which case, $(U_t)$ may be an i.i.d. sequence sampled from $F$).
This assumption is stronger because the algorithm controls the ``noise''. 
In particular, the algorithm may decide to reuse the same random sample from $F$ multiple times, 
in the hope of increasing accuracy, akin to the method of common random numbers from Monte Carlo algorithms.
\todoc{mention simulation optimization as the ``field''}
Again, it is also possible to consider multi-point feedback as in the online case.

\todox[inline]{add multiple-point feedback in optimization setting}
However, what differs in optimization variant is that we do not care the incurred loss in each round, since we only need a final estimate $\hat{X}_n$, which is a deterministic function of $\left( X_1, G_{1,1}, G_{1,2}, \cdots, X_{n},G_{n,1}, G_{n,2}  \right)$ as defined earlier.

\todoc[inline]{Reduction of one-point feedback to two-point feedback}

Our oracle-based framework is relevant for bandit convex optimization problems as a major technique in bandit convex
optimization is to design gradient estimators, which are then used in conjunction with variants of gradient descent. \todoc{zillions of references.}
Next, we review the most common gradient estimation techniques and show that they are all either Type-I, or Type-II oracles.

Flaxman (actually going back to 70s in Russia): \todoc{What does it do}
Proposition:  Flaxman is $(c_1,c_2)$ Type-II oracle when $\dots$. \todoc{Expand}
\todox[inline]{Flaxman's estimator}
\cite{flaxman2005online} proposed a one-point estimate to gradient, which is $(c_1,c_2)$ Type-II oracle when $\cF$ is a general class of functions (this does not even require differentiability).
Given any $f \in \cF$, $x \in \cK$, and $\delta >0$, the oracle returns 
\[
 Y = x+\delta u \in \cK', \quad
 G = \dfrac{d}{\delta}f(x+\delta u)u \in \R^d,
\]
where $u\in \R^d$ is a random unit vector, so the first condition of \cref{def:oracle2} immediately follows.
The variance of $G$ is bounded by $d^2C^2 \delta^{-2}$ for some constant $C = \sup_{y\in \cK'}f(y)$.
 As to the bias condition, it was proved that $\EE{G} = \nabla \tilde{f}(x)$, where $\tilde{f}$ is a smoothed version of $f$, i.e.,
$\tilde{f}(x) = \EE{f(x+\delta v)}$,
$v$ is a random vector in a unit ball. There are different ways to bound the bias depending on the property of $f$.
If $f$ is $L_{lip}$-Lipschitz over $\cK'$, then we have
\begin{align*}
\MoveEqLeft
\norm{\tilde{f}(x)-f(x)}_\infty \\
=&\norm{\EE{f(x+\delta v)-f(x)}}_\infty
\le L_{lip} \delta \,.
\end{align*}
If $f$ is convex, and $L_{smo}$-smooth, 
\begin{align*}
\MoveEqLeft
\norm{\tilde{f}(x)-f(x)}_\infty \\
\le& \norm{\EE{\ip{\nabla f(x), \delta v}+\dfrac{L_{smo}}{2}\delta^2\norm{v}^2}}_\infty\\
\le &\dfrac{L_{smo}}{2}\delta^2 \,.
\end{align*}
Therefore, the Flaxman's estimator can always fit the oracle setting by choosing $c_1(\delta) = C_1 \delta$ (or $C_1\delta^2$), $c_2(\delta) = C_2 \delta^{-2}$, for some constant $C_1$, $C_2$.

\todox[inline]{mention Dikin}
It may be noticed that the set $\cK'$ can be different from $\cK$. This is not a issue when the oracle can be queried outside $\cK$. However, if it is not the case, the algorithm can project $X$ to a shrinked set $(1-\delta)\cK$ (\cite{flaxman2005online}), or use Dikin ellipsoid (\cite{AbHaRa08}).


One point SPSA
Proposition: One-point SPSA is blah-blah 
\todox[inline]{one-point SPSA}
The one-point SPSA given by \cite{spall1997one} is a $(c_1,c_2)$ Type-I oracle when $\cF$ contains only $3$-times continuously differentiable functions where  $c_1 = C_1 \delta^2$, $c_2 = C_2 \delta^{-2}$ for some constant $C_1$, $C_2$.
For any input triple $(x, \delta, f)$, the oracle responds with
\begin{align*}
Y = x+\delta \Delta \,, \quad
G = \dfrac{f(Y)+\epsilon}{\delta}\begin{pmatrix}\Delta_{\cdot1}^{-1}\\\Delta_{\cdot2}^{-1}\\ \vdots\\ \Delta_{\cdot
 d}^{-1}\end{pmatrix} \,,
\end{align*}
where $\epsilon$ is zero-mean measurement noise, $\Delta=\left(\Delta_{\cdot1}, \Delta_{\cdot2}, \cdots, \Delta_{\cdot
 d}  \right)^\top$ is a vector of independent random variables. Each $\Delta_{\cdot i}$ is symmetrically distributed around $0$. $\EE{\Delta_{\cdot i}^{-1}}$ exists and is bounded.  The second and third moment of $\Delta$ are also bounded.
 Since $f$ is $3$-times continuously differentiable, using Taylor's expansion, we get for the the $i^{th}$ component of $G$,
\begin{align*}
&\EE{G_{\cdot i}}\\
=&\EE{\dfrac{1}{\delta \Delta_{\cdot i}} \left( f(x+\delta \Delta)+\epsilon \right) }\\
=& \EE{\dfrac{1}{\delta \Delta_{\cdot i}} \left( f(x)+\delta f'(x)^\top \Delta+\dfrac{1}{2}\delta^2 \Delta^\top f''(x)\Delta \right) } \numberthis \label{eq:spsaTaylorExp} \\
&+\EE{\dfrac{1}{\delta \Delta_{\cdot i}} \left(O(\delta^3 \Delta\otimes\Delta\otimes\Delta) +\epsilon \right) }\\
=& [f'(x)]_i +O(\delta^2) \,,
\end{align*}
where $[f'(x)]_i$ denotes the $i^{th}$ component of $f'(x)$. The last equality comes from the properties of symmetry, and bounded moment for $\Delta$.
Hence, $G$ is a estimate of $f'(x)$ with bias $O(\delta^2)$.

Two point SPSA

Proposition: Two-point SPSA is $(c_1,c_2)$ Type-I oracle when $\cF$ contains only $3$-times continuously differentiable functions where  $c_1 = \cdot$, $c_2 = \dots$. \todoc{Make this precise, add proof.}
\todox[inline]{two-point SPSA}
The two-point SPSA (\cite{spall1992multivariate}) is quite similar. However, the oracle will respond to $(x,\delta,f)$ with
\begin{align*}
Y^+ &= x+\delta \Delta \,,\quad
Y^- = x-\delta \Delta \,, \\
G =& \dfrac{\left(f(Y^+)+\epsilon^+\right) - \left(f(Y^-)+\epsilon^-\right)}{2\delta}\begin{pmatrix}\Delta_{\cdot1}^{-1}\\\Delta_{\cdot2}^{-1}\\ \vdots\\ \Delta_{\cdot
 d}^{-1}\end{pmatrix} \,.
\end{align*}
Under this situation, using Taylor expansion again, the $f(x)$ and $f''(x)$ terms in \eqref{eq:spsaTaylorExp} can be canceled. As a result, we only need $\Delta_{\cdot i}$ to be zero-mean instead of symmetry.


\todoc[inline]{Different regularity conditions on $\cF$! Strong convexity, smoothness, etc.}