%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
The main application of the biased noisy gradient oracle based convex optimization of the previous section 
is to bandit convex optimization, which we briefly introduce now. Readers familiar with these problems may skip this description
and come back to it only to clarify our notation and terminology in case some confusion arises later.

In the \emph{online variant} of bandit convex optimization a learner sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the losses $f_1(X_1),\dots,f_n(X_n)$. More specifically, in round $t$, having observed $f_1(X_1),\dots,f_{t-1}(X_{t-1})$ of the previous rounds, the learner chooses $X_t\in \cK$, after which it observed $f_t(X_t)$. The learner's goal is to minimize its expected regret $\EE{ \sum_{t=1}^n f_t(X_t) - \inf_{x\in \cK} \sum_{t=1}^n f_t(x) }$. 
This problem is also called online convex optimization with one-point feedback.
A slightly different problem is obtained if we allow the learner to choose multiple points in every round, at which points the function $f_t$ is observed. The loss is suffered at $X_t$. The points where the function is observed (``observation points'' for short) may or may not be tied to $X_t$. One possibility is that $X_t$ is one of the observation points.  
Another possibility is that $X_t$ is the average of the observation points (e.g., \citealt{AgDeXi10}). Yet another possibility is that there is no relationship between them. \todoc{I commented out the specifics of  \cite{AgDeXi10} because it is better to discuss the oracles at one place later.}

\if0
For example, \cite{AgDeXi10} used an algorithm that queries at two points per round, and defined the incurred loss as the average of losses at the two observation points. In our oracle, it can be stated as in round $t$, the same oracle $\gamma_t$ responds to the same inputs $(X_t, \delta_t, f_t)$ with two different pairs $(G_{t,1}, Y_{t,1})$ and $(G_{t,2}, Y_{t,2})$. The accumulated regret can be written as 
$% \[
R_n = \sum_{t=1}^n \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2}) \right) -\inf_{x \in \cK}\sum_{t=1}^n f_t(x) \,.
$ %\]
Recalling that $Y_{t,1}$, $Y_{t,2}$ are in the $\delta$-vicinity of $X_t$, the relationship between $f_t(X_t)$ and $\dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)$ is then determined by the environment (i.e. the property of $f_t$). It is straightforward to bound $| \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)- f_t(X_t)|$ as a function of $\delta$. \todoc{How? When $f_t$ is Lipschitz, smooth, etc? So you mean, when $f_t$ is a smooth function?}
The common assumption for this setting is: The oracle is a stochastic mapping from $(X, \delta, f)$ to $(G, Y)$; The algorithm selects the point $X_t$ depending on $\left( X_1, G_{1,1}, G_{1,2}, \cdots, X_{t-1},G_{t-1,1}, G_{t-1,2}  \right)$. This two-point feedback can be easily extended to multi-point feedback, too.
\fi

In the \emph{optimization variant} of bandit (or ``zeroth order'') convex optimization, 
the algorithm sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the loss function at these points in noise.
In particular, in round $t$, the algorithm chooses $X_t$ based on the earlier observations $Z_1,\dots,Z_{t-1}\in \R$ and $X_1,\dots,X_{t-1}$, after which it observes $Z_t$, where $Z_t$ is the value of $f(X_t)$ corrupted by ``noise''.
Previous research considered several possible constraints connecting $Z_t$ and $f(X_t)$.
One simple assumption is that $Z_t-f(X_t)$ is an $\cF_t = \sigma(X_{1:t},Z_{1:t-1})$-adapted martingale difference sequence (with favourable tail properties). \todoc{Some readers might be put off by martingales..}
A specific case is when $Z_t - f(X_t) = \xi_t$, where $(\xi_t)$ is an i.i.d. sequence.
A stronger assumption, which is most appropriate in stochastic programming, 
is that $Z_t = F(X_t,U_t)$, where $U_t\in \R$, $\int F(X_t,u) dP_U(u) = f(X_t)$ with some distribution function $P_U$ over the reals and the algorithm has access to an oracle that can produce independent samples from $F$ (in which case, $(U_t)$ may be an i.i.d. sequence sampled from $P_U$).
This assumption is stronger because the algorithm controls the ``noise''. 
In particular, the algorithm may decide to reuse the same random sample from $F$ multiple times, 
in the hope of increasing accuracy, akin to the method of common random numbers from Monte Carlo algorithms.
\todoc{mention simulation optimization as the ``field''}
Again, it is also possible to consider multi-point feedback as in the online case.

However, what differs in optimization variant is that we do not care the incurred loss in each round, since we only need a final estimate $\hat{X}_n$, which is a deterministic function of the previous observations.
Note that in this setting the distinction between one-point and two-point feedback (or $K$ point feedback with a fixed $K$) is minor: If one is given the opportunity for $n$ one-point feedback observations, this is obviously equivalent to having given the opportunity for $n/2$ two-point feedback observations (or $n/K$ $K$-point observations), which is a (negligible) constant-factor reduction of the number of observations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\centering
\begin{tabular}{|c|c|c|}
\toprule
\textbf{Noise }$\bm{ \rightarrow}$ & \multirow{2}{*}{\textbf{Controlled }($\bm{\xi^+ = \xi^-}$)} & \multirow{2}{*}{\textbf{Uncontrolled }($\bm{\xi^+ \ne \xi^-}$)} \\ 
\textbf{Function } &&\\
$\bm{\downarrow}$ &&\\\midrule
\multirow{2}{*}{\textbf{Convex + Smooth}} & \multirow{2}{*}{$(C_1 \delta, C_2)$} & \multirow{2}{*}{$(C_1\delta, C_2/\delta^2)$}\\ 
 &&\\\midrule
\multirow{2}{*}{$\bm{f \in \C^3}$} & \multirow{2}{*}{$(C_1 \delta^2, C_2/\delta^2)$} & \multirow{2}{*}{$(C_1 \delta^2, C_2/\delta^2)$} \\ 
 &&\\\bottomrule
\end{tabular}
\caption{Gradient oracles for different function classes and noise categories. Each table entry specifies the pair $(c_1(\delta), c_2(\delta))$.}
\label{tab:oracles}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%A major tool in bandit convex optimization is to design gradient estimators, which are then used in conjunction with variants of gradient descent. \todoc{zillions of references.}
A common popular idea is to bandit convex optimization is to use the bandit feedback to construct noisy (and biased) estimated of the gradient \cite{XXXXX}.
The following proposition presents an oracle that encompasses several simultaneous perturbation methods, which are popular for estimating the gradient of a function using two-point feedback. 
For simplicity, we present the result for the case when the point feedbacks are corrupted by noise with zero mean and bounded variance. \todoc{What is really that we need to assume about the noise?}
Note that the proposition does not always require the function to be convex or differentiable. \todoc{Any advantage of adding convexity, smoothness? Can we then maybe drop the condition that $f$ is $C^3$ when some of these conditions are met?}
\begin{proposition}\textbf{\textit{(Two-point feedback)}}
\label{prop:grad-spsa}
Consider a function $f:\D \to \R$, with $\K \subseteq \D \subset \R^d$:\\
For any $x \in \cK$, and $\delta >0$ such that $\B(x,\delta) \in \D$\footnote{$\B(x,\delta)$ denotes the ball of radius $\delta$ centered at $x$.}, let the oracle $\gamma$ return 
\begin{align}
% Y = x+\delta U \,, \quad
\hspace{-1.5ex}G =  V \left(\dfrac{f(x+\delta U) + \xi^+ - (f(x-\delta U) + \xi^-)}{2\delta}\right),
 \label{eq:twosp}
\end{align}
where $V, U \in \R^d$ are random vectors 
that satisfy $\E[V U\tr] = I$, $U$ is unit vector and has finite third moment, $\norm{V}$ is bounded. 

Consider the following function classes:\\ 
\textbf{1:} $f$ is convex, with a $L$-Lipschitz gradient.\\
\textbf{2:} $f \in \C^3$, i.e., $f$ is three times continuously differentiable and has bounded third derivative.

Let $\xi^+, \xi^-$ be  i.i.d. and are either \\
\textbf{Controlled:} $\xi^+= \xi^-$ (or)  \\
\textbf{Uncontrolled:} $\E[\xi^+ \mid V] = \E[\xi^+ \mid V] =0$ and $\E \left( (\xi^{+} - \xi^-)^{2} \mid V\right) < \infty$.
\todox{I don't think we need the controlled noise, assuming zero-mean noise is enough.}

Then, we have that $\gamma$ is a type-I oracle with $c_1(\delta)$ and $c_2(\delta)$ given by Table \ref{tab:oracles}.
\end{proposition}
\begin{proof}
See Appendix \ref{sec:appendix-grad}.
\end{proof}

\paragraph{Popular choices for $V$ and $U$:}\ \\
\begin{inparaenum}[$\bullet$]
 \item If we set $U_i$ to be i.i.d and zero mean, $V_i = 1/U_i$, then we recover the popular SPSA scheme proposed in \cite{spall1992multivariate}.\\ 
 \item If we set $V=U$, with $U_i$ chosen either random on the surface of a unit sphere, then we recover the RDSA scheme proposed in \cite[pp.~58-60]{kushcla}. \\
 \item If we set $V=U$, with $U_i$ to be the standard Gaussian, we recover the smoothed functional (SF) scheme proposed in \cite{katkul}. This scheme can also be interpreted as a smoothing operation that  convolves the gradient of the function $f$ with a Gaussian density.
%  , followed by an integration by parts and the resulting integral can be estimated using samples (without access to the gradient of $f$).   
\end{inparaenum}

\begin{proposition}\textbf{\textit{(One-point feedback)}}
 Letting 
 \[
 G =  V \dfrac{f(x+\delta U) + \xi}{\delta}\,,
 \]
 one obtains a gradient estimate using only one function evaluation.
 
For function class \textbf{1},  let $U$ be drawn from the surface of a unit ball centered at origin w.r.t. some norm $\norm{\cdot}$, $V=dU$, then $\gamma$ is a $(C_1\delta^2, C_2/\delta^2)$ Type-II oracle.
 
 For function class \textbf{2}, in addition to the conditions in Proposition \ref{prop:grad-spsa}, we further assume $V$ is symmetrically distributed, then $\gamma$ is also a $(C_1\delta^2, C_2/\delta^2)$ Type-I oracle.
 
\end{proposition}
As in the case of two-point feedback, one can recover SPSA, RDSA and SF variants by using appropriate choices for $V$ and $U$ See Appendix \ref{sec:appendix-grad} for a formal proof.

% If the function $f$ is assumed to be convex and smooth, then gradient estimates similar to \eqref{eq:twosp} can be constructed and this does not require higher order smoothness conditions as in Proposition \ref{prop:grad-spsa}. 
% \begin{proposition}
% \label{prop:grad-convex}
% Consider a function $f:\D \to \R$, with $\K \subset \D \subset \R^d$,  
% that is convex and has a $L$-Lipschitz continuous gradient. .
% For any $x \in \cK$, and $\delta >0$ such that $\B(x,\delta) \in \D$, let the oracle $\gamma$ return 
% \begin{align}
% % Y = x+\delta U \,, \quad
% G =  V \left(\dfrac{f(x+\delta U) + \xi^+ - (f(x-\delta U) + \xi^-)}{2\delta}\right),
%  \label{eq:twosp}
% \end{align}
% where $\xi^+, \xi^-$, $V$, $U$ are as in Proposition \ref{prop:grad-spsa}. 
% Then, we have that $\gamma$ is a type-I oracle with $c_1(\delta) = C_1 \delta$ and $c_2(\delta) = C_2 d/\delta^2$.
% \end{proposition}
% \begin{proof}
% See Appendix \ref{sec:appendix-grad}.
% \end{proof}


% A popular idea for estimating gradient using one-point feedback is the smoothed function approach, which was originally proposed in \citep{katkul}. The idea is to convolve the gradient of the objective function with a suitable density function and then, via an integration by parts arguments show that the resulting integral is an estimate of the gradient of the smoothed objective function. 
% This approach has been adopted in a stochastic convex optimization setup in \cite{duchi2015optimal}. 

%% \paragraph{Simultaneous perturbation methods:}
%The idea of simultaneous perturbation can work even for functions that are not differentiable, as shown in \cite{flaxman2005online}. 
%% follow this approach in the context of bandit convex optimization.
%Formally,  
%% a $(c_1,c_2)$ Type-II oracle when $\cF$ is a general class of functions (this does not even require differentiability).
%given any $f \in \cF$, $x \in \cK$, and $\delta >0$, the oracle returns\footnote{See also \cite[pp.~58-60]{kushcla} for an old reference that proposed a gradient estimate similar to \eqref{eq:flaxman}.}   
%\begin{align}
% Y = x+\delta u \in \cK', \quad
% G = \dfrac{d}{\delta}f(x+\delta u)u \in \R^d, \label{eq:flaxman}
%\end{align}
%where $u\in \R^d$ is a random unit vector, so the first condition of \cref{def:oracle2} immediately follows. 
%The variance of $G$ is bounded by $d^2C^2 \delta^{-2}$ for some constant $C = \sup_{y\in \cK'}f(y)$. 
%\todox[inline]{Fix this proposition statement to say Flaxman scheme is a type II oracle}
%\begin{proposition}
%Let $\tilde{f}(x) = \EE{f(x+\delta v)}$ denote the smoothed version of $f$. Then, we have
%$\EE{G} = \nabla \tilde{f}(x)$.
%\end{proposition}
%\begin{proof}
% See Lemma 1 in \citep{flaxman2005online}.
%\end{proof}
% As to the bias condition, it was proved that $\EE{G} = \nabla \tilde{f}(x)$, where $\tilde{f}$ is a smoothed version of $f$, i.e.,
%$\tilde{f}(x) = \EE{f(x+\delta v)}$,
%$v$ is a random vector in a unit ball. There are different ways to bound the bias depending on the property of $f$.
%If $f$ is $L_{lip}$-Lipschitz over $\cK'$, then we have
%\begin{align*}
%\MoveEqLeft
%\norm{\tilde{f}(x)-f(x)}_\infty \\
%=&\norm{\EE{f(x+\delta v)-f(x)}}_\infty
%\le L_{lip} \delta \,.
%\end{align*}
%If $f$ is convex, and $L_{smo}$-smooth, 
%\begin{align*}
%\MoveEqLeft
%\norm{\tilde{f}(x)-f(x)}_\infty \\
%\le& \norm{\EE{\ip{\nabla f(x), \delta v}+\dfrac{L_{smo}}{2}\delta^2\norm{v}^2}}_\infty\\
%\le &\dfrac{L_{smo}}{2}\delta^2 \,.
%\end{align*}
%Therefore, the estimator \eqref{eq:flaxman} can always fit the oracle setting by choosing $c_1(\delta) = C_1 \delta$ (or $C_1\delta^2$), $c_2(\delta) = C_2 \delta^{-2}$, for some constant $C_1$, $C_2$.

% \begin{remark}
%  Instead of picking $u$ randomly on the surface of a unit sphere, one can employ an random variable $u$ that satisfies $E[u u\tr] = I_d$, where $I_d$ is the $d$-dimensional identity matrix. A popular choice for $u$ that satisfies the aforementioned constraint is the $d$-dimensional standard Gaussian - a choice that has been explored in the context of zeroth order optimization in \citep{duchi2015optimal}. See \citep{bhatnagar-book} for an overview of gradient and Hessian estimation techniques using random perturbations.   
% \end{remark}

\todox[inline]{mention Dikin}
It may be noticed that the function domain $\mathcal{D}$ can be larger than or equal to the set $\cK$, where the algorithm chooses $x$. This is to ensure that the oracle will not receive invalid inputs, i.e., queries where $f$ is not defined.

When the function is defined outside $\cK$, let $\mathcal{D} \supseteq \left\lbrace y \vert \norm{y-x} \le \delta, \forall x \in \cK \right\rbrace$, where $\delta=\sup_{t}\delta_t$. When the function is not defined outside $\cK$, the algorithm must project $X$ to a shrinked set $(1-\delta)\cK$ (\cite{flaxman2005online}), or use Dikin ellipsoid (\cite{AbHaRa08}).

% An alternative approach, popularly known as SPSA \citep{spall1997one}, employs independent Rademacher random variables $\Delta_i, i=1,\ldots,d$ to estimate the gradient as follows:
% % 
% % One point SPSA
% % Proposition: One-point SPSA is blah-blah 
% % The one-point SPSA given by \cite{spall1997one} is a $(c_1,c_2)$ Type-I oracle when $\cF$ contains only $3$-times continuously differentiable functions where  $c_1 = C_1 \delta^2$, $c_2 = C_2 \delta^{-2}$ for some constant $C_1$, $C_2$.
% For any input triple $(x, \delta, f)$, the oracle responds with
% \begin{align}
% Y = x+\delta \Delta \,, \quad
% G = \dfrac{f(Y)+\epsilon}{\delta}\begin{pmatrix}\Delta_{\cdot1}^{-1}\\\Delta_{\cdot2}^{-1}\\ \vdots\\ \Delta_{\cdot
%  d}^{-1}\end{pmatrix} \,,
%  \label{eq:onespsa}
% \end{align}
% where $\epsilon$ is zero-mean measurement noise. More general distributions could be used for the perturbations $\Delta$, as long as they zero mean and satisfy bounded second and inverse second moments.
% \todox{For one-point SPSA, $\Delta$ has to be symmetric around origin to eliminate the $f(x)$, $f'(x)$ terms}
% % , $\Delta=\left(\Delta_{\cdot1}, \Delta_{\cdot2}, \cdots, \Delta_{\cdot
% %  d}  \right)^\top$ is a vector of independent random variables. Each $\Delta_{\cdot i}$ is symmetrically distributed around $0$. $\EE{\Delta_{\cdot i}^{-1}}$ exists and is bounded.  The second and third moment of $\Delta$ are also bounded.
% 
% % 
% % Proposition: Two-point SPSA is $(c_1,c_2)$ Type-I oracle when $\cF$ contains only $3$-times continuously differentiable functions where  $c_1 = \cdot$, $c_2 = \dots$. \todoc{Make this precise, add proof.}
% % \todox[inline]{two-point SPSA}
% % The two-point SPSA (\cite{spall1992multivariate}) is quite similar. However, the oracle 
% Using two-point feedback, the original SPSA algorithm \citep{spall1992multivariate} based oracle    
% will respond to $(x,\delta,f)$ with
% \begin{align}
% Y^+ &= x+\delta \Delta \,,\quad
% Y^- = x-\delta \Delta \,, \\
% G =& \dfrac{\left(f(Y^+)+\epsilon^+\right) - \left(f(Y^-)+\epsilon^-\right)}{2\delta}\begin{pmatrix}\Delta_{\cdot1}^{-1}\\\Delta_{\cdot2}^{-1}\\ \vdots\\ \Delta_{\cdot
%  d}^{-1}\end{pmatrix} \,.
%  \label{eq:twospsa}
% \end{align}
% 
% \begin{proposition}
% \label{prop:grad-spsa}
%  Assume $f$ is $3$-times continuously differentiable. Then, for $G$ governed by either \eqref{eq:onespsa} or \eqref{eq:twospsa}, we have
%  \begin{align*}
% &\EE{G_{\cdot i}}= [f'(x)]_i +O(\delta^2) \,, \quad \text{ for } i=1,\ldots,d.
% \end{align*}
% \end{proposition}
% \begin{proof}
%  See Appendix \ref{sec:appendix-grad}.
% \end{proof}
% 
% From the foregoing, it is easy to see that, for $3$-times continuously differentiable functions, one-point and two-point SPSA are $(c_1,c_2)$ Type-I oracles with $c_1 = C_1 \delta^2$, $c_2 = C_2 \delta^{-2}$ for some constant $C_1$, $C_2$.

% Under this situation, using Taylor expansion again, the $f(x)$ and $f''(x)$ terms in \eqref{eq:spsaTaylorExp} can be canceled. As a result, we only need $\Delta_{\cdot i}$ to be zero-mean instead of symmetry.


\todoc[inline]{Different regularity conditions on $\cF$! Strong convexity, smoothness, etc.}