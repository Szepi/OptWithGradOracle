%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)

\begin{table*}
\centering
 \caption{Summary of upper and lower bounds for various smooth function classes and different gradient oracles}
\label{tab:mse-1}
\begin{tabular}{|c|c|c|c|c|}
\toprule
\rowcolor{gray!20}
% \multicolumn{3}{|c|}{\multirow{2}{*}{\textbf{Lower bounds}}}\\[1.7em]
% \midrule
  \multirow{2}{*}{\textbf{Oracle type}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Convex + Smooth}}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Strongly Convex + Smooth}}} \\[1.7em]
 \midrule
\multirow{2}{*}{ $C_1=0$ and $q=0$} & \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d}{\sqrt{n}}\right)$} & \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d}{\sqrt{n}}\right)$}  \\[1.5em]\midrule
%%%%%%%%%%%%%%%%
\multirow{2}{*}{ $p=1$ and $q=2$} & \multirow{2}{*}{$\O\left(\dfrac{d^{\frac{1}{4}}}{n^{\frac{1}{4}}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d^\frac{1}{4}}{n^\frac{1}{4}}\right)$}& \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{1}{\sqrt{d n}}\right)$} \\[1.5em]\midrule
%%%%%%%%%%%%%%%%%%%
\multirow{2}{*}{ $p=2$ and $q=2$} & \multirow{2}{*}{$\O\left(\dfrac{d^{\frac{1}{6}}}{n^{\frac{1}{3}}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d^\frac{1}{6}}{n^\frac{1}{3}}\right)$} & \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{1}{d^\frac{2}{3} n^\frac{2}{3}}\right)$}\\[1.5em]
 \bottomrule 
\end{tabular}
\end{table*}

Unconstrained case:

Upper bound: 

\begin{algorithm}
	\caption{Mirror Descent with Type-I Oracle}\label{alg}
	\textbf{Input}: Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$. \\
	Initialize $X_1\in \cK$ arbitrarily.\\
	In round $t=1, 2, \cdots, n-1$:
	\begin{itemize}
	\item Query the oracle at $X_t$.
	\item Receive $G_t$, $Y_t$.
	\item Update
	\[
	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
	\]
	\end{itemize}
	\textbf{Output}: the optimizer
	\[
	\hat{X}_n = \dfrac{1}{n}\sum_{t=1}^n X_t \,.
	\]
	
\end{algorithm}

\begin{theorem}
\label{thm:ub}
Given a bounded, convex set $\cK \subset \R$,
let $\cF$ be a class of convex functions on $\cK$ such that $f \in \cF$ is $L$-smooth.
Assume further that the regularization function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\norm{\cdot}$, and $\interior(\mathcal{K})\subseteq \dom(\mathcal{R})$.
For any $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$, 
 if \cref{alg} is run with 
 \[
 \eta_t = \dfrac{\alpha}{a_t+L} \,,
 \]
 \[
 \delta = \left( \dfrac{C_2}{2aC_1}\sqrt{\dfrac{\alpha}{2D}} \right)^{1/(p+q)}n^{-1/(2p+q)} \,,
 \] 
 \todox{the $n$ should actually be $(n-1)$, the expression will be more natural if we start from $0$.}
 where
 $a = \left( \sqrt{\dfrac{2^{3q}\alpha}{D}}C_1^q C_2^p \right)^{1/(2p+q)}$,
 $a_t = a t^{(p+q)/(2p+q)}$, for $t=1, 2, \cdots, n-1$,
  the minimax error \eqref{eq:minimaxerrdef} can be bounded as
 \begin{align*}
\Delta_n^*(\cF,c_1,c_2)= O(n^{-p/(2p+q)})\,,
 \end{align*}
 where $D=\sup_{x,y\in \cK} \DR(x,y)$. $\DR$ denotes the Bregman divergence w.r.t. $\mathcal{R}$.
 
If the class of functions is also $\mu$-strongly convex w.r.t. $\mathcal{R}$. Choose
\[
 \eta_t = \dfrac{2}{\mu t} \,,
\]
\[
\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n} \,,
\]
the upper bound of minimax error is then
 \begin{align*}
\Delta_n^*(\cF,c_1,c_2)= O(n^{-p/(p+q)})\,.
 \end{align*}
\end{theorem}
\begin{proof}
See Section \ref{sec:ub-proof}.
\end{proof}


Lower bound:
\begin{theorem}
\label{thm:lb-convex}
For any $v \in \{+1,-1\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
$f^{(i)}_{v_i}(x_i) := \dfrac{\epsilon}{2} (x_i - v_i)^2$, for $i=1,\ldots,d$.
Consider the space of functions $\F:= \{f_v \mid v  \in \{+1,-1\}^d\}$, with $\epsilon = d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where \\$K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$. 
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}d^{\frac{q}{(4p+2q)}}\left(\dfrac{1}{2 K_1 \sqrt n}\right)^{\frac{p}{p+\frac{q}{2}}}.
\]
\end{theorem}
\begin{proof}
 See Section \ref{sec:lb-proof}.
\end{proof}

Lower bound: Strongly convex case
\begin{theorem}
\label{thm:lb-strongly-convex}
For any $v \in \{+\nu,-\nu\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
$f^{(i)}_{v_i}(x_i) := \dfrac{1}{2} x^2 - v x$, for $i=1,\ldots,d$.
Consider the space of functions $\F:= \{f_v \mid v  \in \{+\nu,-\nu\}^d\}$, with $\nu = d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where $K_1$ is as defined in Theorem \ref{thm:lb-convex}. 
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}.
\]
\end{theorem}
\begin{proof}
 See Section \ref{sec:lb-proof}.
\end{proof}


Todo: Extend the result to the case when the algorithm can choose $\delta$ in every step.



Todo: Online learning is at least as hard as optimization. Hence, we get a regret lower bound from this, too.

Discussion: The functions are smooth and strongly convex.
However, their strong convexity constant converges to zero as $n\to\infty$.

Todo: What can we show for strongly convex functions?

Of course, the same result holds in $d$ dimensions (add details: why?). However, one expects the lower bound to scale linearly with the dimension and we were not able to derive this so far.

Constrained case:

Upper bound: 

Reductions:
Between oracles;
Optimization and cumulative regret minimization;


