%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
\begin{table*}
\centering
 \caption{Summary of upper and lower bounds for different smooth function classes and  gradient oracles}
\label{tab:mse-1}
 \begin{tabular}{|c|c|c|c|c|}
% \begin{tabular}{||*5{>{\columncolor[gray]{.9}}c}||}
\toprule
% \rowcolor{gray!20}
% \multicolumn{3}{|c|}{\multirow{2}{*}{\textbf{Lower bounds}}}\\[1.7em]
% \midrule
  \multirow{2}{*}{\textbf{Oracle type}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Convex + Smooth}}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Strongly Convex + Smooth}}} \\[1em]
 \midrule
% \rowcolor{gray!20}
 & \textbf{Upper bound} & \textbf{Lower bound} & \textbf{Upper bound} & \textbf{Lower bound}\\
 \midrule
\textbf{ One-point estimates} & \multirow{2}{*}{$\left(\dfrac{C_1^{2}C_2 D}{n}\right)^{1/4}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2^2 d}{n}\right)^{1/4}$}& \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2^2}{d n}\right)^{1/2}$} \\[0.5ex]
 ($p=1$, $q=2$) & & & &\\\midrule
%%%%%%%%%%%%%%%%%%%
\textbf{ Two-point estimates} & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 D}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2^2 \sqrt d}{n}\right)^{1/3}$} & \multirow{2}{*}{$\left(\dfrac{C_1 C_2}{n}\right)^{1/2}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2^2 }{d n}\right)^{2/3}$}\\[1.4ex]
 ($p=2$, $q=2$) & & & &\\
  \midrule
 \textbf{Zero bias} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\left(\dfrac{d^2}{n}\right)^{1/2}$}  \\[0.5ex]
 ($C_1=0$, $q=0$) & & & &\\\bottomrule
%%%%%%%%%%%%%%%%
\end{tabular}
\end{table*}


In this section we provide our main results in forms of upper and lower bounds on the minimax error.
First we provide an upper bound by analyzing a mirror-descent algorithm given in \cref{alg}. 
In the algorithm, we assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex and the target function $f$ is $L$-smooth and $\mu$-strongly convex.
% In the algorithm, we will assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\|\cdot\|$ on $\R^d$, that is $\tfrac{\alpha}{2} \|x-y\|^2 \le \DR(x,y)=f(x)-f(y)-\ip{\nabla f(y),x-y}$ for all $x,y \in \K$. Moreover, we will consider the cases when the target function $f$ is $L$-smooth w.r.t. the norm $\| \cdot \|$ for some $L>0$, that is, $f(x) \le f(y) + \ip{\nabla f(y), y-x} + \tfrac{L}{2} \|x-y\|^2$, and when $f$ is $\mu$-strongly convex w.r.t. $\cR$, that is, 
% $f(x) \ge f(y) + \ip{\nabla f(y),y-x} +\tfrac{\mu}{2} \DR(x,y)$ for all $x,y \in \K$.

%Unconstrained case:
%
%Upper bound: 

\begin{algorithm}[t]
\begin{algorithmic}
    \State {\bf Input:}  Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$.
%     In round $t=1, 2, \cdots, n-1$:
\State Initialize $X_1\in \cK$ arbitrarily.
\For{$t=1, 2, \cdots, n-1$}	
	\State Query the oracle at $X_t$.
	\State Receive $G_t$, $Y_t$.
	\State Update
	$$
	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
	$$
\EndFor
\State {\bf Return:} $\hat{X}_n = \frac{1}{n}\sum_{t=1}^n X_t \,.$
\end{algorithmic}
\caption{Mirror Descent with Type-I/II Oracle.}
\label{alg}
\end{algorithm}


% \begin{algorithm}
% 	\caption{Mirror Descent with Type-I/II Oracle}\label{alg}
% 	\textbf{Input}: Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$. \\
% 	Initialize $X_1\in \cK$ arbitrarily.\\
% 	In round $t=1, 2, \cdots, n-1$:
% 	\begin{itemize}
% 	\item Query the oracle at $X_t$.
% 	\item Receive $G_t$, $Y_t$.
% 	\item Update
% 	\[
% 	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
% 	\]
% 	\end{itemize}
% 	\textbf{Output}: the optimizer
% 	\[
% 	\hat{X}_n = \dfrac{1}{n}\sum_{t=1}^n X_t \,.
% 	\]
% 	
% \end{algorithm}

\begin{theorem}[\textit{Upper bound}]
\label{thm:ub}
Consider the class $\F_{L,0}$ of convex, $L$-smooth functions on the bounded, convex domain $\cK \subset \R^d$.
% let $\cF$ be a class of convex functions on $\cK$ such that $f \in \cF$ is $L$-smooth for some $L>0$.
Assume further that the regularization function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\norm{\cdot}$, and $\interior(\mathcal{K})\subseteq \dom(\mathcal{R})$.
For any $(c_1,c_2)$ Type-II oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$, 
 if \cref{alg} is run with $\eta_t = \dfrac{\alpha}{a_t+L}$ and
% \[
% \delta = \left( \dfrac{C_2}{2aC_1}\sqrt{\dfrac{\alpha}{2D}} \right)^{1/(p+q)}n^{-1/(2p+q)} \,,
% \] 
  \[
 \delta = \left( \dfrac{C_2}{4aC_1}\dfrac{2p+q}{p}\dfrac{n}{n+1}\right)^{1/(p+q)}n^{-1/(2p+q)} \,, \text{ where }
 \] 
% $a = \left( \sqrt{\dfrac{2^{3q}\alpha}{D}}C_1^q C_2^p \right)^{1/(2p+q)}$,
 $a^{2p+q} =2^{q-p}\left( 2+\dfrac{q}{p} \right)^p\left(1+ \dfrac{1}{n} \right)^q \left( \dfrac{\alpha}{D} \right)^{p+q}C_1^q C_2^p $,
 $a_t = a t^{(p+q)/(2p+q)}$, for $t=1, 2, \cdots, n-1$,
then  the minimax error \eqref{eq:minimaxerrdef} can be bounded as
 \begin{align*}
\Delta_n^*(\cF,c_1,c_2)= O\left(\left(\dfrac{D^pC_1^qC_2^p}{n^p}\right)^{1/(2p+q)}\right)\,,
 \end{align*}
 where $D=\sup_{x,y\in \cK} \DR(x,y)$.
 
For the class $\F_{L,\mu}$ of $\mu$-strongly convex and $L$-smooth functions, with $\mu > \dfrac{2L}{\alpha}$, 
$
 \eta_t = \dfrac{2}{\mu t} \,,
$ and
%\[
%\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n} \,,
%\]
\[
\delta^{p+q} =  \dfrac{C_2\left( \log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)} \,,
\]
the minimax error satisfies 
 \begin{align*}
\Delta_n^*(\cF,c_1,c_2)= O\left( \left(\dfrac{C_1^q C_2^p}{n^p} \right)^{1/(p+q)} \right)\,.
 \end{align*}
\end{theorem}
\begin{proof}
See Section \ref{sec:ub-proof}.
\end{proof}


\begin{theorem}[\textit{Lower bound}]
\label{thm:lb-convex}
Let $n>0$ be an integer, $p,q>0$, $C_1,C_2>0$, \todoc{Can we have $q=0$?}
$\cK\subset \R^d$ convex, closed, with  $\{+1,-1\}^d\subset \cK$ and let $L>0$.
Assume that $n$ is large enough (the exact condition is given after the theorem). \todoc{Maybe in the appendix.}
%For any $v \in \{+1,-1\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
%$f^{(i)}_{v_i}(x_i) := \dfrac{\epsilon}{2} (x_i - v_i)^2$, for $i=1,\ldots,d$.
%Consider the space of functions $\F:= \{f_v \mid v  \in \{+1,-1\}^d\}$, with $\epsilon = d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where \\$K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$. 
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 the minimax error \eqref{eq:minimaxerrdef} satisfies the following bounds:\\
\textbf{Class $\F_{L,0}(\K)$ (Convex and smooth):}
% Consider the class $\F_{L,0}$ of functions with domain $\cK$. \todoc{Probably the notation should be $\F_{L,0}(\cK)$?} 
\[
\Delta_n^{*}(\F_{L,0}, c_1,c_2) \ge \dfrac{1}{4}d^{\frac{q}{(4p+2q)}}\left(\dfrac{1}{2 K_1 \sqrt n}\right)^{\frac{p}{p+\frac{q}{2}}},
\]
where $K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.

\textbf{$\F_{L,1}(\K)$ ($1$-strongly convex and smooth):}
% For the class $\F_{L,1}$ of functions with domain $\K$, the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}, 
\]
where $K_1$ is the same as before.
\end{theorem}
The condition connecting the problem parameters is that $n$ should be large enough so that\\
\begin{inparaenum}[\bfseries (i)]
\item $d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}\le L$ for $\F_{L,0}(\K)$ and\\
\item $d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}} \le L$ for $\F_{L,1}(\K)$.
\end{inparaenum}
% where $K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.
\begin{proof}
 See Section \ref{sec:lb-proof}.
\end{proof}

% \begin{theorem}[\textit{Lower bound: Strongly convex}]
% \label{thm:lb-strongly-convex}
% For any $v \in \{+\nu,-\nu\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
% $f^{(i)}_{v_i}(x_i) := \dfrac{1}{2} x^2 - v x$, for $i=1,\ldots,d$.
% Consider the space of functions $\F:= \{f_v \mid v  \in \{+\nu,-\nu\}^d\}$, with $\nu = d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where $K_1$ is as defined in Theorem \ref{thm:lb-convex}. 
% Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
%  with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
%  the minimax error \eqref{eq:minimaxerrdef} satisfies
% \[
% \Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}.
% \]
% \end{theorem}
% \begin{proof}
%  See Appendix \ref{sec:appendix-lbscconvex}.
% \end{proof}


% \todop{Extend the result to the case when the algorithm can choose $\delta$ in every step - will update the proof for adaptive $\delta$ soon}



Todo: Online learning is at least as hard as optimization. Hence, we get a regret lower bound from this, too.

Discussion: The functions are smooth and strongly convex.
However, their strong convexity constant converges to zero as $n\to\infty$.

Todo: What can we show for strongly convex functions?

Of course, the same result holds in $d$ dimensions (add details: why?). However, one expects the lower bound to scale linearly with the dimension and we were not able to derive this so far.

Constrained case:

Upper bound: 

Reductions:
Between oracles;
Optimization and cumulative regret minimization;


