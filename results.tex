%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)

Unconstrained case:

Upper bound: 

In this section we prove a upper regret bound with a fixed, smooth objective function. 
The algorithm $\A$ we choose to interact with Type-I oracle is Mirror Descent (MD).

Let the regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$ be such that $\interior(\mathcal{K})\subseteq \dom(\mathcal{R})$. Fix an $L$-smooth, convex loss function $f: \mathcal{K} \to \mathbb{R}$. 
Recall the MD algorithm: We let $x_1\in \cK$ be arbitrary,
and for $t\ge 1$, 
\begin{align*}
x_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,x_t) \right] \,,
\end{align*}
where $D_{\mathcal{R}}$ denotes the Bregman divergence w.r.t. $\mathcal{R}$ and $G_t \in \R^d$.
We assume that $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\norm{\cdot}$. 

When $G_t$ is a noisy, biased estimate of $\nabla f(x_t)$, we have the following lemma.
\begin{lemma}
Let $(\cF_t)_{t}$ be a filtration such that $x_t$ is $\cF_t$-measurable.
Let $g_t = \EE{G_t|\cF_t}$ 
and assume that the nonnegative real-valued deterministic sequence $(\beta_t)_{1\le t\le n}$ is such that 
$\norm{g_t - \nabla f(x_t)}_* \le \beta_t$ holds almost surely. 
Further, assume that $\sup_{x,y\in \cK} D_{\mathcal{R}}(x,y) \le D$ and let $\eta_t = \frac{\alpha}{a_t+L}$ for some increasing 
sequence $(a_t)_{t=1}^{n-1}$ of numbers. Then, 
\begin{align*}
\MoveEqLeft \EE{ \sum_{t=1}^n f(x_t) - f(x) }  \\
\le& 	 \EE{f(x_1)-f(x)}+ \\
 & \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t 
 +\frac{D(a_{n-1}+L)}{\alpha} +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,,
\end{align*}
where $\sigma_t^2 = \EE{ \norm{G_t-g_t}^2}$ is the ``variance'' of $G_t$.
\end{lemma}
This is essentially Theorem~C.4 of \cite{MahdaviPhd:2014}, which is identical to Theorem~6.3 of \cite{Bu:Convex14}, who cites \cite{Dekel:minibatch12} as the source. These results assume that $\beta_t=0$ above.

Next, we consider the minimax regret of MD algorithm interacting with Type-I oracle.
\begin{theorem}
Given a bounded, convex set $\cK \subset \R$,
let $\cF$ be a class of convex functions on $\cK$ such that $f \in \cF$ is $L$-smooth.
For any $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$, the expected minimax regret \eqref{eq:minimaxregdef} can be bounded as
 \begin{align*}
\dfrac{1}{n} \mathbb{E}\left[ R_n^* \right](\cF,c_1,c_2) \leq O(n^{-p/(2p+q)})\,.
 \end{align*}\todox{The regret is actually w.r.t. a fixed $f$, modify definition of minimax regret or define a new minimax regret here?}
\end{theorem}

Lower bound:
\begin{theorem}
Let $\cK\subset \R$ be an interval such that contains the interval $[-1,+1]$.
 For any $n$ and any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 for any space of functions $\F$ with $\{f_{+1}, f_{-1} \}\subset \F$, 
 where $f_v:= \dfrac{\epsilon}{2} (x - v)^2$, $x\in \cK$, $v\in \{\pm 1\}$,
and $\epsilon = \left(\frac{\delta^{-q/2}C_2}{4\sqrt{n}} + \frac{C_1\delta^p}{2}\right)$ \todoc{Cannot depend on $\delta$! We will plug in $\delta^*$. Then we need to recheck the end of the proof, it should work.}
 the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^*(\F, c_1,c_2) \ge \dfrac{C_3}{ n^{1/4}}, \text{ when } p=1, q=2\,; 
\]
 \[
 \Delta_n^*(\F, c_1,c_2) \ge \dfrac{C_4}{ n^{1/3}},  \text{ when } p=q=2.
 \]
\end{theorem}
\todop[inline]{work out $C_3$ and $C_4$.}
\begin{proof}
 See Section \ref{sec:lb-proof}.
\end{proof}
Todo: Extend the result to the case when the algorithm can choose $\delta$ in every step.

Todo: Online learning is at least as hard as optimization. Hence, we get a regret lower bound from this, too.

Discussion: The functions are smooth and strongly convex.
However, their strong convexity constant converges to zero as $n\to\infty$.

Todo: What can we show for strongly convex functions?

Of course, the same result holds in $d$ dimensions (add details: why?). However, one expects the lower bound to scale linearly with the dimension and we were not able to derive this so far.

Constrained case:

Upper bound: 

Reductions:
Between oracles;
Optimization and cumulative regret minimization;


