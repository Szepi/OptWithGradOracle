%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)

\begin{table*}
\centering
 \caption{Summary of upper and lower bounds for various function classes and different gradient oracles}
\label{tab:mse-1}
\begin{tabular}{|c|c|c|c|c|}
\toprule
\rowcolor{gray!20}
% \multicolumn{3}{|c|}{\multirow{2}{*}{\textbf{Lower bounds}}}\\[1.7em]
% \midrule
  \multirow{2}{*}{\textbf{Oracle type}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Convex}}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Strongly Convex}}} \\[1.7em]
 \midrule
\multirow{2}{*}{ $C_1=0$ and $q=0$} & \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d}{\sqrt{n}}\right)$} & \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d}{\sqrt{n}}\right)$}  \\[1.5em]\midrule
%%%%%%%%%%%%%%%%
\multirow{2}{*}{ $p=1$ and $q=2$} & \multirow{2}{*}{$\O\left(\dfrac{d^{\frac{1}{4}}}{n^{\frac{1}{4}}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d^\frac{1}{4}}{n^\frac{1}{4}}\right)$}& \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d^\frac{1}{4}}{\sqrt{n}}\right)$} \\[1.5em]\midrule
%%%%%%%%%%%%%%%%%%%
\multirow{2}{*}{ $p=2$ and $q=2$} & \multirow{2}{*}{$\O\left(\dfrac{d^{\frac{1}{6}}}{n^{\frac{1}{3}}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d^\frac{1}{6}}{n^\frac{1}{3}}\right)$} & \multirow{2}{*}{$\O\left(\dfrac{d}{\sqrt{n}}\right)$}  & \multirow{2}{*}{$\Omega\left(\dfrac{d^\frac{1}{6}}{\sqrt{n}}\right)$}\\[1.5em]
 \bottomrule 
\end{tabular}
\end{table*}

Unconstrained case:

Upper bound: 

\begin{algorithm}
	\caption{Mirror Descent with Type-I Oracle}\label{alg}
	\textbf{Input}: Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$. \\
	Initialize $X_1\in \cK$ arbitrarily.\\
	In round $t=1, 2, \cdots, n-1$:
	\begin{itemize}
	\item Query the oracle at $X_t$.
	\item Receive $G_t$, $Y_t$.
	\item Update
	\[
	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
	\]
	\end{itemize}
	\textbf{Output}: the optimizer
	\[
	\hat{X}_n = \dfrac{1}{n}\sum_{t=1}^n X_t \,.
	\]
	
\end{algorithm}

\begin{theorem}
\label{thm:ub}
Given a bounded, convex set $\cK \subset \R$,
let $\cF$ be a class of convex functions on $\cK$ such that $f \in \cF$ is $L$-smooth.
Assume further that the regularization function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\norm{\cdot}$, and $\interior(\mathcal{K})\subseteq \dom(\mathcal{R})$.
For any $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$, 
 if \cref{alg} is run with 
 \[
 \eta_t = \dfrac{\alpha}{a_t+L} \,,
 \]
 \[
 \delta = \left( \dfrac{C_2}{2aC_1}\sqrt{\dfrac{\alpha}{2D}} \right)^{1/(p+q)}n^{-1/(2p+q)} \,,
 \]
 where
 $a = \left( \sqrt{\dfrac{2^{3q}\alpha}{D}}C_1^q C_2^p \right)^{1/(2p+q)}$,
 $a_t = a t^{(p+q)/(2p+q)}$, for $t=1, 2, \cdots, n-1$,
  the minimax error \eqref{eq:minimaxerrdef} can be bounded as
 \begin{align*}
\Delta_n^*(\cF,c_1,c_2)= O(n^{-p/(2p+q)})\,,
 \end{align*}
 where $D=\sup_{x,y\in \cK} \DR(x,y)$. $\DR$ denotes the Bregman divergence w.r.t. $\mathcal{R}$.
\end{theorem}
\begin{proof}
See Section \ref{sec:ub-proof}.
\end{proof}


Lower bound:
\begin{theorem}
Let $\cK\subset \R$ be an interval such that contains the interval $[-1,+1]$.
 For any $n$ and any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 for any space of functions $\F$ with $\{f_{+1}, f_{-1} \}\subset \F$, 
 where $f_v:= \dfrac{\epsilon}{2} (x - v)^2$, $x\in \cK$, $v\in \{\pm 1\}$,
and $\epsilon = \left(\frac{\delta^{-q/2}C_2}{4\sqrt{n}} + \frac{C_1\delta^p}{2}\right)$ \todoc{Cannot depend on $\delta$! We will plug in $\delta^*$. Then we need to recheck the end of the proof, it should work.}
 the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^*(\F, c_1,c_2) \ge \dfrac{C_3}{ n^{1/4}}, \text{ when } p=1, q=2\,; 
\]
 \[
 \Delta_n^*(\F, c_1,c_2) \ge \dfrac{C_4}{ n^{1/3}},  \text{ when } p=q=2.
 \]
\end{theorem}
\todop[inline]{work out $C_3$ and $C_4$.}
\begin{proof}
 See Section \ref{sec:lb-proof}.
\end{proof}
Todo: Extend the result to the case when the algorithm can choose $\delta$ in every step.

Todo: Online learning is at least as hard as optimization. Hence, we get a regret lower bound from this, too.

Discussion: The functions are smooth and strongly convex.
However, their strong convexity constant converges to zero as $n\to\infty$.

Todo: What can we show for strongly convex functions?

Of course, the same result holds in $d$ dimensions (add details: why?). However, one expects the lower bound to scale linearly with the dimension and we were not able to derive this so far.

Constrained case:

Upper bound: 

Reductions:
Between oracles;
Optimization and cumulative regret minimization;


