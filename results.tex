%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)

Unconstrained case:

Upper bound: 

Lower bound:
\begin{theorem}
Let $\cK\subset \R$ be an interval such that contains the interval $[-1,+1]$.
 For any $n$ and any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 for any space of functions $\F$ with $\{f_{+1}, f_{-1} \}\subset \F$, 
 where $f_v:= \dfrac{\epsilon}{2} (x - v)^2$, $x\in \cK$, $v\in \{\pm 1\}$,
and $\epsilon = \left(\frac{\delta^{-q/2}C_2}{4\sqrt{n}} + \frac{C_1\delta^p}{2}\right)$ \todoc{Cannot depend on $\delta$! We will plug in $\delta^*$. Then we need to recheck the end of the proof, it should work.}
 the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^*(\F, c_1,c_2) \ge \dfrac{C_3}{ n^{1/4}}, \text{ when } p=1, q=2\,; 
\]
 \[
 \Delta_n^*(\F, c_1,c_2) \ge \dfrac{C_4}{ n^{1/3}},  \text{ when } p=q=2.
 \]
\end{theorem}
\todop[inline]{work out $C_3$ and $C_4$.}
\begin{proof}
 See Section \ref{sec:lb-proof}.
\end{proof}
Todo: Extend the result to the case when the algorithm can choose $\delta$ in every step.

Todo: Online learning is at least as hard as optimization. Hence, we get a regret lower bound from this, too.

Discussion: The functions are smooth and strongly convex.
However, their strong convexity constant converges to zero as $n\to\infty$.

Todo: What can we show for strongly convex functions?

Of course, the same result holds in $d$ dimensions (add details: why?). However, one expects the lower bound to scale linearly with the dimension and we were not able to derive this so far.

Constrained case:

Upper bound: 

Reductions:
Between oracles;
Optimization and cumulative regret minimization;


