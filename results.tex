%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
\begin{table*}
\centering
 \caption{Summary of upper and lower bounds for different smooth function classes and  gradient oracles}
\label{tab:mse-1}
 \begin{tabular}{|c|c|c|c|c|}
% \begin{tabular}{||*5{>{\columncolor[gray]{.9}}c}||}
\toprule
% \rowcolor{gray!20}
% \multicolumn{3}{|c|}{\multirow{2}{*}{\textbf{Lower bounds}}}\\[1.7em]
% \midrule
  \multirow{2}{*}{\textbf{Oracle type}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Convex + Smooth}}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Strongly Convex + Smooth}}} \\[1em]
 \midrule
% \rowcolor{gray!20}
 & \textbf{Upper bound} & \textbf{Lower bound} & \textbf{Upper bound} & \textbf{Lower bound}\\
 \midrule
 \textbf{Zero bias} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\left(\dfrac{d^2}{n}\right)^{1/2}$}  \\[0.5ex]
 ($C_1=0$, $q=0$) & & & &\\\midrule
%%%%%%%%%%%%%%%%
\textbf{ One-point estimates} & \multirow{2}{*}{$\dfrac{1}{n^{{1/4}}}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2^2 d}{n}\right)^{1/4}$}& \multirow{2}{*}{$\dfrac{1}{n^{1/3}}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2^2}{d n}\right)^{1/2}$} \\[0.5ex]
 ($p=1$, $q=2$) & & & &\\\midrule
%%%%%%%%%%%%%%%%%%%
\textbf{ Two-point estimates} & \multirow{2}{*}{$\dfrac{1}{n^{{1/3}}}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2^2 \sqrt d}{n}\right)^{1/3}$} & \multirow{2}{*}{$\dfrac{1}{n^{1/2}}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2^2 }{d n}\right)^{2/3}$}\\[1.4ex]
 ($p=2$, $q=2$) & & & &\\
 \bottomrule 
\end{tabular}
\end{table*}
\todox{Update upper bounds in the table}

Unconstrained case:

Upper bound: 

\begin{algorithm}
	\caption{Mirror Descent with Type-I Oracle}\label{alg}
	\textbf{Input}: Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$. \\
	Initialize $X_1\in \cK$ arbitrarily.\\
	In round $t=1, 2, \cdots, n-1$:
	\begin{itemize}
	\item Query the oracle at $X_t$.
	\item Receive $G_t$, $Y_t$.
	\item Update
	\[
	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
	\]
	\end{itemize}
	\textbf{Output}: the optimizer
	\[
	\hat{X}_n = \dfrac{1}{n}\sum_{t=1}^n X_t \,.
	\]
	
\end{algorithm}

\begin{theorem}
\label{thm:ub}
Given a bounded, convex set $\cK \subset \R$,
let $\cF$ be a class of convex functions on $\cK$ such that $f \in \cF$ is $L$-smooth.
Assume further that the regularization function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\norm{\cdot}$, and $\interior(\mathcal{K})\subseteq \dom(\mathcal{R})$.
For any $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$, 
 if \cref{alg} is run with 
 \[
 \eta_t = \dfrac{\alpha}{a_t+L} \,,
 \]
 \[
 \delta = \left( \dfrac{C_2}{2aC_1}\sqrt{\dfrac{\alpha}{2D}} \right)^{1/(p+q)}n^{-1/(2p+q)} \,,
 \] 
 \todox{the $n$ should actually be $(n-1)$, the expression will be more natural if we start from $0$.}
 where
 $a = \left( \sqrt{\dfrac{2^{3q}\alpha}{D}}C_1^q C_2^p \right)^{1/(2p+q)}$,
 $a_t = a t^{(p+q)/(2p+q)}$, for $t=1, 2, \cdots, n-1$,
  the minimax error \eqref{eq:minimaxerrdef} can be bounded as
 \begin{align*}
\Delta_n^*(\cF,c_1,c_2)= O(n^{-p/(2p+q)})\,,
 \end{align*}
 where $D=\sup_{x,y\in \cK} \DR(x,y)$. $\DR$ denotes the Bregman divergence w.r.t. $\mathcal{R}$.
 
If the class of functions is also $\mu$-strongly convex w.r.t. $\mathcal{R}$. Choose
\[
 \eta_t = \dfrac{2}{\mu t} \,,
\]
\[
\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n} \,,
\]
the upper bound of minimax error is then
 \begin{align*}
\Delta_n^*(\cF,c_1,c_2)= O(n^{-p/(p+q)})\,.
 \end{align*}
\end{theorem}
\begin{proof}
See Section \ref{sec:ub-proof}.
\end{proof}


Lower bound:
\begin{theorem}
\label{thm:lb-convex}
For any $v \in \{+1,-1\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
$f^{(i)}_{v_i}(x_i) := \dfrac{\epsilon}{2} (x_i - v_i)^2$, for $i=1,\ldots,d$.
Consider the space of functions $\F:= \{f_v \mid v  \in \{+1,-1\}^d\}$, with $\epsilon = d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where \\$K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$. 
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}d^{\frac{q}{(4p+2q)}}\left(\dfrac{1}{2 K_1 \sqrt n}\right)^{\frac{p}{p+\frac{q}{2}}}.
\]
\end{theorem}
\begin{proof}
 See Section \ref{sec:lb-proof} for a proof sketch and Appendix \ref{sec:appendix-lbconvex} for a detailed proof.
\end{proof}

Lower bound: Strongly convex case
\begin{theorem}
\label{thm:lb-strongly-convex}
For any $v \in \{+\nu,-\nu\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
$f^{(i)}_{v_i}(x_i) := \dfrac{1}{2} x^2 - v x$, for $i=1,\ldots,d$.
Consider the space of functions $\F:= \{f_v \mid v  \in \{+\nu,-\nu\}^d\}$, with $\nu = d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where $K_1$ is as defined in Theorem \ref{thm:lb-convex}. 
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}.
\]
\end{theorem}
\begin{proof}
 See Section \ref{sec:lb-proof} for a proof sketch and Appendix \ref{sec:appendix-lbscconvex} for a detailed proof.
\end{proof}


\todop{Extend the result to the case when the algorithm can choose $\delta$ in every step - will update the proof for adaptive $\delta$ soon}



Todo: Online learning is at least as hard as optimization. Hence, we get a regret lower bound from this, too.

Discussion: The functions are smooth and strongly convex.
However, their strong convexity constant converges to zero as $n\to\infty$.

Todo: What can we show for strongly convex functions?

Of course, the same result holds in $d$ dimensions (add details: why?). However, one expects the lower bound to scale linearly with the dimension and we were not able to derive this so far.

Constrained case:

Upper bound: 

Reductions:
Between oracles;
Optimization and cumulative regret minimization;


