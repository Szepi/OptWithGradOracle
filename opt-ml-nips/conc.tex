%!TEX root =  bgo-opt-ml-nips.tex
% please do not delete or change the first line (needed by Csaba's editor)

We presented a general noisy  gradient oracle for model for convex optimization. The oracle model covers all gradient estimation methods in the literature designed for algorithms that can observe only noisy function values, while allowing to handle explicitly the bias-variance tradeoff of these estimators. In the smooth convex setting, the framework allows to derive sharp upper and lower bounds on the minimax optimization error, reducing the optimization problem to study properties of gradient estimators.

%\section{Conclusions}
%In this paper, motivated by the quest for improving the rates in smooth stochastic bandit convex optimization,
%we proposed a new convex optimization framework where the optimization algorithm interacts with the objective function
%through a gradient evaluation oracle with a controllable bias-variance tradeoff. Our main results establish matching lower and upper bounds for the optimization error in this model. The application of this result with the best known gradient estimation techniques suggests that already for the smooth-convex case, the algorithms that fit our model (which is essentially \emph{all} algorithms considered in the literature so far) cannot achieve the optimal rate for this problem.
%One interesting open question is whether the results about the bias-variance tradeoff for the gradient estimation techniques available today are unimprovable.
