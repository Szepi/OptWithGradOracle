%!TEX root =  bgo-opt-ml-nips.tex
% please do not delete or change the first line (needed by Csaba's editor)
Let us now consider how our result can be applied to stochastic bandit convex optimization. 
In particular, we consider the case when in round $t$ the algorithm upon querying at $X_t$ gets
the observation $Z_t = f(X_t) + \xi_t$, where (for simplicity) $(\xi_t)$ is an i.i.d. noise sequence (zero mean, subgaussian tail).
We consider the case when the objective function is smooth and convex.
Using the so-called smoothing technique \citep{PoTsy90,flaxman2005online,HaLe14:SOC}, one can obtain
type-II oracles. For example, the oracle described by \citet{flaxman2005online} (and used by many subsequent works)
when queried at point $x$ with accuracy parameter $\delta$ queries $f$ at $x+\delta U$ to get
$Z=f(x+\delta U)+\xi$, which is used to construct the gradient estimate $G = \frac{d Z}{\delta} U$.
One can then show that if $f$ is convex and smooth, this is a type-II oracle, and in fact a type-IIb oracle, as well, with 
$c_1(\delta)=C_1 \delta^2$, $c_2(\delta) = C_2/\delta^2$ with some specific numeric constants $C_1$ and $C_2$.
Since every type-IIb oracle is also a type-I oracle, we get that this is also a type-I oracle.
Now, plugging in $p=q=2$ into the lower bound \cref{thm:lb-convex}, we get that \emph{no algorithm using this type, or an
equivalent oracle can get better rate than $\Omega(n^{-1/3})$} (our upper bound from \cref{thm:ub} matches this rate,
replicating the result of \citet{saha2011improved}).
In particular, the only hope to improve this rate, \emph{while staying within the realm of our oracle model}, is to improve the bias-variance properties of the oracle (either by proving a better bound, or constructing better oracles).
One case when this is possible is when the noise in the function evaluation oracle is under control
(e.g., \citep{Ne11:TR,duchi2015optimal}). \todoc{Another case is when the objective is linear.}
In this case, for smooth convex objectives, 
one can use two observations (reducing the sample size by a factor of two only, i.e., in a negligible manner) 
to ``cancel the noise out''  to get an oracle with bias $c_1(\delta) = C_1\delta$ 
and a constant variance (i.e., $p=1$, $q=0$). Plugging these values into our upper and lower bounds, we get the optimal $\Theta(n^{-1/2})$ rate, replicating the results \citet{Ne11:TR,duchi2015optimal}.
For strongly and smooth convex functions, our upper bound from  \cref{thm:ub}  also gives the optimal rate,
replicating the result of \citet{HaLe14:SOC} (when not considering the constraint set).
By our earlier remarks, our lower bound also provide lower bounds on the regret. In particular,
our $\Omega(n^{-1/3})$ lower bound for the smooth convex case transfers into an $\Omega(n^{2/3})$ lower bound
for the regret.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bgo"
%%% End:
