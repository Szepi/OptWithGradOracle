%!TEX root =  bgo-arxiv.tex
% please do not delete or change the first line (needed by Csaba's editor)
\begin{table*}
\small
\centering
 \caption{Summary of upper and lower bounds for different smooth function classes and  gradient oracles for the settings of \cref{thm:ub} and \cref{thm:lb-convex}. Note that when $\cR$ is the squared norm and $\K$ is the hypercube (as in the lower bounds), $D=\theta(d)$ in the upper bounds.}
\label{tab:mse-1}
 \begin{tabular}{|c|c|c|c|c|}
% \begin{tabular}{||*5{>{\columncolor[gray]{.9}}c}||}
\toprule
% \rowcolor{gray!20}
% \multicolumn{3}{|c|}{\multirow{2}{*}{\textbf{Lower bounds}}}\\[1.7em]
% \midrule
  \multirow{2}{*}{\textbf{Oracle type}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Convex + Smooth}}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Strongly Convex + Smooth}}} \\[1em]
 \midrule
% \rowcolor{gray!20}
 & \textbf{Upper bound} & \textbf{Lower bound} & \textbf{Upper bound} & \textbf{Lower bound}\\
 \midrule
%\textbf{ $\delta$-bias, $\delta^{-2}$-variance} & \multirow{2}{*}{$\left(\dfrac{C_1^{2}C_2 D}{n}\right)^{1/4}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2^2 d}{n}\right)^{1/4}$}& \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2^2}{d n}\right)^{1/2}$} \\[0.5ex]
% ($p=1$, $q=2$) & & & &\\\midrule
%%%%%%%%%%%%%%%%%%%
\textbf{$\delta^2$-bias, $\delta^{-2}$-variance } & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 D}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2^2 \sqrt d}{n}\right)^{1/3}$} & \multirow{2}{*}{$\left(\dfrac{C_1 C_2}{n}\right)^{1/2}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2^2 }{d n}\right)^{2/3}$}\\[1.4ex]
 ($p=2$, $q=2$) & & & &\\
%  \midrule
% \textbf{No bias or variance} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\left(\dfrac{d^2}{n}\right)^{1/2}$}  \\[0.5ex]
% ($C_1=0$, $C_2=0$) & & & &\\
\bottomrule
%%%%%%%%%%%%%%%%
\end{tabular}
\end{table*}


In this section we provide our main results in forms of upper and lower bounds on the minimax error.
First we provide an upper bound by analyzing a mirror-descent algorithm given in \cref{alg}.
In the algorithm, we assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex and the target function $f$ is smooth or  smooth and strongly convex.
% In the algorithm, we will assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\|\cdot\|$ on $\R^d$, that is $\tfrac{\alpha}{2} \|x-y\|^2 \le \DR(x,y)=f(x)-f(y)-\ip{\nabla f(y),x-y}$ for all $x,y \in \K$. Moreover, we will consider the cases when the target function $f$ is $L$-smooth w.r.t. the norm $\| \cdot \|$ for some $L>0$, that is, $f(x) \le f(y) + \ip{\nabla f(y), y-x} + \tfrac{L}{2} \|x-y\|^2$, and when $f$ is $\mu$-strongly convex w.r.t. $\cR$, that is,
% $f(x) \ge f(y) + \ip{\nabla f(y),y-x} +\tfrac{\mu}{2} \DR(x,y)$ for all $x,y \in \K$.

%Unconstrained case:
%
%Upper bound:

\begin{algorithm}[t]
\begin{algorithmic}
    \State {\bf Input:}  Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$.
%     In round $t=1, 2, \cdots, n-1$:
\State Initialize $X_1\in \cK$ arbitrarily.
\For{$t=1, 2, \cdots, n-1$}
	\State Query the oracle at $X_t$.
	\State Receive $G_t$, $Y_t$.
	\State Update
	$
	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
	$
\EndFor
\State {\bf Return:} $\hat{X}_n = \frac{1}{n}\sum_{t=1}^n X_t \,.$
\end{algorithmic}
\caption{Mirror Descent with Type-I/II Oracle.}
\label{alg}
\end{algorithm}


\begin{theorem}[\textit{\textbf{Upper bound}}]
\label{thm:ub}
Assume that the regularization function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t. some norm $\norm{\cdot}$, and $\interior(\mathcal{K})\subseteq \dom(\mathcal{R})$.
Then, for any $(c_1,c_2)$ Type-II oracle
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 Algorithm \cref{alg} run with appropriate $\eta_t, \delta$ that depend on $p, q, C_1, C_2$ and $L$, satisfies the following bounds:
%  $\eta_t= \dfrac{\alpha}{a_t+L}$ and
%  $
%  \delta = \left( \dfrac{C_2}{4aC_1}\dfrac{2p+q}{p}\dfrac{n}{n+1}\right)^{1/(p+q)}n^{-1/(2p+q)} \,,$  where
%  $a^{2p+q} =2^{q-p}\left( 2+\dfrac{q}{p} \right)^p\left(1+ \dfrac{1}{n} \right)^q \left( \dfrac{\alpha}{D} \right)^{p+q}C_1^q C_2^p $,
%  $a_t = a t^{(p+q)/(2p+q)}$, for $t=1, 2, \cdots, n-1$,
% then  the minimax error \eqref{eq:minimaxerrdef} can be bounded as
%  \begin{align*}
%  \Delta_n^*(\cF,c_1,c_2)= O\left(\left(\dfrac{D^pC_1^qC_2^p}{n^p}\right)^{1/(2p+q)}\right)\,,
%  \end{align*}
%  where $D=\sup_{x,y\in \cK} \DR(x,y)$.
%
% For the class $\F_{L,\mu,\cR}$ of $\mu$-strongly convex (w.r.t. $\cR$) and $L$-smooth functions, with $\mu > \dfrac{2L}{\alpha}$,
% $
%  \eta_t = \dfrac{2}{\mu t} \,,
% $ and
% %\[
% %\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n} \,,
% %\]
% \[
% \delta^{p+q} =  \dfrac{C_2\left( \log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)} \,,
% \]
% the minimax error satisfies
%  \begin{align*}
% \Delta_n^*(\cF,c_1,c_2)= O\left( \left(\dfrac{C_1^q C_2^p}{n^p} \right)^{1/(p+q)} \right)\,
%  \end{align*}
%  the minimax error \eqref{eq:minimaxerrdef} satisfies the following bounds:\\
% Consider the class $\F_{L,0}$ of functions with domain $\cK$. \todoc{Probably the notation should be $\F_{L,0}(\cK)$?}
\begin{align*}
&\textbf{$\bm{\F_{L,0}(\K)}$ (Convex and smooth):} \hspace{3em} \Delta_n^{*}(\F_{L,0}, c_1,c_2) \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
&\textbf{$\bm{\F_{L,1}(\K)}$ ($1$-strongly convex and smooth):} \hspace{1em}
\Delta_n^{*}(\F_{L,0}, c_1,c_2) \le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
where $D:=\sup_{x,y\in \cK} \DR(x,y)$, $K_1$ is a constant that depends on oracle parameters $p, q$ and strong convexity constant $\alpha$ of $\cal R$ and
where $K_2$ is a constant that depends on $p, q, \alpha$ and $\mu$.
\end{theorem}
\begin{proof}
See \cref{sec:appendix-md}.
\end{proof}

We next state lower bounds for both convex as well as strongly convex function classes. In particular, we observe that for convex and smooth functions the upper bound for the mirror descent scheme matches the lower bound, up to constants, whereas there is a gap for strongly convex+smooth functions.
Filling the gap is left for future work.
\begin{theorem}[\textit{\textbf{Lower bound}}]
\label{thm:lb-convex}
Let $n>0$ be an integer, $p>0, q\ge 2$, $C_1,C_2>0$,
$\cK\subset \R^d$ convex, closed, with  $\{+1,-1\}^d\subset \cK$ and let $L>0$.
Assume that $n$ is large enough (the exact condition is given after the theorem).
%For any $v \in \{+1,-1\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
%$f^{(i)}_{v_i}(x_i) := \dfrac{\epsilon}{2} (x_i - v_i)^2$, for $i=1,\ldots,d$.
%Consider the space of functions $\F:= \{f_v \mid v  \in \{+1,-1\}^d\}$, with $\epsilon = d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where \\$K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 the minimax error \eqref{eq:minimaxerrdef} satisfies the following bounds:\\
\textbf{$\bm{\F_{L,0}(\K)}$ (Convex and smooth):}
% Consider the class $\F_{L,0}$ of functions with domain $\cK$. \todoc{Probably the notation should be $\F_{L,0}(\cK)$?}
\[
\Delta_n^{*}(\F_{L,0}, c_1,c_2) \ge K_3 \left(\dfrac{d^{\frac{q}{2p}}  C_1^{\frac{q}{p}} C_2^2}{ n}\right)^{\frac{p}{2p+q}},
\]
where $K_3$ is a constant that depends on $p$ and $q$.\\
% = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.\\
\textbf{$\bm{\F_{L,1}(\K)}$ ($1$-strongly convex and smooth):}
% For the class $\F_{L,1}$ of functions with domain $\K$, the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_n^{*}(\F_{L,1}, c_1,c_2) \ge K_3^2  \left(\dfrac{C_1^{\frac{q}{p}} C_2^2 }{ {d n}}\right)^{\frac{2p}{2p+q}},
\]
where $K_4$ is a constant that depends on $p$ and $q$.
\end{theorem}
In the above, $K_3 = \dfrac{1}{4}\left(\dfrac{1}{2 K_4}\right)^{\frac{2p}{2p+q}}$, where $K_4 = \frac{p}{(p+\tfrac{q}{2})} \left(\frac{q}{2(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.
Further, the condition connecting the problem parameters is that $n$ should be large enough so that
\begin{enumerate}[\bfseries (i)]
\item $d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{ C_1^{\frac{q}{2p}} C_2 }{2\sqrt{n} K_4} \right)^{\frac{2p}{2p+q}}\le \min\left( \frac{C_1 (2p+q)}{q},  L\right)$ for $\F_{L,0}(\K)$ and
\item $d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{ C_1^{\frac{q}{2p}} C_2}{2\sqrt{n} K_4} \right)^{\frac{2p}{2p+q}} \le \min\left( \frac{C_1 (2p+q)}{q},  L\right)$ for $\F_{L,1}(\K)$.
\end{enumerate}
\begin{proof}
 See Section \ref{sec:appendix-lb-proof}.
\end{proof}

\begin{remark}(\textbf{\textit{Scaling}})
For any function class $\F$, by the definition of the minimax error \eqref{eq:minimaxerrdef}, it is easy to see that
$$\Delta_n^*(\mu \F, c_1,c_2) = \mu \Delta_n^*\left(\F, \frac{c_1}{\mu},\frac{c_2}{\mu^2}\right),$$
 where $\mu F$ denotes the function class comprised of functions in $\F$, each scaled by $\mu>0$. In particular, this relation implies that the bound for $\mu$-strongly convex function class is only a constant factor away from the bound for $1$-strongly convex function class.
\end{remark}

Table \ref{tab:oracles} presents the upper and lower bounds for two specific choices of $p$ and $q$ (relevant in applications, as we shall see later). These bounds can be inferred from the results in Theorems \ref{thm:ub}--\ref{thm:lb-convex}.
While it appears that for the strongly convex case the error becomes smaller with a larger dimension,
in most applications $C_1, C_2$ will hide dimension dependent constants
and as the lower bound actually increases with the dimension increasing.
Some specific examples will be discussed in the next section.
% \begin{theorem}[\textit{Lower bound: Strongly convex}]
% \label{thm:lb-strongly-convex}
% For any $v \in \{+\nu,-\nu\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
% $f^{(i)}_{v_i}(x_i) := \dfrac{1}{2} x^2 - v x$, for $i=1,\ldots,d$.
% Consider the space of functions $\F:= \{f_v \mid v  \in \{+\nu,-\nu\}^d\}$, with $\nu = d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where $K_1$ is as defined in Theorem \ref{thm:lb-convex}.
% Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle
%  with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
%  the minimax error \eqref{eq:minimaxerrdef} satisfies
% \[
% \Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}.
% \]
% \end{theorem}
% \begin{proof}
%  See Appendix \ref{sec:appendix-lbscconvex}.
% \end{proof}


% \todop{Extend the result to the case when the algorithm can choose $\delta$ in every step - will update the proof for adaptive $\delta$ soon}
