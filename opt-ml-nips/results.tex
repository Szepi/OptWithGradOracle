%!TEX root =  bgo-opt-ml-nips.tex
% please do not delete or change the first line (needed by Csaba's editor)

In this section we provide our main results in forms of upper and lower bounds on the minimax error.
%First we provide an upper bound by analyzing a mirror-descent algorithm given in \cref{alg}.
%In the algorithm, we assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex and the target function $f$ is smooth or  smooth and strongly convex.
\begin{theorem}[\textit{\textbf{Upper bound}}]
\label{thm:ub}
For any $(c_1,c_2)$ Type-II oracle returning $\tilde{f}$ which inherits the properties of the objective function $f$,
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
the mirror descent algorithm \citep{NeYu83}
with a $\alpha$-strongly convex (w.r.t. some norm $\norm{\cdot}$) regularizer $\mathcal{R}$ such that $\interior(\mathcal{K})\subseteq \dom(\mathcal{R})$,
satisfies the following bounds under appropriate choices for the learning rate $\eta_t$, and the accuracy parameter $\delta$ (these depend on $p, q, C_1, C_2$ and $L$):
\vspace*{-0.17in}
\begin{align*}
&\textbf{$\bm{\F_{L,0}(\K)}$ (Convex and $L$-smooth):} \hspace{5.5em} \Delta_n^{*}(\F_{L,0}, c_1,c_2) \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
&\textbf{$\bm{\F_{L,\mu}(\K)}$ ($\mu$-strongly convex and $L$-smooth):} \hspace{1em}
\Delta_n^{*}(\F_{L,\mu}, c_1,c_2) \le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
where $D:=\sup_{x,y\in \cK} \DR(x,y)$, $K_1$ is a constant that depends on oracle parameters $p, q$ and strong convexity constant $\alpha$ of $\cal R$ and
where $K_2$ is a constant that depends on $p, q, \alpha$ and $\mu$.
\end{theorem}
We next state lower bounds for both convex as well as strongly convex function classes. In particular, we observe that for convex and smooth functions the upper bound for the mirror descent scheme matches the lower bound, up to constants, whereas there is a gap for strongly convex and smooth functions.
Filling the gap is left for future work.
\begin{theorem}[\textit{\textbf{Lower bound}}]
\label{thm:lb-convex}
Let $n>0$ be an integer, $p,q >0$, $C_1,C_2>0$,
$\cK\subset \R^d$ convex, closed, with  $\{+1,-1\}^d\subset \cK$ and let $L>0$.
Assume that $n$ is large enough (the exact condition is given after the theorem).
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
 the minimax error \eqref{eq:minimaxerrdef} satisfies the following bounds:
 \vspace*{-0.1in}
\begin{align*}
&\textbf{$\bm{\F_{L,0}(\K)}$ (Convex and $L$-smooth):} \hspace{5.5em} \Delta_n^{*}(\F_{L,0}, c_1,c_2) \ge K_3 \left(\dfrac{d^{\frac{q}{2p}}  C_1^{\frac{q}{p}} C_2^2}{ n}\right)^{\frac{p}{2p+q}},\\
&\textbf{$\bm{\F_{L,1}(\K)}$ ($1$-strongly convex and $L$-smooth):} \hspace{1em}
\Delta_n^{*}(\F_{L,1}, c_1,c_2) \ge K_3^2  \left(\dfrac{C_1^{\frac{q}{p}} C_2^2 }{ {d n}}\right)^{\frac{2p}{2p+q}},
\end{align*}
where $K_3 = \frac{1}{4}\left(\dfrac{1}{2 K_4}\right)^{\frac{2p}{2p+q}}$ and $K_4 = \frac{p}{(p+\tfrac{q}{2})} \left(\frac{q}{2(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.
Further, the condition connecting the problem parameters is that $n$ should be large enough so that
\begin{inparaenum}[\bfseries (i)]
\item $d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{ C_1^{\frac{q}{2p}} C_2 }{2\sqrt{n} K_4} \right)^{\frac{2p}{2p+q}}\le \min\left( \frac{C_1 (2p+q)}{q}, \frac{2z+1}{C_1^{2z-1}(z+1)^z},  L\right)$ with $z=p/q$, for $\F_{L,0}(\K)$ and
\item $d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{ C_1^{\frac{q}{2p}} C_2}{2\sqrt{n} K_4} \right)^{\frac{2p}{2p+q}} \le \min\left( \frac{C_1 (2p+q)}{q}, \frac{2z+1}{C_1^{2z-1}(z+1)^z},  L\right)$  for $\F_{L,1}(\K)$.
\end{inparaenum}
\end{theorem}
\begin{remark}(\textbf{\textit{Scaling}})
For any function class $\F$, by the definition of the minimax error \eqref{eq:minimaxerrdef}, it is easy to see that
$\Delta_n^*(\mu \F, c_1,c_2) = \mu \Delta_n^*\left(\F, \frac{c_1}{\mu},\frac{c_2}{\mu^2}\right),$
 where $\mu F$ denotes the function class comprised of functions in $\F$, each scaled by $\mu>0$. In particular, this relation implies that the bound for $\mu$-strongly convex function class is only a constant factor away from the bound for $1$-strongly convex function class.
\end{remark}
%Table \ref{tab:oracles} presents the upper and lower bounds for two specific choices of $p$ and $q$ (relevant in applications, as we shall see later). These bounds can be inferred from the results in Theorems \ref{thm:ub}--\ref{thm:lb-convex}.
While it may appear that for the strongly convex case the error becomes smaller with a larger dimension,
$C_1, C_2$ hide dimension dependent constants (for all oracles we know of)
and thus the lower bound actually increases with the dimension increasing.
%Some specific examples will be discussed in the next section.
% \begin{theorem}[\textit{Lower bound: Strongly convex}]
% \label{thm:lb-strongly-convex}
% For any $v \in \{+\nu,-\nu\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
% $f^{(i)}_{v_i}(x_i) := \dfrac{1}{2} x^2 - v x$, for $i=1,\ldots,d$.
% Consider the space of functions $\F:= \{f_v \mid v  \in \{+\nu,-\nu\}^d\}$, with $\nu = d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where $K_1$ is as defined in Theorem \ref{thm:lb-convex}.
% Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ Type-I oracle
%  with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
%  the minimax error \eqref{eq:minimaxerrdef} satisfies
% \[
% \Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}.
% \]
% \end{theorem}
% \begin{proof}
%  See Appendix \ref{sec:appendix-lbscconvex}.
% \end{proof}


% \todop{Extend the result to the case when the algorithm can choose $\delta$ in every step - will update the proof for adaptive $\delta$ soon}
