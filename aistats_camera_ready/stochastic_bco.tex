%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
The main application of the biased noisy gradient oracle based convex optimization of the previous section
is bandit convex optimization. We introduce here briefly the stochastic version of the problem, while online BCO will be considered in Section~\ref{sec:obco}. Readers familiar with these problems and the associated
gradient estimation techniques, may skip this description to jump directly to Theorem~\ref{thm:aaa},
and come back here only if clarifications are needed.

\if0
For example, \cite{AgDeXi10} used an algorithm that queries at two points per round, and defined the incurred loss as the average of losses at the two observation points. In our oracle, it can be stated as in round $t$, the same oracle $\gamma_t$ responds to the same inputs $(X_t, \delta_t, f_t)$ with two different pairs $(G_{t,1}, Y_{t,1})$ and $(G_{t,2}, Y_{t,2})$. The accumulated regret can be written as
$% \[
R_n = \sum_{t=1}^n \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2}) \right) -\inf_{x \in \cK}\sum_{t=1}^n f_t(x) \,.
$ %\]
Recalling that $Y_{t,1}$, $Y_{t,2}$ are in the $\delta$-vicinity of $X_t$, the relationship between $f_t(X_t)$ and $\dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)$ is then determined by the environment (i.e. the property of $f_t$). It is straightforward to bound $| \dfrac{1}{2}\left( f_t(Y_{t,1})+f_t(Y_{t,2})\right)- f_t(X_t)|$ as a function of $\delta$. \todoc{How? When $f_t$ is Lipschitz, smooth, etc? So you mean, when $f_t$ is a smooth function?}
The common assumption for this setting is: The oracle is a stochastic mapping from $(X, \delta, f)$ to $(G, Y)$; The algorithm selects the point $X_t$ depending on $\left( X_1, G_{1,1}, G_{1,2}, \cdots, X_{t-1},G_{t-1,1}, G_{t-1,2}  \right)$. This two-point feedback can be easily extended to multi-point feedback, too.
\fi

In the \emph{stochastic BCO} setting,
the algorithm sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the loss function at these points in noise.
In particular, in round $t$, the algorithm chooses $X_t$ based on the earlier observations $Z_1,\dots,Z_{t-1}\in \R$ and $X_1,\dots,X_{t-1}$, after which it observes $Z_t$, where $Z_t$ is the value of $f(X_t)$ corrupted by ``noise''.


Previous research considered several possible constraints connecting $Z_t$ and $f(X_t)$.
One simple assumption is that $\{Z_t-f(X_t)\}_t$ is an $\{\cF_t\}_t = \{\sigma(X_{1:t},Z_{1:t-1})\}_t$-adapted martingale difference sequence (with favorable tail properties).
% \todoc{Some readers might be put off by martingales..}
A specific case is when $Z_t - f(X_t) = \xi_t$, where $(\xi_t)$ is a sequence of independent and identically distributed (i.i.d.) variables.
A stronger assumption, common in stochastic programming, is that 
\begin{equation}
\label{eq:controlled}
Z_t = F(X_t,\noiseCap_t), \quad f(x) = \int F(x,\psi) P_{\noiseCap}(d\psi)\,,
\end{equation} 
where $\noiseCap_t\in \R$ is chosen by the algorithm and in particular the algorithm can draw $\noiseCap_t$ 
at random from $P_{\noiseCap}$.
As in \cite{duchi2015optimal}, we assume that the function $F(\cdot, \psi)$ is $L_{\psi}$-smooth $P_{\noiseCap}$-a.s. and the quantity $\overline{L}_{\Psi} = \sqrt{\E [L_{\Psi}^2]}$ is finite.  
%That is the algorithm can call the oracle $F(x,\noise)$ by selecting both $x$ and $\noise$. 
Note that the algorithm is aware of $P_{\noiseCap}$, but does not know how different values of $\noise$ affect the noise $\xi(x,\noise)=F(x,\noise)-f(x)$. Nevertheless, as the algorithm can control $\noise$ and thus $\xi$, we refer to this as
\emph{controlled noise} setting and to the others as the case of \emph{uncontrolled noise}.
As we will see, and is well known in the simulation optimization literature \citep{KlSpNa99,duchi2015optimal},
this extra structure allows the algorithm to reduce the variance of the noise of its gradient estimates by reusing the same $\noiseCap_t$ in consecutive measurements, while measuring the gradient at the same point, an instance of the method of the method of common random variables.
As creating an estimate from $K$ points (which is equivalent to the so-called ``multi-point feedback setup'' from the literature where $K$ points are queried in each round) changes the number of rounds from $n$ to $n/K$, which does not change the convergence rate as long as $K$ is fixed. 


%
% as in the online case.
%
%However, what differs in the optimization variant is that the value of the loss at the points sent to the oracle does not matter.
%Hence, in this setting the distinction between one-point and multi-point feedback (as long as the number observations is fixed, independently of the dimension) is irrelevant:


%A major tool in bandit convex optimization is to design gradient estimators, which are then used in conjunction with variants of gradient descent. \todoc{zillions of references.}
\subsection{Estimating the Gradient}

A common popular idea in bandit convex optimization is to use the bandit feedback to construct noisy (and biased) estimates of the gradient.
In the following, we provide a few examples for oracles that construct gradient estimates for function classes that are increasingly general: from smooth, convex to non-differentiable functions.

\paragraph{One-point feedback}
Given $x\in \cK$, $0<\delta\le 1$, common gradient estimates that are
based on a single query to the function evaluation oracle (the so-called
``one-point feedback'') take the form 
%\vspace{-0.2cm}
\begin{equation}
  \label{eq:one-point}
G = \frac{Z}{\delta}V, \textrm{ where } Z = f(x+\delta U) + \xi\,,
%\vspace{-0.2cm}
\end{equation}
where $(U,V,\xi)\in \R^d\times \R^d\times \R$ are jointly distributed random variables,
$\xi$ is the function evaluation noise whose distribution may depend on $x+\delta U$ but $\E[\xi|V]=0$, and $G$ is the estimate of $\nabla f(x)$ ($f:\cK\to \R$). 

In all oracle constructions we will use the following assumption:
\begin{ass}
  \label{ass:gradbasic}
  Let $\K \subset \D^\circ \subset \R^d$, where $f:\D \to \R$. %\footnote{Here, $\D^\circ$ denotes the interior of $\D$.}
  For any $x\in \K$, $x+\delta U \in \D$ a.s.,
  and $\EE{\norm{V}_*^2}$, $\EE{ \norm{U}^3 }<+\infty$.
\end{ass}
Note that here the function domain $\D$ can be larger than or equal to the set $\K$, where the algorithm chooses $x$. This is to ensure that the oracle will not receive invalid inputs, that is, queries where $f$ is not defined.
When the functions are defined over $\K$ only and $\K$ is bounded, the above constructions only work for $\delta$ small enough.
In this case, the best approach perhaps is to use Dikin ellipsoids to construct the oracles, as done by \citet{HaLe14:SOC}.

The next proposition, whose proof is based on ideas from \citet{spall1992multivariate}
%\todoa{add citation for the convex case}
 shows that the above one-point gradient estimator leads to a type-I (and, hence, also type-II) oracle.
\begin{proposition}
\label{prop:grad-onepoint}
Let Assumption~\ref{ass:gradbasic} hold and let $\gamma$ be the one-point feedback oracle defined in \eqref{eq:one-point}.
Assume further that
  $U$ is symmetrically distributed,
  $V = h(U)$, where $h:\R^d \to \R^d$ is an odd function,
  $\EE{V}=0$, and $\E[V U\tr] = I$.
Then, in the uncontrolled noise case, $\gamma$ is a $(c_1(\delta),c_2(\delta))$ type-I oracle given in Table~\ref{tab:oracles}, where 
$C_2=4 \EE{\norm{V}_*^2}\left( \essup \E[\xi^2|V]+\sup_{x\in\D} f^2(x)\right)$, and $C_1 =
\frac{L}{2} \E[ \dnorm{V} \norm{U}^2]$ when $f \in \F_{L,0}$ and  
$C_1 = \tfrac{B_3}{6} \EE{ \norm{V}_* \norm{U}^3 }$ for  $f \in \C^3$ where $B_3 = \sup_{x\in D} \norm{ \nabla^3 f(x) }_T$ where $\norm{\cdot}_T$ denotes the implied norm for rank-3 tensors.%
\end{proposition}
%\begin{proof}
%See Section~\ref{sec:appendix-grad}.
%\end{proof}

%\paragraph{One-point feedback with smoothing}
Another possibility is to use the so-called smoothing technique
\citep{PoTsy90,flaxman2005online,HaLe14:SOC}
%\todoa{ Cs: Is this correct?} \todoc{Yes, they all use this smoothing technique.}
to obtain type-II oracles. Following the analysis in \citet{flaxman2005online}, one gets the 
following result, which improves the bias of the previous result from $O(\delta)$ to $O(\delta^2)$ in the smooth+convex case:
%This uses the same estimates as above, but with a different analysis, yielding improved oracles for the smooth and convex case: 
\begin{proposition}
\label{prop:flaxman} Let Assumption~\ref{ass:gradbasic} hold and let $\gamma$ be the one-point feedback oracle defined in \eqref{eq:one-point}.
Define
%\begin{align*}
$
V = n_W(U)\dfrac{\lvert \partial W\rvert}{\lvert W \rvert}\,,
$
%\end{align*}
where $W \subset \R^n$ is a convex body with boundary $\partial W$, $U$ is uniformly distributed on $\partial W$, $n_W(U)$ denotes the normal vector of $\partial W$ at $U$, and $\lvert \cdot \rvert$ denotes the appropriate volume. Let $C_2>0$ be defined as in Proposition~\ref{prop:grad-onepoint}.
Then, if $f$ is $L_0$-Lipschitz, $\gamma$ is a memoryless, uniform type-II oracle with $c_1(\delta)=C_1 \delta$, $c_2(\delta) = C_2/\delta^2$ where
$C_1=L_0 \sup_{w \in W}\|w\|$. 
Further, assuming $W$ is symmetric w.r.t. the origin, if $f$ is $L$-smooth, then $\gamma$ is a type-I (and type-II oracle) with $c_1(\delta) = C_1\delta^2$, $c_2(\delta) = C_2/\delta^2$ where $C_1=(L/|W|)\int_W\|w\|^2 dw$, and, if in addition $f$ is also convex (i.e., $f\in \F_{L,0}$) then $\gamma$ is a type-I oracle with $c_1(\delta)=C_1 \delta^2/2$ and $c_2(\delta)=C_2/\delta^2$.
\end{proposition}
Note that the improvement did not even require convexity.
Also, the bias is smaller for smoother functions, a property that will be enjoyed by all the gradient estimators.
%In particular, the gradient estimators adapt to the smoothness of $f$ in this sense.}

%Note that by the above proposition, the smoothing technique provides an improved gradient oracle for the smooth and convex case compared to \cref{prop:grad-onepoint}.
%\begin{proof}
%See Section~\ref{sec:appendix-grad}.
%\end{proof}



\paragraph{Two-point feedback}
While the one-point estimators are intriguing,
in the optimization setting one can also always group two consecutive observations and obtain similar smoothing-type estimates 
at the price of reducing the number of rounds by a factor of two only, 
which does not change the rate of convergence.
Next we present an oracle that uses two function evaluations to obtain a gradient estimate.
As will be discussed later, this oracle encompasses several simultaneous perturbation methods (see \citealp{bhatnagar-book}):
Given the inputs $x\in \K$,  $0<\delta\le 1$,
% \todoc{We need an upper bound I believe. Add it earlier.}
the gradient estimate is
\begin{align}
G &=  \dfrac{Z^+ - Z^-}{2\delta}\, V \,, 
 \label{eq:twosp}
\end{align}
where $Z^{\pm} = f(X^{\pm}) + \xi^{\pm}$, $X^{\pm} = x \pm \delta U$, $U,V\in \R^d$, $\xi^{\pm}\in \R$ are random, jointly distributed random variables, $U,V$ chosen by the oracle in the uncontrolled case and chosen by the algorithm in the controlled case
from some fixed distribution characterizing the oracle (depending on $F$), and $\xi^{\pm}$ being the noise of the returned feedback $Z^{\pm}$ at points $X^{\pm}$.
For the following proposition we consider $4=2\times 2$ cases.
First, the function is either assumed to be $L$-smooth and convex (i.e., the derivative of $f$ is $L$-Lipschitz w.r.t. $\norm{\cdot}_*$), or it is assumed to be three times continuously differentiable ($f\in C^3$).
The other two options are either the controlled noise  setting of \eqref{eq:controlled}, or, in the uncontrolled case, we make the alternate assumptions 
%\vspace{-0.2cm}
\begin{align}
\E[\xi^+-\xi^- |\, U,V] &= 0 \text{~~ and ~~}\nonumber\\
\E [ (\xi^{+} - \xi^-)^{2} |\, V] &\le \sigma_\xi^2 <\infty\,.
\label{eq:noiseass}
\end{align}
%\vspace{-0.8cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\small
\centering
\begin{tabular}{|c|c|c|}
\toprule
\textbf{Noise }$\bm{ \rightarrow}$ & \textbf{Controlled } & \textbf{Uncontrolled } \\
\textbf{Function } &(see~\eqref{eq:controlled})&(see~\eqref{eq:noiseass})\\
$\bm{\downarrow}$ &&\\\midrule
\multirow{3}{*}{\textbf{Convex + Smooth}} 
	& \multirow{3}{*}{$(C_1 \delta, C_2)$} 
	& %\multirow{2}{*}
		{Props~\ref{prop:grad-onepoint},\ref{prop:grad-spsa}: $(C_1\delta, \frac{C_2}{\delta^2}) $}\\
&& \\
 &&{Prop~\ref{prop:flaxman}: $(C'_1\delta^2, \frac{C_2}{\delta^2}) $}\\\midrule
\multirow{2}{*}{$\bm{f \in \C^3}$} 
	& \multirow{2}{*}{$(C_1 \delta^2, \frac{C_2}{\delta^2})$} 
	& \multirow{2}{*}{Props~\ref{prop:grad-onepoint},\ref{prop:grad-spsa}: $(C_1 \delta^2, \frac{C_2}{\delta^2})$} \\
 &&\\\bottomrule
\end{tabular}
\caption{Gradient oracles for different function classes and noise categories. %Each table entry specifies the pair $(c_1(\delta), c_2(\delta))$, with 
The constants $C_1, C'_1, C_2$ are defined in Propositions~\ref{prop:grad-onepoint}--\ref{prop:grad-spsa}.
}
\label{tab:oracles}
%\vspace{-0.2cm}
\end{table}
%\footnotetext{
%For the first row:  
%$C_1 = \frac{\overline{L}_{\Psi}}{2} \E[ \dnorm{V} \norm{U}^2]$ and 
%and
%$C_2 =  2 B_1^2  + \frac{ \overline{L}_{\Psi}^2}{2}\E\left[ \dnorm{V}^2 \norm{U}^4 \right]$, with $B_1 = \sup_{x\in \K} \scnorm{\nabla f(x)}_*$, for the controlled noise;
%$C_1 =
%L\E[ \dnorm{V} \norm{U}^2]/2$, $C'_1=L \E[\|V\|^2]$,  and
 %$C_2 =  C_{2}^{(u)} \doteq 4 \EE{\norm{V}_*^2}\left( \sigma_\xi^2+\fspan(f)\right)$ for the uncontrolled noise.
%For the second row, $C_1 = B_3 \EE{ \norm{V}_* \norm{U}^3 }/6$ and $C_2 =  C_{2}^{(u)}$,
%with $B_3 = \sup_{x\in \K} \norm{\nabla^3 f(x)}$, where $\norm{\cdot}$ is the implied norm for rank-3 tensors.
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following proposition, whose proof is based on \citep[Lemma~1]{spall1992multivariate} and \citep[Lemma~1]{duchi2015optimal}, provides conditions under which the bias-variance parameters $(c_1,c_2)$ can be bounded as shown in Table~\ref{tab:oracles}:
\begin{proposition}
\label{prop:grad-spsa}
Let Assumption~\ref{ass:gradbasic} hold and let $\gamma$  be a two-point feedback oracle defined by \eqref{eq:twosp}.
Suppose furthermore that $\E[V U\tr] = I$.
Then $\gamma$ is a type-I oracle with the pair $(c_1(\delta),c_2(\delta))$ 
given by Table~\ref{tab:oracles}. For uncontrolled noise and for controlled noise with $f\in\C^3$, $C_1$ is as in Proposition~\ref{prop:grad-onepoint} and $C_2$ is $4 C_2$ from Proposition~\ref{prop:grad-onepoint}. For the controlled noise case with $f \in \F_{L,0}$,
$C_1 = \frac{\overline{L}_{\Psi}}{2} \E[ \dnorm{V} \norm{U}^2]$  
and
$C_2 =  2 B_1^2  + \frac{ \overline{L}_{\Psi}^2}{2}\E\left[ \dnorm{V}^2 \norm{U}^4 \right]$, with $B_1 = \sup_{x\in \K} \scnorm{\nabla f(x)}_*$.
\end{proposition}
%\begin{proof}
%See Section~\ref{sec:appendix-grad}.
%\end{proof}
%\todoc[inline]{I am pretty sure that the bias could be brought down for two-point feedback, too, using the technique of \cref{prop:flaxman}.}

\paragraph{Popular choices for $U$ and $V$:}
\begin{itemize}
 \item If we set $U_i$ to be independent, symmetric $\pm 1$-valued random variables and $V_i = 1/U_i$, then we recover the popular SPSA scheme proposed by \citet{spall1992multivariate}.
It is easy to see that $\EE{  V U\tr } = I$ holds in this case.
 When the norm $\norm{\cdot}$ is the $2$-norm, $C_1 = O(d^2)$ and $C_2 = O(d)$. If we set $\norm{\cdot}$ to be the max-norm, $C_1 = O(\sqrt{d})$ and $C_2 = O(d)$.
 \item If we set $V=U$ with $U$ chosen uniform at random on the surface of a sphere with radius $\sqrt{d}$,
 then we recover the RDSA scheme proposed by  \citet[pp.~58--60]{kushcla}.
 In particular, the $(U_i)$ are identically distributed with $\EE{ U_i U_j } = 0$ if $i\ne j$ and $\EE{ U\tr U } = d$, hence $\EE{U_i^2} = 1$. Thus, if we choose $\norm{\cdot}$ to be the $2$-norm, $C_1 = O( d^2 )$ and $C_2 = O(d)$.
 \item If we set $V=U$ with $U$ the standard $d$-dimensional Gaussian with unit covariance matrix, we recover the smoothed functional (SF) scheme proposed by \citet{katkul}.
Indeed, in this case, by definition, $\EE{VU\tr} = \EE{U U\tr } = I$.
When $\norm{\cdot}$ is the $2$-norm, $C_1 = O(d^2)$
%$\norm{U}^4 = (\sum_{i=1}^d U_i^2)^{2} = \sum_i U_i^4 + 2 \sum_{i<j} U_i^2 U_j^2$,
%hence $\EE{ \norm{U}^4} = O(d^2)$
 and $C_2 = O( d)$.
 This scheme can also be interpreted as a smoothing operation that  convolves the gradient of  $f$ with a Gaussian density.
%  , followed by an integration by parts and the resulting integral can be estimated using samples (without access to the gradient of $f$).
\end{itemize}


% If the function $f$ is assumed to be convex and smooth, then gradient estimates similar to \eqref{eq:twosp} can be constructed and this does not require higher order smoothness conditions as in Proposition~\ref{prop:grad-spsa}.
% \begin{proposition}
% \label{prop:grad-convex}
% Consider a function $f:\D \to \R$, with $\K \subset \D \subset \R^d$,
% that is convex and has a $L$-Lipschitz continuous gradient. .
% For any $x \in \cK$, and $\delta >0$ such that $\B(x,\delta) \in \D$, let the oracle $\gamma$ return
% \begin{align}
% % Y = x+\delta U \,, \quad
% G =  V \left(\dfrac{f(x+\delta U) + \xi^+ - (f(x-\delta U) + \xi^-)}{2\delta}\right),
%  \label{eq:twosp}
% \end{align}
% where $\xi^+, \xi^-$, $V$, $U$ are as in Proposition~\ref{prop:grad-spsa}.
% Then, we have that $\gamma$ is a type-I oracle with $c_1(\delta) = C_1 \delta$ and $c_2(\delta) = C_2 d/\delta^2$.
% \end{proposition}
% \begin{proof}
% See Appendix~\ref{sec:appendix-grad}.
% \end{proof}


% A popular idea for estimating gradient using one-point feedback is the smoothed function approach, which was originally proposed in \citep{katkul}. The idea is to convolve the gradient of the objective function with a suitable density function and then, via an integration by parts arguments show that the resulting integral is an estimate of the gradient of the smoothed objective function.
% This approach has been adopted in a stochastic convex optimization setup in \cite{duchi2015optimal}.

%% \paragraph{Simultaneous perturbation methods:}
%The idea of simultaneous perturbation can work even for functions that are not differentiable, as shown in \cite{flaxman2005online}.
%% follow this approach in the context of bandit convex optimization.
%Formally,
%% a $(c_1,c_2)$ Type-II oracle when $\cF$ is a general class of functions (this does not even require differentiability).
%given any $f \in \cF$, $x \in \cK$, and $\delta >0$, the oracle returns\footnote{See also \cite[pp.~58-60]{kushcla} for an old reference that proposed a gradient estimate similar to \eqref{eq:flaxman}.}
%\begin{align}
% Y = x+\delta u \in \cK', \quad
% G = \dfrac{d}{\delta}f(x+\delta u)u \in \R^d, \label{eq:flaxman}
%\end{align}
%where $u\in \R^d$ is a random unit vector, so the first condition of \cref{def:oracle2} immediately follows.
%The variance of $G$ is bounded by $d^2C^2 \delta^{-2}$ for some constant $C = \sup_{y\in \cK'}f(y)$.
%\todox[inline]{Fix this proposition statement to say Flaxman scheme is a type II oracle}
%\begin{proposition}
%Let $\tilde{f}(x) = \EE{f(x+\delta v)}$ denote the smoothed version of $f$. Then, we have
%$\EE{G} = \nabla \tilde{f}(x)$.
%\end{proposition}
%\begin{proof}
% See Lemma 1 in \citep{flaxman2005online}.
%\end{proof}
% As to the bias condition, it was proved that $\EE{G} = \nabla \tilde{f}(x)$, where $\tilde{f}$ is a smoothed version of $f$, i.e.,
%$\tilde{f}(x) = \EE{f(x+\delta v)}$,
%$v$ is a random vector in a unit ball. There are different ways to bound the bias depending on the property of $f$.
%If $f$ is $L_{lip}$-Lipschitz over $\cK'$, then we have
%\begin{align*}
%\MoveEqLeft
%\norm{\tilde{f}(x)-f(x)}_\infty \\
%=&\norm{\EE{f(x+\delta v)-f(x)}}_\infty
%\le L_{lip} \delta \,.
%\end{align*}
%If $f$ is convex, and $L_{smo}$-smooth,
%\begin{align*}
%\MoveEqLeft
%\norm{\tilde{f}(x)-f(x)}_\infty \\
%\le& \norm{\EE{\ip{\nabla f(x), \delta v}+\dfrac{L_{smo}}{2}\delta^2\norm{v}^2}}_\infty\\
%\le &\dfrac{L_{smo}}{2}\delta^2 \,.
%\end{align*}
%Therefore, the estimator \eqref{eq:flaxman} can always fit the oracle setting by choosing $c_1(\delta) = C_1 \delta$ (or $C_1\delta^2$), $c_2(\delta) = C_2 \delta^{-2}$, for some constant $C_1$, $C_2$.

% \begin{remark}
%  Instead of picking $u$ randomly on the surface of a unit sphere, one can employ an random variable $u$ that satisfies $E[u u\tr] = I_d$, where $I_d$ is the $d$-dimensional identity matrix. A popular choice for $u$ that satisfies the aforementioned constraint is the $d$-dimensional standard Gaussian - a choice that has been explored in the context of zeroth order optimization in \citep{duchi2015optimal}. See \citep{bhatnagar-book} for an overview of gradient and Hessian estimation techniques using random perturbations.
% \end{remark}


\subsection{Achievable Results for Stochastic BCO}
We now consider stochastic BCO with $L$-smooth functions over a convex, closed non-empty domain $\K$. 
Let $\cF$ denote the set of these functions.
\citet{duchi2015optimal} proves that the minimax expected optimization error
for the functions $\cF$ with uncontrolled noise is lower bounded by $\Omega(n^{-1/2})$. \todoc{We should sort out dimension dependences later.}
They also give an algorithm which uses two-point gradient estimates which matches this lower bound for the case of \emph{controlled noise}.
For controlled noise, the constructions in the previous section give that for two-point estimators $c_1(\delta) = C_1 \delta^p$ and $c_2(\delta) = C_2\delta^{-q}$ with $p=1$ and $q=0$. Plugging this into
Theorem~\ref{thm:ub} we get the rate $O(n^{-1/2})$ (which is unsurprising
given that the algorithms and the upper bound proof techniques are essentially the same as that of \citet{duchi2015optimal}).
However, when the noise is uncontrolled, the best that we get is $p=2$ and $q=2$.
From Theorem~\ref{thm:lb-convex} we get that with such oracles, no algorithm can get better rate than $\Omega(n^{-1/3})$, while from
Theorem~\ref{thm:ub} we get that these rates are matched by mirror descent.
We can summarize these findings as follows:
%\todoc{Can we show that no better oracles exist?}
\begin{theorem}\label{thm:aaab}
Consider $\F_{L,0}$, the space of convex, $L$-smooth functions over a convex, closed non-empty domain $\K$.
Then, we have the following:\\
\textit{\textbf{Uncontrolled noise}}:
Take any $(\delta^2,\delta^{-2})$ type-I oracle $\gamma$.
There exists an algorithm that uses $\gamma$
and achieves the rate $O(n^{-1/3})$.
Furthermore, no algorithm using $\gamma$
 can achieve better error than $\Omega(n^{-1/3})$ for every $(\delta^2,\delta^{-2})$ type-I oracle $\gamma$.\\
\textit{\textbf{Controlled noise}}:
Take any $(\delta,1)$ type-I oracle $\gamma$.
There exists an algorithm that uses $\gamma$ an
achieves the rate $O(n^{-1/2})$.
Furthermore, no algorithm using $\gamma$
 can achieve better error than $\Omega(n^{-1/2})$ for every $(\delta,1)$ type-I oracle $\gamma$.
\end{theorem}

For stochastic BCO with uncontrolled noise, \citet{AgFoHsuKaRa13:SIAM} analyze a variant of the well-known ellipsoid method and provide regret bounds for the case of convex, $1$-Lipschitz functions over the unit ball. 
Their regret bound implies a minimax error \eqref{eq:minimaxerrdef} bound of order  $O\left(\sqrt{d^{32}/n}\right)$. 
%\todoc{Not for this setting! This paper should be mentioned in the previous section actually. For this setting, we  do not have \emph{any} algorithms that would achieve $O(n^{1/2})$.}
\citet{liang2014zeroth} provide an algorithm based on random walks (and not using gradient estimates) for the setting of convex, bounded functions whose domain is contained in the unit cube and their algorithm results in a bound of the order $\O\left((d^{14}/n)^{1/2}\right)$ for the minimax error.
%\todoc{What are the conditions for their theorem? Boundedness? Lipschitzness? Or nothing?}
%This algorithm achieves an upper bound of $\O\left((d^{14}/n)^{1/2}\right)$,
These bounds decrease faster in $n$ than the bound available in Theorem~\ref{thm:aaab}, while showing a much worse dependence on the dimension.\todoc{We should show the dimension dependence in the result to make this point. This should be shown for the same setting as the one considered by \cite{liang2014zeroth}.}
However, what is more interesting is that our results also shows that an $O(n^{-1/2})$ upper bound \emph{cannot} be achieved solely based on the oracle properties of the gradient estimates considered. Since the analysis of all gradient algorithms for stochastic BCO does this, it is no wonder that the best known upper bound for convex+smooth functions is $O(n^{-1/3})$ \citep{saha2011improved}. (We will comment on the recent paper of \citet{DeElKo15} later.)

The above result also shows that the gradient oracle based algorithms are optimal for smooth problems, under a controlled noise setting.
While \citet{duchi2015optimal} suggests that it is the power of two-point gradient estimators that helps to achieve this, we need to add that having controlled noise is also critical. %a critical condition to achieve the optimal rate is that the noise must be controlled. 

Finally, let us make some remarks on the early literature on this problem.
A finite time lower bound for stochastic, smooth BCO is presented by  \citet{Chen88:LB-AoS} for
convex functions on the real line. When applied to our setting in the uncontrolled noise case, his results imply that $\EE{ |\hat{X}_n - x^* |}$, that is, the distance of the estimate to the optimum, is at least $\Omega(n^{-1/3})$. 
Note that this is larger than the error achieved by the algorithms of \citet{liang2014zeroth,BubeckDKP15,BuEl15}, but the apparent contradiction is easily resolved by noticing the difference in their error measure: distance to the optimum vs. error in the function value (in particular, compressing the range of functions makes locating the minimizer harder).
 \citet{PoTsy90}, who also considered distance to optimum, proved that mirror descent with gradient estimation achieves asymptotically optimal rates for functions that enjoy high order smoothness.
%While the lower bound is stated for $r$-smooth functions with $r$ odd (these are functions $f$ with $\norm{f^{(r)}}_{\infty}\le L$), and $r$ greater than one, careful checking of the results show that nor $r>1$, neither that $r$ must be odd is ever used in the proof.
%Hence, the result holds for $r\ge 2$.
%The lower bound presented for a given value of $r$ takes the form $\Omega( n^{ -(r-1)/(2r)} )$. For $r=2$ (which approximately corresponds to the smooth case), we get $\Omega(n^{-1/4})$, while for $r=3$ (which is ``almost'' the same as $f\in \C^3$), one gets $\Omega(n^{-1/3})$.
%While the second bound matches the bound in \cref{thm:aaab}, the first one is larger, and both bounds are larger than the error achieved by
%the algorithm of \cite{liang2014zeroth,BubeckDKP15,BuEl15}. The resolution of the apparent contradiction is that the lower bounds of \citet{Chen88:LB-AoS} concern distance to the optimum (i.e., $\EE{ \norm{\hat{X}_n - x^* }}$), while the optimization error is defined in terms of the objective function gap $\EE{ f(\hat{X}_n) -f( x^*)}$.
%Similar results are obtained by \citet{PoTsy90}, who also considered distance to the optimum and proved that mirror descent with gradient estimation achieves asymptotically optimal rates for these settings.
\todoc{Cite all the other papers, summarizing what they achieve. }

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bgo"
%%% End:
