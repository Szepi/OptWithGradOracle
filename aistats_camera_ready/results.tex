%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
\begin{table*}
\small
\centering
\label{tab:mse-1}
 \begin{tabular}{|c|c|c|c|c|}
% \begin{tabular}{||*5{>{\columncolor[gray]{.9}}c}||}
\toprule
% \rowcolor{gray!20}
% \multicolumn{3}{|c|}{\multirow{2}{*}{\textbf{Lower bounds}}}\\[1.7em]
% \midrule
  \multirow{2}{*}{\textbf{Oracle type}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Convex + Smooth}}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Strongly Convex + Smooth}}} \\[1em]
 \midrule
% \rowcolor{gray!20}
 & \textbf{Upper bound} & \textbf{Lower bound} & \textbf{Upper bound} & \textbf{Lower bound}\\
 \midrule
\textbf{ $\delta$-bias, $\delta^{-2}$-variance} & \multirow{2}{*}{$\left(\dfrac{C_1^{2}C_2 D}{n}\right)^{1/4}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2 d^2}{n}\right)^{1/4}$}& \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2}{ n}\right)^{1/2}$} \\[0.5ex]
 ($p=1$, $q=2$) & & & &\\\midrule
%%%%%%%%%%%%%%%%%%%
\textbf{$\delta^2$-bias, $\delta^{-2}$-variance } & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 D}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 \sqrt{d^3}}{n}\right)^{1/3}$} & \multirow{2}{*}{$\left(\dfrac{C_1 C_2}{n}\right)^{1/2}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 }{ n}\right)^{2/3}$}\\[1.4ex]
 ($p=2$, $q=2$) & & & &\\
  %\midrule
 %\textbf{No bias or variance} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\left(\dfrac{d^2}{n}\right)^{1/2}$}  \\[0.5ex]
 %($C_1=0$, $C_2=0$) & & & &\\
\bottomrule
%%%%%%%%%%%%%%%%
\end{tabular}
\caption{Summary of upper and lower bounds for different smooth function classes and  gradient oracles for the settings of \cref{thm:ub} and \cref{thm:lb-convex}. Note that when $\cR$ is the squared norm and $\K$ is the hypercube (as in the lower bounds), $D=\theta(d)$ in the upper bounds and also that $C_1$, $C_2$ may hide dimension-dependent quantities for the common gradient estimators, as will be discussed later.}
\end{table*}


In this section we provide our main results in forms of upper and lower bounds on the minimax error.
First we give an upper bound by analyzing a mirror-descent algorithm given in \cref{alg}.
In the algorithm, we assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex and the target function $f$ is smooth or  smooth and strongly convex. A standard analysis yields the following upper bounds:
% In the algorithm, we will assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t.\  some norm $\|\cdot\|$ on $\R^d$, that is $\tfrac{\alpha}{2} \|x-y\|^2 \le \DR(x,y)=f(x)-f(y)-\ip{\nabla f(y),x-y}$ for all $x,y \in \K$. Moreover, we will consider the cases when the target function $f$ is $L$-smooth w.r.t.\  the norm $\| \cdot \|$ for some $L>0$, that is, $f(x) \le f(y) + \ip{\nabla f(y), y-x} + \tfrac{L}{2} \|x-y\|^2$, and when $f$ is $\mu$-strongly convex w.r.t.\  $\cR$, that is,
% $f(x) \ge f(y) + \ip{\nabla f(y),y-x} +\tfrac{\mu}{2} \DR(x,y)$ for all $x,y \in \K$.

%Unconstrained case:
%
%Upper bound:

\begin{algorithm}[t]
\begin{algorithmic}
    \State {\bf Input:}  Closed convex set $\cK\ne \emptyset$, regularization function $\mathcal{R}:\dom(\mathcal{R})\to \mathbb{R}$, $\cK^{\circ}\subset \dom(\mathcal{R})$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$.
%     In round $t=1, 2, \cdots, n-1$:
\State Initialize $X_1\in \cK$ arbitrarily.
\For{$t=1, 2, \cdots, n-1$}
	\State Query the oracle at $X_t$ to receive $G_t$, $Y_t$.
	\State Set
	$X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right].$
\EndFor
\State {\bf Return:} $\hat{X}_n = \frac{1}{n}\sum_{t=1}^n X_t \,.$
\end{algorithmic}
\caption{Mirror Descent with Type-I/II Oracle.
\label{alg}}
\end{algorithm}


% \begin{algorithm}
% 	\caption{Mirror Descent with Type-I/II Oracle}\label{alg}
% 	\textbf{Input}: Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$. \\
% 	Initialize $X_1\in \cK$ arbitrarily.\\
% 	In round $t=1, 2, \cdots, n-1$:
% 	\begin{itemize}
% 	\item Query the oracle at $X_t$.
% 	\item Receive $G_t$, $Y_t$.
% 	\item Update
% 	\[
% 	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
% 	\]
% 	\end{itemize}
% 	\textbf{Output}: the optimizer
% 	\[
% 	\hat{X}_n = \dfrac{1}{n}\sum_{t=1}^n X_t \,.
% 	\]
%
% \end{algorithm}

\begin{theorem}[\textit{Upper bound}]
\label{thm:ub}
Consider the class $\F_{L,0}$ of convex, $L$-smooth functions on the bounded, convex domain $\cK\ne \emptyset$, $\cK \subset \R^d$.
% let $\cF$ be a class of convex functions on $\cK$ such that $f \in \cF$ is $L$-smooth for some $L>0$.
Assume that the regularization function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t.\  some norm $\norm{\cdot}$, and $\cK^\circ\subseteq \dom(\mathcal{R})$.
For any $(c_1,c_2)$ type-II oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$, 
 if \cref{alg} is run with $\eta_t = \alpha/(a_t+L)$ and
% \[
% \delta = \left( \dfrac{C_2}{2aC_1}\sqrt{\dfrac{\alpha}{2D}} \right)^{1/(p+q)}n^{-1/(2p+q)} \,,
% \] 
 $
 \delta = \left( \tfrac{C_2}{4aC_1}\tfrac{2p+q}{p}\tfrac{n}{n+1}\right)^{\frac{1}{p+q}}n^{-\frac{1}{2p+q}} \,,$  where
 $a^{2p+q} =2^{q-p}\left( 2+\tfrac{q}{p} \right)^p\left(1+ \tfrac{1}{n} \right)^q \left( \tfrac{\alpha}{D} \right)^{p+q}C_1^q C_2^p $,
 $a_t = a t^{\frac{p+q}{2p+q}}$, for $t=1, 2, \cdots, n-1$,
then  the the worst-case error (and hence the minimax error) is bounded as  %\eqref{eq:minimaxerrdef} 
can be bounded as
% \begin{align*}
 \[
 \Delta_{\F_{L,0},n}^{MD}(c_1,c_2) 
 %\frac{R_{\F_{L,0},n}^*(c_1,c_2)}{n}
 = O\left(\left(\dfrac{DC_1^{\frac{q}{p}}C_2}{n}\right)^{\frac{p}{2p+q}}\right),
%\vspace{-0.3cm}
 \]
 %\end{align*}
 where $D=\sup_{x,y\in \cK} \DR(x,y)$.
 
For the class $\F=\F_{L,\mu,\cR}$ of $\mu$-strongly convex (w.r.t.\  $\cR$) and $L$-smooth functions, with $\alpha >2L/\mu$, 
$
 \eta_t = 2/(\mu t),
$ and 
%\[
%\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n} \,,
%\]
$
\delta^{p+q} =  \tfrac{C_2\left( \log n+1+\tfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)} \,,
$
the worst-case error of \cref{alg} (and the minimax error) satisfies
\begin{align*}
%\lefteqn{
\Delta_{\F_{L,\mu,\cR},n}^{MD}(c_1,c_2) 
%} \\
& %\le  \frac{R_{\F_{L,\mu,\cR},n}^*(c_1,c_2)}{n} 
=  \tilde{O}\left( \left(\dfrac{C_1^{\frac{q}{p}} C_2}{n} \right)^{\frac{p}{p+q}} \right)\,.
%\vspace{-0.5cm}
\end{align*}
In the bounds $O(\cdot)$ hides a constant that is a function of $p$, $q$, $\alpha$, $L$ and $\mu$.%
\footnote{In the second bound, $\tilde{O}(f(n))$ denotes $O(\log(n) f(n))$.}
\end{theorem}
%\begin{proof}
%See \cref{sec:appendix-md}.
%\end{proof}

We next state lower bounds for both convex as well as strongly convex function classes. In particular, we observe that for convex and smooth functions the upper bound for the mirror descent scheme matches the lower bound, up to constants, whereas there is a gap for strongly convex+smooth functions.
Filling the gap is left for future work.
\begin{theorem}[\textit{Lower bound}]
\label{thm:lb-convex}
Let $n>0$ be an integer, $p,q>0$, $C_1,C_2>0$, 
$\cK\subset \R^d$ convex, closed, with  $[+1,-1]^d\subset \cK$.
%For any $v \in \{+1,-1\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
%$f^{(i)}_{v_i}(x_i) := \dfrac{\epsilon}{2} (x_i - v_i)^2$, for $i=1,\ldots,d$.
%Consider the space of functions $\F:= \{f_v \mid v  \in \{+1,-1\}^d\}$, with $\epsilon = d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where \\$K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$. 
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$,
 the minimax error (and hence the regret) satisfies the following bounds:\\
\textbf{$\bm{\F_{L,0}(\K)}$ (Convex and smooth) w.r.t. the Euclidean norm $\|\cdot\|_2$ with $L\ge \frac12$:}
% Consider the class $\F_{L,0}$ of functions with domain $\cK$. \todoc{Probably the notation should be $\F_{L,0}(\cK)$?} 
\[
 \Delta_{\F_{L,0},n}^{*}(c_1,c_2) \ge K_1\sqrt{d}\left(\dfrac{C_1^{\frac{q}{p}}C_2}{n}\right)^{\frac{p}{2p+q}},
\]
where $K_1 = \frac{\left(2p+q\right)^2}{2q^{\frac{q}{2p+q}}\left(4p+q\right)^{\frac{4p+q}{2p+q}}}$.
%where $K_1 =\frac{4p+q}{\sqrt{C_2}(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.

\textbf{$\bm{\F_{L,1}(\K)}$ ($1$-strongly convex and smooth) with $L\ge 1$:}
% For the class $\F_{L,1}$ of functions with domain $\K$, the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_{\F_{L,1}, n}^{*}(c_1,c_2) \ge K_2 \left(\dfrac{C_1^{\frac{q}{p}} C_2}{n} \right)^{\frac{p}{p+q/2}}, 
\]
where $K_2 = 2^{\frac{2p-q}{2p+q}} \frac{(2p+q)^3}{q^{\frac{2q}{2p+q}}(6p+q)^{\frac{6p+q}{2p+q}}} $.
\end{theorem}
%The condition connecting the problem parameters is that $n$ should be large enough so that\\
%\begin{inparaenum}[\bfseries (i)]
%\item 
%$\frac{1}{\sqrt{d}}\left(\frac{1}{\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}\le \min\left( \frac{C_1 (2p+q)}{q}, \frac{2z+1}{C_1^{2z-1}(z+1)^z}\right)$ with $z=(p/q)$ for $\F_{L,0}(\K)$; and\\
%\item 
%$\!d^{\frac{-(4p+q)}{(2p+q)}}\!\!\left(\frac{1}{2\sqrt{n} K_1} \right)^{\!\!\frac{p}{p+\frac{q}{2}}} \!\!\!\le \!\!\min\!\left( \frac{C_1 (2p+q)}{q}, \frac{2z+1}{C_1^{2z-1}(z+1)^z}\!\right)$  for $\F_{L,1}(\K)$.
%\end{inparaenum}
%% where $K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.
%%\begin{proof}
%% See Section \ref{sec:appendix-lb-proof}.
%%\end{proof}

By continuity, the above claim can be extended to cover the case of $q=0$ (constant variance). 
For the special case of $p=0$ and $C_1>0$, which implies a constant bias, it is possible to derive an $\Omega(1)$ lower bound by tweaking the proof. On the other hand, the case of $p=0$ and $C_1=0$ (no bias) leads to an $\Omega(d/\sqrt{n})$ lower bound. The proof of the lower bound is obtained in the usual way by presenting a family of functions and a type-I oracle such that any algorithm suffers at least the stated error on one of the functions. In particular, for $\F_{L,0}$ with $L\ge 1/2$ we use
$f_{v,\epsilon}(x)=\epsilon\left( x-v\right)+2\epsilon^2 \ln\left(1+e^{-\frac{x-v}{\epsilon}}  \right)$ with $v=\pm 1$, $\epsilon>0$, and $x \in \cK \subset \R$ for appropriate $\epsilon$. Note that for \emph{any} $\epsilon>0$, $f_{v,\epsilon}\in \F_{1/2,0}\setminus \cup_{0<\lambda<1/2} \F_{\lambda,0}$.


\begin{remark}(\textbf{\textit{Scaling}})
For any function class $\F$, by the definition of the minimax error \eqref{eq:minimaxerrdef}, it is easy to see that
$$\Delta_n^*(\mu \F, c_1,c_2) = \mu \Delta_n^*\left(\F, c_1/\mu,c_2/\mu^2\right),$$
 where $\mu F$ denotes the function class comprised of functions in $\F$, each scaled by $\mu>0$. In particular, this relation implies that the bound for $\mu$-strongly convex function class is only a constant factor away from the bound for $1$-strongly convex function class.
\end{remark}

Table \ref{tab:oracles} presents the upper and lower bounds for two specific choices of $p$ and $q$ (relevant in applications, as we shall see later). These bounds can be inferred from the results in Theorems~\ref{thm:ub} and~\ref{thm:lb-convex}.
%While it appears that for the strongly convex case the error becomes smaller with a larger dimension, in most applications $C_1, C_2$ will hide dimension dependent constants, and the lower bound actually increases with the dimension increasing.
Some specific examples will be discussed in the next section.
% \begin{theorem}[\textit{Lower bound: Strongly convex}]
% \label{thm:lb-strongly-convex}
% For any $v \in \{+\nu,-\nu\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
% $f^{(i)}_{v_i}(x_i) := \dfrac{1}{2} x^2 - v x$, for $i=1,\ldots,d$.
% Consider the space of functions $\F:= \{f_v \mid v  \in \{+\nu,-\nu\}^d\}$, with $\nu = d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where $K_1$ is as defined in Theorem \ref{thm:lb-convex}.
% Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ type-I oracle
%  with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
%  the minimax error \eqref{eq:minimaxerrdef} satisfies
% \[
% \Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}.
% \]
% \end{theorem}
% \begin{proof}
%  See Appendix \ref{sec:appendix-lbscconvex}.
% \end{proof}


% \todop{Extend the result to the case when the algorithm can choose $\delta$ in every step - will update the proof for adaptive $\delta$ soon}
