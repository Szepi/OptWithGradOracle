%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)

In this section we provide our main results in forms of upper and lower bounds on the minimax error.
First we give an upper bound for the mirror-descent algorithm shown as \cref{alg}.
In the algorithm, we assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex and the target function $f$ is smooth or  smooth and strongly convex.
We give results for polynomial oracles, i.e., when $c_1$ and $c_2$ are polynomial functions (in particular, monomial functions)
of their argument. The reason, as we will see is that the existing oracle constructions give rise to polynomial oracles for the function classes that we consider.
% In the algorithm, we will assume that the regularizer function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t.\  some norm $\|\cdot\|$ on $\R^d$, that is $\tfrac{\alpha}{2} \|x-y\|^2 \le \DR(x,y)=f(x)-f(y)-\ip{\nabla f(y),x-y}$ for all $x,y \in \K$. Moreover, we will consider the cases when the target function $f$ is $L$-smooth w.r.t.\  the norm $\| \cdot \|$ for some $L>0$, that is, $f(x) \le f(y) + \ip{\nabla f(y), y-x} + \tfrac{L}{2} \|x-y\|^2$, and when $f$ is $\mu$-strongly convex w.r.t.\  $\cR$, that is,
% $f(x) \ge f(y) + \ip{\nabla f(y),y-x} +\tfrac{\mu}{2} \DR(x,y)$ for all $x,y \in \K$.

%Unconstrained case:
%
%Upper bound:

\begin{algorithm}[t]
\begin{algorithmic}
    \State {\bf Input:}  Closed convex set $\cK\ne \emptyset$, regularization function $\mathcal{R}:\dom(\mathcal{R})\to \mathbb{R}$, $\cK^{\circ}\subset \dom(\mathcal{R})$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$.
%     In round $t=1, 2, \cdots, n-1$:
\State Initialize $X_1\in \cK$ arbitrarily.
\For{$t=1, 2, \cdots, n-1$}
	\State Query the oracle at $X_t$ to receive $G_t$, $Y_t$.
	\State Set
	$X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right].$
\EndFor
\State {\bf Return:} $\hat{X}_n = \frac{1}{n}\sum_{t=1}^n X_t \,.$
\end{algorithmic}
\caption{Mirror Descent with Type-I/II Oracle.
\label{alg}}
\end{algorithm}


% \begin{algorithm}
% 	\caption{Mirror Descent with Type-I/II Oracle}\label{alg}
% 	\textbf{Input}: Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$. \\
% 	Initialize $X_1\in \cK$ arbitrarily.\\
% 	In round $t=1, 2, \cdots, n-1$:
% 	\begin{itemize}
% 	\item Query the oracle at $X_t$.
% 	\item Receive $G_t$, $Y_t$.
% 	\item Update
% 	\[
% 	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
% 	\]
% 	\end{itemize}
% 	\textbf{Output}: the optimizer
% 	\[
% 	\hat{X}_n = \dfrac{1}{n}\sum_{t=1}^n X_t \,.
% 	\]
%
% \end{algorithm}

\begin{theorem}[\textit{Upper bound}]
\label{thm:ub}
Consider the class $\F=\F_{L,0}$ of convex, $L$-smooth functions whose domain includes the bounded, convex set $\cK\ne \emptyset$, $\cK \subset \R^d$. \todoc{Was this needed at the end..?}
% let $\cF$ be a class of convex functions on $\cK$ such that $f \in \cF$ is $L$-smooth for some $L>0$.
Assume that the regularization function $\mathcal{R}$ is $\alpha$-strongly convex w.r.t.\  some norm $\norm{\cdot}$, and $\cK^\circ\subseteq \dom(\mathcal{R})$.
For any $(c_1,c_2)$ type-I or any memoryless uniform $(c_1,c_2)$ type-II oracle 
with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$, 
the worst-case error (and hence the minimax error) of \cref{alg} run with an appropriate parameter setting
%for the function class $\F=\F_{L,0}$ 
can be bounded as \todoc{I would perhaps use $K_1$ and $K_2$ as in the lower bound, and give the log term in the second bound. The huge paranthesis are ugly.}
 \begin{align}
 \label{eq:MDbound1TypeI}
 \Delta_{\F_{L,0},n}^{MD, \mathrm{type-I}}(c_1,c_2) 
 %\frac{R_{\F_{L,0},n}^*(c_1,c_2)}{n}
 %&= O\left(\left(\dfrac{DC_1^{\frac{q}{p}}C_2}{n}\right)^{\frac{p}{2p+q}}\right),
 &\leq  K_1 D^{ \frac{1}{2}} C_1^{\frac{q}{2p+q} } C_2^{\frac{p}{2p+q}} n ^{-\frac{p}{2p+q}} + \frac{1}{n}\left(\EE{f(X_1)-\inf_{x \in \cK}f(x)}+\frac{DL}{\alpha}  \right),
%\vspace{-0.3cm}
 \end{align}
 \begin{align}
 \label{eq:MDbound1}
 \Delta_{\F_{L,0},n}^{MD, \mathrm{type-II}}(c_1,c_2) 
 %\frac{R_{\F_{L,0},n}^*(c_1,c_2)}{n}
 %&= O\left(\left(\dfrac{DC_1^{\frac{q}{p}}C_2}{n}\right)^{\frac{p}{2p+q}}\right),
 &\leq  K'_1 D^{ \frac{p}{2p+q}} C_1^{\frac{q}{2p+q} } C_2^{\frac{p}{2p+q}} n ^{-\frac{p}{2p+q}} + \frac{1}{n}\left(\EE{f(X_1)-\inf_{x \in \cK}f(x)}+\frac{DL}{\alpha}  \right),
%\vspace{-0.3cm}
 \end{align}
 where $D=\sup_{x,y\in \cK} \DR(x,y)$.
 For the class $\F=\F_{L,\mu,\cR}$ of $\mu$-strongly convex (w.r.t.\  $\cR$) and $L$-smooth functions, with $\alpha >2L/\mu$, 
we have
\begin{align}
%\lefteqn{
\Delta_{\F_{L,\mu,\cR},n}^{MD}(c_1,c_2) 
%} \\
& %\le  \frac{R_{\F_{L,\mu,\cR},n}^*(c_1,c_2)}{n} 
%=  \tilde{O}\left( \left(\dfrac{C_1^{\frac{q}{p}} C_2}{n} \right)^{\frac{p}{p+q}} \right)
\leq  K_2C_1^{\frac{q}{p+q}}C_2^{\frac{p}{p+q}} \left( \frac{\log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}}{n} \right)^{\frac{p}{p+q}}  + \frac{1}{n}\EE{f(X_1)-\inf_{x \in \cK}f(x)} 
\,.
\label{eq:MDbound2}
\end{align}
%In the bounds $O(\cdot)$ hides a constant that is a function of $p$, $q$, $\alpha$, $L$ and $\mu$. 
Above,
the constants $K_1$, $K'_1$ and $K_2$ depend on $p, q, \alpha, \mu$.%
\footnote{
In particular,
$K_1 = 2^{\frac{q}{2(2p+q)}} \left( \alpha^{-1}+2\alpha^{-\frac{q}{2(p+q)}} \right) \left( \frac{2p+q}{2p} \right)^{\frac{p}{2p+q}}$,
$K'_1 = 3\cdot2^{\frac{2q-p}{2p+q} }\left(2+q/p \right)^{\frac{p}{2p+q}} \left(1/\alpha\right)^{\frac{p}{2p+q}}$
and
$K_2=2^{\frac{q}{p+q}}\alpha^{-\frac{p}{p+q}}\mu^{-\frac{p}{p+q}}$.}
If the oracle is unbiased (but may be non-uniform and may have memory) and all functions in $\F$ have bounded gradients,\footnote{This follows from the smoothness if, for example, the functions in $f$  are bounded.} then the above bounds also hold for the normalized regret $\tfrac{1}{n} R_{\F}^{MD}(c_1,c_2)$ with $C_1+L/4$ in place of $C_1$ for $p=2$, and $C_1$ replaced with $L/4$ and $p$ with $2$ for $p \ge 2$.
\end{theorem}
%\begin{proof}
%See \cref{sec:appendix-md}.
%\end{proof}
The proof of this theorem follows the steps of the standard analysis of the mirror descent algorithm
 and is provided in \cref{sec:appendix-md}, mainly for completeness 
 and because it is somewhat cumbersome to extract from the existing results what properties of the oracles they use.

We next state lower bounds for both convex as well as strongly convex function classes. In particular, we observe that for convex and smooth functions the upper bound for the mirror descent scheme matches the lower bound, up to constants, whereas there is a gap for strongly convex+smooth functions.
Filling the gap is left for future work.
\begin{theorem}[\textit{Lower bound}]
\label{thm:lb-convex}
Let $n>0$ be an integer, $p,q>0$, $C_1,C_2>0$, 
$\cK\subset \R^d$ convex, closed, with  $[+1,-1]^d\subset \cK$.
%For any $v \in \{+1,-1\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
%$f^{(i)}_{v_i}(x_i) := \dfrac{\epsilon}{2} (x_i - v_i)^2$, for $i=1,\ldots,d$.
%Consider the space of functions $\F:= \{f_v \mid v  \in \{+1,-1\}^d\}$, with $\epsilon = d^{\frac{-(4p+q)}{(4p+2q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where \\$K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$. 
Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ type-I oracle 
 with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$,
 the minimax error (and hence the regret) satisfies the following bounds:
 \begin{itemize}
 \item
${\F_{L,0}(\K)}$ (Convex and smooth) w.r.t. the Euclidean norm $\|\cdot\|_2$ with $L\ge \frac12$
% Consider the class $\F_{L,0}$ of functions with domain $\cK$. \todoc{Probably the notation should be $\F_{L,0}(\cK)$?} 
\[
 \Delta_{\F_{L,0},n}^{*}(c_1,c_2) \ge K_1\sqrt{d} \, C_1^{\frac{q}{2p+q}} C_2^{\frac{p}{2p+q}} n^{-\frac{p}{2p+q}}, % \left(\dfrac{C_1^{\frac{q}{p}}C_2}{n}\right)^{\frac{p}{2p+q}},
\]
%where $K_1 =\frac{4p+q}{\sqrt{C_2}(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.
\item
${\F_{L,1}(\K)}$ ($1$-strongly convex and smooth) with $L\ge 1$
% For the class $\F_{L,1}$ of functions with domain $\K$, the minimax error \eqref{eq:minimaxerrdef} satisfies
\[
\Delta_{\F_{L,1}, n}^{*}(c_1,c_2) \ge K_2 \,  C_1^{\frac{2q}{2p+q}} C_2^{\frac{2p}{2p+q}} n^{-\frac{2p}{2p+q}}\,. % \left(\dfrac{C_1^{\frac{q}{p}} C_2}{n} \right)^{\frac{p}{p+q/2}}\,.
\]
\end{itemize}
Above,
the constants $K_1$ and $K_2$ depend on $p$ and $q$ only.%
\footnote{
In particular,
$K_1= \frac{\left(2p+q\right)^2}{2q^{\frac{q}{2p+q}}\left(4p+q\right)^{\frac{4p+q}{2p+q}}}$
and
$K_2= 2^{\frac{2p-q}{2p+q}} \frac{(2p+q)^3}{q^{\frac{2q}{2p+q}}(6p+q)^{\frac{6p+q}{2p+q}}}$.}
\end{theorem}
%The condition connecting the problem parameters is that $n$ should be large enough so that\\
%\begin{inparaenum}[\bfseries (i)]
%\item 
%$\frac{1}{\sqrt{d}}\left(\frac{1}{\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}\le \min\left( \frac{C_1 (2p+q)}{q}, \frac{2z+1}{C_1^{2z-1}(z+1)^z}\right)$ with $z=(p/q)$ for $\F_{L,0}(\K)$; and\\
%\item 
%$\!d^{\frac{-(4p+q)}{(2p+q)}}\!\!\left(\frac{1}{2\sqrt{n} K_1} \right)^{\!\!\frac{p}{p+\frac{q}{2}}} \!\!\!\le \!\!\min\!\left( \frac{C_1 (2p+q)}{q}, \frac{2z+1}{C_1^{2z-1}(z+1)^z}\!\right)$  for $\F_{L,1}(\K)$.
%\end{inparaenum}
%% where $K_1 = \frac{p}{C_2(p+\tfrac{q}{2})} \left(\frac{q}{2C_1(p+\tfrac{q}{2})}\right)^{\frac{q}{2p}}$.
%%\begin{proof}
%% See Section \ref{sec:appendix-lb-proof}.
%%\end{proof}

By continuity, the above claim can be extended to cover the case of $q=0$ (constant variance). 
For the special case of $p=0$ and $C_1>0$, which implies a constant bias, it is possible to derive an $\Omega(1)$ lower bound by tweaking the proof. On the other hand, the case of $p=0$ and $C_1=0$ (no bias) leads to an $\Omega(d/\sqrt{n})$ lower bound. The proof of the lower bound, presented in \cref{sec:appendix-lb-proof}, is obtained in the usual way by providing a family of functions and a type-I oracle such that any algorithm suffers at least the stated error on one of the functions. In particular, for $\F_{L,0}$ with $L\ge 1/2$ we use
$f_{v,\epsilon}(x)=\epsilon\left( x-v\right)+2\epsilon^2 \ln\left(1+e^{-\frac{x-v}{\epsilon}}  \right)$ with $v=\pm 1$, $\epsilon>0$, and $x \in \cK \subset \R$ for appropriate $\epsilon$. Note that for \emph{any} $\epsilon>0$, $f_{v,\epsilon}\in \F_{1/2,0}\setminus \cup_{0<\lambda<1/2} \F_{\lambda,0}$.


\begin{remark}(\textbf{\textit{Scaling}})
For any function class $\F$, by the definition of the minimax error \eqref{eq:minimaxerrdef}, it is easy to see that
$$\Delta_n^*(\mu \F, c_1,c_2) = \mu \Delta_n^*\left(\F, c_1/\mu,c_2/\mu^2\right),$$
 where $\mu \F$ denotes the function class comprised of functions in $\F$, each scaled by $\mu>0$. In particular, this relation implies that the bound for $\mu$-strongly convex function class is only a constant factor away from the bound for $1$-strongly convex function class.
\end{remark}

Table \ref{tab:mse-1} presents the upper and lower bounds for two specific choices of $p$ and $q$ (relevant in applications, as we shall see later). These bounds can be inferred from the results in Theorems~\ref{thm:ub} and~\ref{thm:lb-convex}.
%While it appears that for the strongly convex case the error becomes smaller with a larger dimension, in most applications $C_1, C_2$ will hide dimension dependent constants, and the lower bound actually increases with the dimension increasing.
%Some specific examples will be discussed in the next section.

\begin{table*}
\small
\centering
 \begin{tabular}{|c|c|c|c|c|}
% \begin{tabular}{||*5{>{\columncolor[gray]{.9}}c}||}
\toprule
% \rowcolor{gray!20}
% \multicolumn{3}{|c|}{\multirow{2}{*}{\textbf{Lower bounds}}}\\[1.7em]
% \midrule
  \multirow{2}{*}{\textbf{Oracle type}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Convex + Smooth}}} & \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Strongly Convex + Smooth}}} \\[1em]
 \midrule
% \rowcolor{gray!20}
 & \textbf{Upper bound} & \textbf{Lower bound} & \textbf{Upper bound} & \textbf{Lower bound}\\
 \midrule
\textbf{ $\delta$-bias, $\delta^{-2}$-variance} & \multirow{2}{*}{$\left(\dfrac{C_1^{2}C_2 D}{n}\right)^{1/4}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2 d^2}{n}\right)^{1/4}$}& \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1^2 C_2}{ n}\right)^{1/2}$} \\[0.5ex]
 ($p=1$, $q=2$) & & & &\\\midrule
%%%%%%%%%%%%%%%%%%%
\textbf{$\delta^2$-bias, $\delta^{-2}$-variance } & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 D}{n}\right)^{1/3}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 \sqrt{d^3}}{n}\right)^{1/3}$} & \multirow{2}{*}{$\left(\dfrac{C_1 C_2}{n}\right)^{1/2}$}  & \multirow{2}{*}{$\left(\dfrac{C_1 C_2 }{ n}\right)^{2/3}$}\\[1.4ex]
 ($p=2$, $q=2$) & & & &\\
  %\midrule
 %\textbf{No bias or variance} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$} & \multirow{2}{*}{$\dfrac{d}{\sqrt{n}}$}  & \multirow{2}{*}{$\left(\dfrac{d^2}{n}\right)^{1/2}$}  \\[0.5ex]
 %($C_1=0$, $C_2=0$) & & & &\\
\bottomrule
%%%%%%%%%%%%%%%%
\end{tabular}
\caption{Summary of upper and lower bounds on the minimax optimization error for different smooth function classes and  gradient oracles for the settings of \cref{thm:ub} and \cref{thm:lb-convex}. Note that when $\cR$ is the squared norm and $\K$ is the hypercube (as in the lower bounds), $D=\Theta(d)$ in the upper bounds and also that $C_1$, $C_2$ may hide dimension-dependent quantities for the common gradient estimators, as will be discussed later.
\todoc[inline]{Next mystery: If $D= \Theta(d)$, the upper bound in the first row scales with $d^{1/4}$, while the lower bound scales with $d^{1/2}$??? Similar contradiction for the second row.}
}
\label{tab:mse-1}
\end{table*}


% \begin{theorem}[\textit{Lower bound: Strongly convex}]
% \label{thm:lb-strongly-convex}
% For any $v \in \{+\nu,-\nu\}^d$ and $x \in \cK\subset \R^d$, define $f_v := \sum_{i=1}^d f^{(i)}_{v_i}(x_i)$, where
% $f^{(i)}_{v_i}(x_i) := \dfrac{1}{2} x^2 - v x$, for $i=1,\ldots,d$.
% Consider the space of functions $\F:= \{f_v \mid v  \in \{+\nu,-\nu\}^d\}$, with $\nu = d^{\frac{-(4p+q)}{(2p+q)}}\left(\frac{1}{2\sqrt{n} K_1} \right)^{\frac{p}{p+\frac{q}{2}}}$, where $K_1$ is as defined in Theorem \ref{thm:lb-convex}.
% Then, for any algorithm that observes $n$ random elements from a $(c_1,c_2)$ type-I oracle
%  with $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$,
%  the minimax error \eqref{eq:minimaxerrdef} satisfies
% \[
% \Delta_n^{*}(\F, c_1,c_2) \ge \dfrac{1}{4}  \left(\dfrac{1}{2 K_1 \sqrt {d n}}\right)^{\frac{2p}{p+\frac{q}{2}}}.
% \]
% \end{theorem}
% \begin{proof}
%  See Appendix \ref{sec:appendix-lbscconvex}.
% \end{proof}


% \todop{Extend the result to the case when the algorithm can choose $\delta$ in every step - will update the proof for adaptive $\delta$ soon}
