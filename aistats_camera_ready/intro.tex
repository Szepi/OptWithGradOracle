%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
In the current era of big data and vast computational resources, gradient based convex optimization methods are more popular than ever. Several models, using different types of gradient information, are considered in the literature. These models range from accessing the full gradient to constructing gradient estimates only from observing samples from the target function (cf. \citealp{nesterov2004introductory,DeGliNe14,HaLe14:SOC,PoTsy90,flaxman2005online,AbHaRa08,AgDeXi10,Ne11:TR,AgFoHsuKaRa13:SIAM,katkul,kushcla,spall1992multivariate,spall1997one,Dip03:AoS,bhatnagar-book,duchi2015optimal}). In this paper, we present and analyze a novel framework for convex optimization with biased gradient oracles, which encompasses several models considered before.

In this model, an optimization algorithm can query an oracle repeatedly, and the oracle returns a noisy version of the gradient (or a subgradient for non-differentiable functions) and a point in the vicinity of the query point, whereas in an online setting the cost is incurred. In particular, the algorithm controls the distance to the query point, which in turn controls the bias and variance of the returned gradient estimate in an a priori known way. 
Gradient oracles have been considered in the literature before:
Several previous works assume that the accuracy requirements hold with probability one 
\citep{dAsp08,Baes09,DeGliNe14} or consider adversarial noise \citep{SchRoBa11}. Gradient oracles with stochastic noise, which is central to our development, were also considered \citep{JN11a,Hon12,DvoGa15}; however, these papers assume that the bias and the variance are controlled separately, and consider the performance of special algorithms (in some cases in special setups).  

The main feature of our model is that we allow stochastic noise, control of the bias and the variance, and we also consider lower bounds on the achievable error rates. Our gradient oracle model applies to several gradient estimation techniques extensively used in the literature, mostly for the case when the gradient is estimated only based on noisy observations of the target function \citep{katkul,kushcla,spall1992multivariate,spall1997one,Dip03:AoS,bhatnagar-book,duchi2015optimal}. A particularly interesting application of our model is bandit convex optimization, where an algorithm can only query a single function value in each round, contaminated with noise \citep{PoTsy90,flaxman2005online,AbHaRa08,AgDeXi10,Ne11:TR,AgFoHsuKaRa13:SIAM,HaLe14:SOC}. As it turns out, the gradient oracles, which we construct, capture the bias-variance properties of the gradient estimates used in the above papers, reducing the analysis to our problem. Furthermore, previous practical attempts to bandit convex optimization are all centered around constructing gradient estimates that fit our model.
%((some issues with whether you can cancel the noise; we assume you cannot))A

In this paper, we consider the optimization accuracy in both the stochastic and online bandit convex optimization (BCO) setting.
In the stochastic BCO setting, an algorithm repeatedly queries the oracle at different points in order to minimize the optimization error. On the other hand, in an online BCO setting, the aim is to minimize the regret, which roughly translates to ensuring that the sum of the function values at the points returned by the oracle is not too far from the optimal value (see Section \ref{sec:sbco} for a detailed description).
We  provide upper and lower bounds on the minimax optimization error (or regret) for several oracle models, which correspond to different ways of quantifying the bias-variance tradeoff of the gradient estimate. In particular, we provide matching upper and lower bounds for optimizing smooth, convex functions. We do not claim to invent methods for proving upper bounds, as the methods we use have been developed for special cases for a long time by now (see the references above).
Our main contribution lies in abstracting away the properties of gradient estimation procedures
to create our new oracle model, in which we are able to study not only upper bounds, but also lower bounds about the rate of convergence.
%(earlier work of \citet{Chen88:LB-AoS} considered a related lower bound on the converge of the iterate instead of the function value).
Note that our oracle model does not capture the full strength of the gradient estimates used in previous work, but it fully describes the properties of the estimates that \emph{so far have been used in their analysis}.
As a consequence, our lower bounds show that the known minimax regret of $\sqrt{T}$ \citep{BubeckDKP15,BuEl15}
of online and stochastic bandit convex optimization\todoa{add reference to stochastic} cannot be achieved by the current analysis techniques of the currently used gradient estimation procedures, and our lower bounds even invalidate the claimed weaker upper bound of \citet{DeElKo15}.
%Hence, an interesting corollary to our results is that algorithms which are purely based on gradient estimation will not be able to achieve the minimax regret in online bandit convex optimization using the current proof techniques.

%The rest of the paper is organized as follows: The problem is introduced in \cref{sec:problem}. Our upper and lower bounds on the minimax error are presented in \cref{sec:results}. Sections \ref{sec:sbco}--\ref{sec:obco} describe applications to stochastic and online BCO , respectively. Related general gradient oracle models are discussed in detail in \cref{sec:related}, while proofs are given in \cref{sec:proofs}.

The rest of the paper is organized as follows: The oracle model is introduced in Section~\ref{sec:problem}. The main upper and lower bounds are provided in Section~\ref{sec:results}, with applications to online and stochastic BCO in Sections~\ref{sec:sbco} and~\ref{sec:obco}. All proofs can be found in the extended version of the paper \citep{HuPrGySz16long}.


\todoc[inline]{Check out the book Stochastic Adaptive Search for Global Optimization by Zabinsky (2003, Springer).
It has results for convex and nonconvex, smooth, zeroth order optimization, with no noise in the observations. 
}

\if0
strongly convex


Bandit convex optimization:

Online adversarial setting. Optimal rate is $\Theta(\sqrt{T})$.
No one knows general practical algorithms achieving this rate.
Strongly convex + smooth: \cite{hazan2014bandit}

Stochastic optimization setting: ??? Shamir's paper \cite{shamir2012complexity}??
Lower bounds, upper bounds for the general case?
\cite{hazan2014bandit} for upper bound.

All kind of papers about how to estimate gradients going back to maybe 70s in Russia.

One-point: \cite{flaxman2005online}

Two-point: \cite{AgDeXi10}

Lower bds: \cite{raginsky2011information} \cite{Chen88:LB-AoS}

Ellipsoid: \cite{AgFoHsuKaRa13:SIAM}



 framework
What is the problem? (New problem: Convex Optimization with Biased Gradient Oracles)
What are the results? Matching lower and upper bounds (several oracle models and relation between them; cumulative regret and optimization error (aka simple regret) and relation between them -- this is mostly known; we consider both constrained, unconstrained)
Why should we care?
Reason 1: For bandit convex optimization we can construct these oracles and reduce to this problem.
Reason 2: Previous practical attempts for bandit convex optimization are all centered around such gradient estimates.
((some issues with whether you can cancel the noise; we assume you cannot))

Main message: Everyone was trying to get better rates with these algorithms (e.g., open question whether in the smooth case you can get better rates for these algorithms -- now we see this is not possible).
\fi
