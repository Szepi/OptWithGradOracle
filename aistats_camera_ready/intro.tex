%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
In the current era of big data and vast computational resources, gradient based convex optimization methods are more popular than ever. Several optimization problems, differing in what can be observed about the objective function, are considered in the literature. These models range from accessing the full gradient to constructing gradient estimates only from observing samples from the objective function (cf. \citealp{nesterov2004introductory,DeGliNe14,HaLe14:SOC,PoTsy90,flaxman2005online,AbHaRa08,AgDeXi10,Ne11:TR,AgFoHsuKaRa13:SIAM,katkul,kushcla,spall1992multivariate,spall1997one,Dip03:AoS,bhatnagar-book,duchi2015optimal}). In this paper, we present and analyze a novel framework for convex optimization with biased gradient oracles, which encompasses most of what has been done in the bandit framework where the algorithms observe noisy point-evaluations of the target framework and use these to construct gradient estimators.

In our new framework, an optimization algorithm can query an oracle repeatedly to get a noisy and biased version of the gradient (or a subgradient for non-differentiable functions) and a response point in the vicinity of the query point, which, in the online setting, serves as the point where the cost is incurred. In addition to providing the query point where the gradient is to be estimated, the algorithm using the oracle also provides a parameter that controls the distance between the response and the query points, as well as the bias and variance of the returned gradient estimate. 
Gradient oracles have been considered in the literature before \citep{dAsp08,Baes09,SchRoBa11,DeGliNe14}. The main feature of our model is that we allow stochastic noise (and bias), while most \todoc{Why most!? Which one is the exception?}
of the previous works either assume that the accuracy requirements hold with probability one, or consider adversarial noise. Our gradient oracle model applies to several gradient estimation techniques extensively used in the literature, mostly for the case when the gradient is estimated only based on noisy observations of the objective function \citep{katkul,kushcla,spall1992multivariate,spall1997one,Dip03:AoS,bhatnagar-book,duchi2015optimal}. A particularly interesting application of our model is the widely studied bandit convex optimization problem,
mentioned above,
where all previous algorithms 
essentially use gradient estimates and first-order methods 
%, where an algorithm can only query a single function value in each round, contaminated with noise 
\citep{PoTsy90,flaxman2005online,AbHaRa08,AgDeXi10,Ne11:TR,AgFoHsuKaRa13:SIAM,HaLe14:SOC}.

%which reduces to our problem with appropriately chosen oracles.
%By capturing the bias-variance properties of the gradient estimates used in the above papers, reducing the analysis to our problem.
%As it turns out, the gradient oracles, which we construct, capture.
%Furthermore, previous practical attempts to bandit convex optimization are all centered around constructing gradient estimates that fit our model.
%((some issues with whether you can cancel the noise; we assume you cannot))A

In this paper, we consider the optimization accuracy in both the stochastic and online bandit convex optimization (BCO) setting.
In the stochastic BCO setting, an algorithm repeatedly queries the oracle at different points in order to find a near minimizer of the objective function. Here, performance is measured in terms of the excess cost of the last point compared to  that of the minimizer,
also known as the optimization error. In the online setting, each query points incurs a cost and performance is measured
in terms of the total excess cost of these points, or the regret
%On the other hand, in an online BCO setting, the aim is to minimize the regret, which roughly translates to ensuring
 %that the sum of the function values at the points returned by the oracle is not too far from the optimal value 
 (see Section \ref{sec:sbco} for a detailed description).
We  provide upper and lower bounds on the minimax optimization error (or regret) for several oracle models, which correspond to different ways of quantifying the bias-variance tradeoff of the gradient estimate. In particular, we provide matching upper and lower bounds for optimizing smooth, convex functions. We do not claim to invent methods for proving upper bounds, as the methods we use have been developed for special cases for a long time by now (see the references above),
but our main contribution lies in abstracting away the properties of gradient estimation procedures, 
thereby unifying previous analysis, providing a concise summary and explanation of differences between previous works.
More importantly, our framework also allows to prove lower bounds about for any algorithm that relies on gradient
estimation oracles of the type our framework captures
(earlier work of \citet{Chen88:LB-AoS} considered a related lower bound on the converge of the iterate instead of the function value).

Note that our oracle model does not capture the full strength of the gradient estimates used in previous work, but it fully describes the properties of the estimates that \emph{so far have been used in their analysis}.
As a consequence, our lower bounds show that the known minimax regret of $\sqrt{T}$ \citep{BubeckDKP15,BuEl15}
of online and stochastic bandit convex optimization\todoa{add reference to stochastic} cannot be shown to hold
for any algorithm that uses current gradient estimation procedures, unless the proof exploited finer properties
of the gradient estimators than used in all prior works. In particular,
our lower bounds even invalidate the claimed (weaker) upper bound of \citet{DeElKo15}.
%Hence, an interesting corollary to our results is that algorithms which are purely based on gradient estimation will not be able to achieve the minimax regret in online bandit convex optimization using the current proof techniques.

%The rest of the paper is organized as follows: The problem is introduced in \cref{sec:problem}. Our upper and lower bounds on the minimax error are presented in \cref{sec:results}. Sections \ref{sec:sbco}--\ref{sec:obco} describe applications to stochastic and online BCO , respectively. Related general gradient oracle models are discussed in detail in \cref{sec:related}, while proofs are given in \cref{sec:proofs}.

The rest of the paper is organized as follows: The oracle model is introduced in Section~\ref{sec:problem}. The main upper and lower bounds are provided in Section~\ref{sec:results}, with applications to online and stochastic BCO in Sections~\ref{sec:sbco} and~\ref{sec:obco}. All proofs can be found in the extended version of the paper \citep{HuPrGySz16long}.


\todoc[inline]{Check out the book Stochastic Adaptive Search for Global Optimization by Zabinsky (2003, Springer).
It has results for convex and nonconvex, smooth, zeroth order optimization, with no noise in the observations. 
}

\if0
strongly convex


Bandit convex optimization:

Online adversarial setting. Optimal rate is $\Theta(\sqrt{T})$.
No one knows general practical algorithms achieving this rate.
Strongly convex + smooth: \cite{hazan2014bandit}

Stochastic optimization setting: ??? Shamir's paper \cite{shamir2012complexity}??
Lower bounds, upper bounds for the general case?
\cite{hazan2014bandit} for upper bound.

All kind of papers about how to estimate gradients going back to maybe 70s in Russia.

One-point: \cite{flaxman2005online}

Two-point: \cite{AgDeXi10}

Lower bds: \cite{raginsky2011information} \cite{Chen88:LB-AoS}

Ellipsoid: \cite{AgFoHsuKaRa13:SIAM}



 framework
What is the problem? (New problem: Convex Optimization with Biased Gradient Oracles)
What are the results? Matching lower and upper bounds (several oracle models and relation between them; cumulative regret and optimization error (aka simple regret) and relation between them -- this is mostly known; we consider both constrained, unconstrained)
Why should we care?
Reason 1: For bandit convex optimization we can construct these oracles and reduce to this problem.
Reason 2: Previous practical attempts for bandit convex optimization are all centered around such gradient estimates.
((some issues with whether you can cancel the noise; we assume you cannot))

Main message: Everyone was trying to get better rates with these algorithms (e.g., open question whether in the smooth case you can get better rates for these algorithms -- now we see this is not possible).
\fi
