%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
%In the proof, we use \cref{lem:ub} to deal with noisy and biased gradient, which is essentially Theorem~C.4 of \cite{MahdaviPhd:2014}, and also identical to Theorem~6.3 of \cite{Bu:Convex14}, who cites \citet{Dekel:minibatch12} as the source. The lemma and its proof can be found below.

In this section we prove Theorem~\ref{thm:ub}. First we derive the bounds for the optimization settings and then for the regret. 

\subsection{Stochastic optimization}

The proof for the stochastic optimization scenario is based on Lemma~\ref{lem:ub} stated below.
This is essentially Theorem~C.4 of \citet{MahdaviPhd:2014}, and also identical to Theorem~6.3 of \citet{Bu:Convex14}, who cites \citet{Dekel:minibatch12} as the source. For completeness, the proof of the lemma is given in Section~\ref{sec:lemub-proof}.
%The lemma is somewhat more general than what we need (we will only need it for the case when $\beta_t=0$); 
%the general form is presented because its proof is not significantly different than the simpler form and it may find
%other applications in the future.
\begin{lemma}
\label{lem:ub}
Let $({\cS}_t)_{t}$ be a filtration such that $X_t$ is ${{\cS}}_t$-measurable.
Let $\overline G_t = \EE{G_t|{{\cS}}_t}$
and assume that the nonnegative real-valued deterministic sequence $(\beta_t)_{1\le t\le n}$ is such that
$\norm{\overline G_t - \nabla {f}(X_t)}_* \le \beta_t$ holds almost surely.
Further, assume that $\mathcal{R}$ is $\alpha$-strongly convex with respect to $\norm{\cdot}$, $D=\sup_{x,y\in \cK} D_{\mathcal{R}}(x,y) < \infty$,  and let $\eta_t = \frac{\alpha}{a_t+L}$ for some increasing
sequence $(a_t)_{t=1}^{n-1}$ of numbers. Then, the cumulative loss of Algorithm~\ref{alg} for a fixed convex and $L$-smooth  function $f$ can be bounded as
\begin{align*}
\EE{ \sum_{t=1}^n {f}(X_t) - {f}(x) }
\le 	 \EE{{f}(X_1)-{f}(x)}+
  \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t
 +\frac{D(a_{n-1}+L)}{\alpha} +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,,
\end{align*}
where $\sigma_t^2 = \EE{ \norm{G_t-\overline G_t}_*^2}$ is the ``variance'' of $G_t$.

If ${{f}}$ is also $\mu$-strongly convex with respect to $\mathcal{R}$ with $\mu > 2L/\alpha$, then letting $\eta_t = \dfrac{2}{\mu t}$ and $a_t = \alpha \mu t/2-L > 0$, the cumulative loss of  Algorithm~\ref{alg} can be bounded as
\begin{align*}
 \EE{ \sum_{t=1}^n {f}(X_t) - {f}(x) }
\le 	 \EE{{f}(X_1)-{f}(x)}+
 \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t
 +\sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,.
\end{align*}
\end{lemma}

Now we can easily prove the theorem.
%By the discussion in \cref{sec:problem}, we have $\Delta^*_{\F,n}(c_1,c_2) \le R^*_{\F,n}(c_1,c_2)/n$ by Jensen's inequality, so it is enough to bound the regret for the online case.
First we consider the case of smooth and convex functions. We select $$\eta_t = \alpha/(a_t+L)$$ as in the lemma with 
$a_t=a t^r$ for some $0<r<1$. For type-I oracles, the result immediately follows by substituting $\beta_t = C_1\delta^p$, $\sigma^2_t = C_2 \delta^{-q}$, using that $\sum_{t=1}^{n-1} t^{-r} \le 1 +\int_1^n t^{-r} \le n^{1-r}/(1-r)$:
\begin{align}
\MoveEqLeft
\frac{1}{n} \EE{ \sum_{t=1}^n f( X_t) - \inf_{x \in \cK} \sum_{t=1}^n f(x)} \nonumber \\
&\le \frac{1}{n}\left(\EE{f(X_1)-\inf_{x \in \cK}f(x)}+\frac{DL}{\alpha}  \right) +\sqrt{\dfrac{2D}{\alpha}} C_1\delta^p
+\frac{Da}{\alpha} n^{r-1}+\dfrac{C_2 \delta^{-q}}{2a(1-r)}n^{-r} \,.
\label{eq:ubToBeOptTypeI}
 \end{align}
 Choosing 
 \begin{align*}
 r &= \tfrac{p+q}{2p+q},  \\
 a &= 2^{\frac{q}{2(2p+q)}}\left(\tfrac{2p+q}{2p}\right)^{\frac{p}{2p+q}} D^{-\frac{1}{2}} C_1^{\frac{q}{2p+q}} C_2^{\frac{p}{2p+q}} \\
 \delta &= \alpha^{\frac{1}{2(p+q)}}\left(\tfrac{2p+q}{4p}\right)^{\frac{1}{2p+q}} C_1^{-\frac{2}{2p+q}} C_2^{\frac{1}{2p+q}}n^{-\frac{1}{2p+q}},
 \end{align*}
the last $3$ terms in \eqref{eq:ubToBeOptTypeI} are optimized to
 \[
 K_1 D^{1/2} C_1^{q/(2p+q)} C_2^{p/(2p+q)} n ^{-p/(2p+q)} \,,
 \]
 with
 $K_1 = 2^{\frac{q}{2(2p+q)}} \left( \alpha^{-1}+2\alpha^{-\frac{q}{2(p+q)}} \right) \left( \frac{2p+q}{2p} \right)^{\frac{p}{2p+q}}$. This implies \eqref{eq:MDbound1TypeI}.

For type-II oracles, from the bias condition in Definition~\ref{def:oracle2} and using that the oracle is memoryless and uniform, we get
\begin{align*}
 \frac{1}{n} \EE{ \sum_{t=1}^n f(X_t) - \inf_{x \in \cK}\sum_{t=1}^n f(x) }
 \le \frac{1}{n}\EE{ \sum_{t=1}^n \tilde{f}(X_t) - \inf_{x \in \cK}\sum_{t=1}^n \tilde{f}(x) } +2C_1 \delta^p
 \,.
\end{align*}

Given $\overline{G}_t=\EE{G_t} = \nabla \tilde{f}(X_t)$, where $\tilde{f}\in \cF_{L,0}$ is convex and smooth,
the result immediately follows by applying Lemma~\ref{lem:ub} to $\tilde{f}$.
Substituting
 $\beta_t = 0$ (since we have a type-II oracle), $\sigma^2_t = C_2 \delta^{-q}$, respectively, and using the bias condition again, we obtain
 \begin{align}
\MoveEqLeft
\frac{1}{n} \EE{ \sum_{t=1}^n f( X_t) - \inf_{x \in \cK} \sum_{t=1}^n f(x)} \nonumber \\
&\le \frac{1}{n}\left(\EE{\tilde{f}(X_1)-\inf_{x \in \cK}\tilde{f}(x)}+\frac{DL}{\alpha}  \right) %\nonumber
+\frac{Da}{\alpha} n^{r-1}+\dfrac{C_2 \delta^{-q}}{2a(1-r)}n^{-r}+ 2 C_1\delta^p \\
&\le \frac{1}{n}\left(\EE{f(X_1)-\inf_{x \in \cK}f(x)}+\frac{DL}{\alpha}  \right) %\nonumber
+\frac{Da}{\alpha} n^{r-1}+\dfrac{C_2 \delta^{-q}}{2a(1-r)}n^{-r}+ \left(2+\dfrac{2}{n}\right)C_1\delta^p \,.
\label{eq:ubToBeOpt}
 \end{align}
 Choosing 
 \begin{align*} 
 r &= \tfrac{p+q}{2p+q}, \\  
a&= \left(2+\tfrac{2}{n}\right)^{\frac{q}{2p+q}}\left(\tfrac{2p+q}{2p}\right)^{\frac{p}{2p+q}} \left(\tfrac{D}{\alpha}\right)^{-\frac{p+q}{2p+q}}  C_1^{\frac{q}{2p+q}} C_2^{\frac{p}{2p+q}} \\
 \delta &=  \left(2+\tfrac{2}{n}\right)^{-\frac{2}{2p+q}}\left(\tfrac{2p+q}{2p}\right)^{\frac{1}{2p+q}} \left(\tfrac{D}{\alpha}\right)^{\frac{1}{2p+q}}  C_1^{-\frac{2}{2p+q}} C_2^{\frac{1}{2p+q}} n^{-\frac{1}{2p+q}}, 
 \end{align*}
%  \left( \tfrac{C_2}{4aC_1}\tfrac{2p+q}{p}\tfrac{n}{n+1}\right)^{\frac{1}{p+q}}n^{-\frac{1}{2p+q}}$  and
%$a^{2p+q} =2^{q-p}\left( 2+\tfrac{q}{p} \right)^p\left(1+ \tfrac{1}{n} \right)^q \left( \tfrac{\alpha}{D} \right)^{p+q}C_1^q C_2^p $,
the last $3$ terms in \eqref{eq:ubToBeOpt} are optimized to
 \[
 K'_1 D^{p/(2p+q)} C_1^{q/(2p+q)} C_2^{p/(2p+q)} n ^{-p/(2p+q)} \,,
 \]
where $K_1'= 3\left(2+\frac{2}{n}\right)^{\frac{q}{2p+q}}\left(\frac{2p+q}{2p}\right)^{\frac{p}{2p+q}}  \alpha^{-\frac{p}{2p+q}}$.
 %$(K'_1/3)^{2p+q} \le 2^{2q-p}\left(2+q/p \right)^p \left(1/\alpha\right)^{p}$. 
 This implies \eqref{eq:MDbound1}.
% \begin{align*}
 
When $\tilde{f} \in \cF_{L,\mu, \cR}$ is $L$-smooth and $\mu$-strongly convex, for
$\eta_t = 2/(\mu t)$ and \\
$
\delta^{p+q} =  \tfrac{C_2\left( \log n+1+\tfrac{\alpha \mu}{\alpha \mu -2L}\right)}{\sqrt{2D\alpha} \mu C_1 n} \,,
$
we similarly obtain, for type-I oracle, 
 \begin{align*}
 \MoveEqLeft
\frac{1}{n} \EE{ \sum_{t=1}^n f( X_t) - \inf_{x \in \cK} \sum_{t=1}^n f(x)} -\frac{1}{n}\EE{f(X_1)-\inf_{x \in \cK}f(x)}\\
&\le \sqrt{\dfrac{2D}{\alpha}} C_1\delta^p+\dfrac{C_2 \delta^{-q}}{\alpha \mu n} \sum_{t=1}^{n-1}\dfrac{1}{t-\dfrac{2L}{\alpha \mu}}\\
&\le \sqrt{\dfrac{2D}{\alpha}}C_1\delta^p+\dfrac{C_2 }{\alpha \mu}\delta^{-q} \dfrac{\log n+1+\alpha \mu/(\alpha \mu-2L)}{n}\\
&\le 2^{\frac{q}{2(p+q)}}\alpha^{-\frac{2p+q}{2(p+q)}}\mu^{-\frac{p}{p+q}} D^{\frac{q}{2(p+q)}}C_1^{\frac{q}{p+q}}C_2^{\frac{p}{p+q}} \left( \frac{\log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}}{n} \right)^{\frac{p}{p+q}}\,.
 \end{align*}

For type-II oracle, choosing
$
\delta^{p+q} =  \tfrac{C_2\left( \log n+1+\tfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)} \,,
$
we get
 \begin{align*}
 \MoveEqLeft
\frac{1}{n} \EE{ \sum_{t=1}^n f( X_t) - \inf_{x \in \cK} \sum_{t=1}^n f(x)} -\frac{1}{n}\EE{f(X_1)-\inf_{x \in \cK}f(x)}\\
&\le (2+\dfrac{2}{n})C_1\delta^p+\dfrac{C_2 \delta^{-q}}{\alpha \mu n} \sum_{t=1}^{n-1}\dfrac{1}{t-\dfrac{2L}{\alpha \mu}}\\
&\le (2+\dfrac{2}{n})C_1\delta^p+\dfrac{C_2 }{\alpha \mu}\delta^{-q} \dfrac{\log n+1+\alpha \mu/(\alpha \mu-2L)}{n}\\
&\le 2^{\frac{q}{p+q}}\alpha^{-\frac{p}{p+q}}\mu^{-\frac{p}{p+q}} K_2C_1^{\frac{q}{p+q}}C_2^{\frac{p}{p+q}} \left( \frac{\log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}}{n} \right)^{\frac{p}{p+q}}\,,
 \end{align*}
where the bound is optimized in the last step via the choice of $\delta$.
%$\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n}$,
%$\delta^{p+q} =  \dfrac{C_2\left( \log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)}$,
%where
%$K_2^{p+q}=\sqrt{2}^{2p+3q}D^{q/2}\alpha^{-p-q/2}\mu^{-p}$.



\subsection{Online optimization}
The proof in this section follows closely the derivation of \citet{saha2011improved}. First we consider the case of type-II oracles.

Let $\cS_t$ denote the $\sigma$-algebra of all random events up until and including the selection of $X_t$. Since the oracle is unbiased, that is,  $\EE{Y_t|\cS_t}=X_t$, we have
\begin{equation}
\EE{\tf_t(Y_t)-\tf_t(X_t)|\cS_t} \le \EE{\ip{\nabla \tf_t(X_t),Y_t-X_t}+\tfrac{L}{2}\|X_t-Y_t\|^2|\cS_t} \le L\delta^2/2~.
\label{eq:YX}
\end{equation}
This inequality, the definition of type-II oracles, and the convexity of $\tf_t$ implies, for any $x \in \cK$,
\begin{align}
\EE{\sum_{t=1}^n f_t(Y_t)} - \sum_{t=1}^n f_t(x) 
& \le \EE{\sum_{t=1}^n \tf_t(Y_t) -  \sum_{t=1}^n \tf_t(x)} + 2 n C_1 \delta^{p} \nonumber \\
& \le \EE{\sum_{t=1}^n \tf_t(X_t) -  \sum_{t=1}^n \tf_t(x)} + 2 n C_1 \delta^{p} + \frac{n L \delta^2}{2} \nonumber \\
& \le \EE{\sum_{t=1}^n \ip{\nabla \tf_t(X_t), X_t-x }} + 2 n C_1 \delta^{p} + \frac{n L \delta^2}{2} \label{eq:t2-lin} \\
& = \EE{\sum_{t=1}^n \ip{G_t, X_t-x }} + 2 n C_1 \delta^{p} + \frac{n L \delta^2}{2}~.
\label{eq:t2-grad}
\end{align}
Instead of Lemma~\ref{lem:mdlinregret} used in the optimization proof, we apply the prox-lemma \citep[see, e.g.,][]{Beck2003mirror, NeJuLaSh09}:
\begin{equation}
\label{eq:proxlemma}
\ip{G_t,X_t-x} \le \frac{1}{\eta_t}\big(D_\cR(x,X_t)-D_\cR(x,X_{t+1})\big) + \eta_t \frac{\|G_t\|_*^2}{2\alpha}~.
\end{equation}
Summing up the above bound for all $t$, the divergence terms telescope, since
\begin{align}
\lefteqn{\sum_{t=1}^{n-1} \frac{1}{\eta_t} \left(\DR(x,X_t)-\DR(x,X_{t+1})\right)}
 \nonumber \\
&= \DR(x,X_1) \frac{1}{\eta_1} + \DR(x,X_2) \left(\frac{1}{\eta_2}-\frac{1}{\eta_1}\right)
+ \ldots+\DR(x,X_{n-1}) \left(\frac{1}{\eta_{n-1}} -\frac{1}{\eta_{n-2}}\right)\nonumber\\
& \quad- \frac{1}{\eta_{n-1}} \DR(x,X_n) \nonumber \\
& \le \frac{D}{\eta_1} + D \sum_{t=2}^{n-1} \left(\frac1{\eta_{t}}-\frac1{\eta_{t-1}}\right) \nonumber \\
& = \frac{D}{\eta_{n-1}}\,,  \label{eq:div-telescope}
\end{align}
where the inequality results from the fact that $\{\eta_t\}$ is non-increasing.

To bound the last term in \eqref{eq:proxlemma}, we use the assumption $\|\nabla \tf(x)\|_* \le M$ for all $x \in \cK$ to obtain
\begin{equation}
\label{eq:t2-g2}
\EE{\norm{G_t}_*^2 | \cS_t} \le 2\EE{\norm{G_t - \nabla \tf_t(X_t)}_*^2 + \norm{\nabla \tf_t(X_t)}_*^2 \Big| \cS_t}
\le 2(M^2 + C_2 \delta^{-q}),
\end{equation}
Combining the latter with \eqref{eq:t2-grad}, \eqref{eq:proxlemma}, and \eqref{eq:div-telescope}, we obtain, for any $x \in \cK$,
\begin{align}
\EE{\sum_{t=1}^n f_t(Y_t)} - \sum_{t=1}^n f_t(x) 
& \le \frac{D}{\eta_{n-1}}  + \sum_{t=1}^n \eta_t \frac{M^2 + C_2 \delta^{-q}}{\alpha} +  2 n C_1 \delta^{p} + \frac{n L \delta^2}{2}~.
\label{eq:t2-full}
\end{align}
Setting the parameters 
$\delta=(\frac{q}{2p'})^{\frac{2}{2\hp+q}} (\frac{C_2 D}{\alpha \hC_1^2})^{\frac{1}{2\hp+q}} n^{-\frac{1}{2\hp+q}}$, 
where $\hp=\min\{p,2\}$, $\hC_1=C_1 \indic{p \le 2} + (L/4) \indic{p \ge 2}$ (i.e., $\hp$ is the dominating exponent from $\delta^p$ and $\delta^2$, and $\hC_1$ is the coefficient of the dominating term), $\eta_t=D^{\frac{\hp+q}{2\hp+q}} (\frac{q}{2\hp})^{\frac{q}{2\hp+q}} (\frac{C_2}{\alpha})^{-\frac{\hp}{2\hp+q}} \hC_1^{-\frac{q}{2\hp+q}} n^{-\frac{\hp+q}{2\hp+q}}$ gives, when $f_t \in \F_{L,0}$ for all $t$, 
\begin{equation}
\frac{1}{n}\left(\EE{\sum_{t=1}^n f_t(Y_t)} - \inf_{x \in \cK} \sum_{t=1}^n f_t(x) \right) = O\left( \hC_1^{\frac{q}{2\hp+q}} (C_2 D)^{\frac{\hp}{2\hp+q}} n^{-\frac{\hp}{2\hp+q}} \right)
\label{eq:t2-bound}
\end{equation}
where the coefficient of the main term equals $K=2^{1+\frac{q/2}{2\hp+q}}(2\hp+q) (2\hp \alpha)^{-\frac{\hp}{2\hp+q}} q^{-\frac{q}{2\hp+q}}$.

When the set of functions is also strongly convex, in \eqref{eq:t2-lin} we can use strong convexity instead of linearization:
\[
\tf(X_t) - \tf(x) \le \ip{\nabla \tf_t,X_t-x} - \frac{\mu}{2} D_\cR(x,X_t) = \EE{\ip{ G_t,X_t-x}| \cS_t} - \frac{\mu}{2} D_\cR(x,X_t) ~.
\]
Combining this with \eqref{eq:proxlemma} and \eqref{eq:t2-g2} gives the well-known variant of \eqref{eq:t2-full} for strongly convex loss functions \citep{BaHaRa07} for the choice $\eta_t=2/(t\mu)$:
\begin{align*}
\EE{\sum_{t=1}^n f_t(Y_t)}- \sum_{t=1}^n f_t(x) 
& \le \sum_{t=1}^n  \frac{\EE{\|G_t\|_*^2}}{ t \alpha \mu} + 2 n C_1 \delta^{p} + \frac{n L \delta^2}{2} \\
&\le \frac{\max_t \EE{\|G_t\|_*^2}}{\alpha \mu} (1+\log n) + 2 n C_1 \delta^{p} + \frac{n L \delta^2}{2} \\
&\le \frac{2(M^2 + C_2 \delta^{-q})}{\alpha \mu} (1+\log n)+  2 n C_1 \delta^{p} + \frac{n L \delta^2}{2}~.
\end{align*}
Setting $\delta=(\frac{C_2 q (1+\log n)}{\alpha \mu \hC_1 \hp n})^{\frac{1}{\hp+q}}$, we obtain 
\begin{equation}
\label{eq:t2-sc}
\frac{1}{n}\left(\EE{\sum_{t=1}^n \tf_t(Y_t)} - \inf_{x \in \cK} \sum_{t=1}^n \tf_t(x)\right) 
= O\left(\hC_1^{\frac{q}{\hp+q}} C_2^{\frac{\hp}{\hp+q}} n^{-\frac{\hp}{\hp+q}} (1+\log n)^{\frac{\hp}{\hp+q}} \right),
\end{equation}
where the coefficient of the leading term is $K'=(\hp+q)\hp^{-\frac{\hp}{\hp+q}} q^{-\frac{q}{\hp+q}} (\alpha \mu)^{-\frac{\hp}{\hp+q}}$.

For a type-I oracle, we need a slightly different derivation. Using the oracle's definition, similarly to \eqref{eq:t2-grad}, we get for evry $x \in \cK$,
\begin{align}
\EE{\sum_{t=1}^n f_t(Y_t)} - \sum_{t=1}^n f_t(x) 
& \le \EE{\sum_{t=1}^n f_t(X_t) -  \sum_{t=1}^n f_t(x)} + \frac{n L \delta^2}{2} \nonumber \\
& \le \EE{\sum_{t=1}^n \ip{\nabla f_t(X_t), X_t-x }} + \frac{n L \delta^2}{2} \nonumber \\
& = \EE{\sum_{t=1}^n \ip{G_t, X_t-x } + \ip{\nabla f_t(X_t)-G_t,X_t-x}} + \frac{n L \delta^2}{2}  \nonumber \\
& \le \EE{\sum_{t=1}^n \ip{G_t, X_t-x }} + C_1 \delta^p \sum_{t=1}^n \EE{\|X_1-x\|} +  \frac{n L \delta^2}{2} \nonumber \\
& \le \EE{\sum_{t=1}^n \ip{G_t, X_t-x }}  + 2 n R C_1 \delta^p +  \frac{n L \delta^2}{2},
\label{eq:t1-grad}
\end{align}
where the second to last inequality holds by the Cauchy-Schwarz inequality, and in the last step we used our assumption that $\sup_{x \in \cK} \|x\| \le R$. We now proceed similarly to the type-II case, applying the prox-lemma \eqref{eq:proxlemma}, but bound the second moment of $G_t$ differently:
\begin{equation}
\label{eq:t1-g2}
\EE{\norm{G_t}_*^2 | \cS_t} \le 2\EE{\norm{G_t - \EE{G_t | \cS_t}}_*^2} + 2 \norm{\EE{G_t|\cS_t}-\nabla f_t(X_t)}_*^2 
\le 2(C_1^2 \delta^{2p}+ C_2 \delta^{-q}),
\end{equation}
Combining  this with \eqref{eq:proxlemma}, \eqref{eq:div-telescope}, and \eqref{eq:t1-grad} yields
\begin{align*}
\EE{\sum_{t=1}^n f_t(Y_t)} - \sum_{t=1}^n f_t(x) 
& \le \frac{D}{\eta_{n-1}}+   \frac{C_1^2 \delta^{2p}+ C_2 \delta^{-q}}{\alpha} \sum_{t=1}^n \eta_t + 2 n R C_1 \delta^p +  \frac{n L \delta^2}{2} ~.
\end{align*}
Now, the main terms in the above inequality are identical to those of \eqref{eq:t2-full} except that instead of $C_1$ we have $RC_1$ here. Thus, optimizing the parameters of the algorithm for this case, \eqref{eq:t2-full} holds for non-strongly convex loss functions with $\hC_1=R C_1 \indic{p \le 2} + (L/4) \indic{p \ge 2}$. Similarly, \eqref{eq:t2-sc} holds with the latter choice of $\hC_1$ for $\mu$-strongly convex loss functions and type-I oracles.




