%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
%In the proof, we use \cref{lem:ub} to deal with noisy and biased gradient, which is essentially Theorem~C.4 of \cite{MahdaviPhd:2014}, and also identical to Theorem~6.3 of \cite{Bu:Convex14}, who cites \citet{Dekel:minibatch12} as the source. The lemma and its proof can be found below.

In this section we prove \cref{thm:ub}. First we derive the bounds for the optimization settings and then for the regret. 

\subsection{Stochastic optimization}

The proof for the stochastic optimization scenario is based on \cref{lem:ub} stated below.
This is essentially Theorem~C.4 of \citet{MahdaviPhd:2014}, and also identical to Theorem~6.3 of \citet{Bu:Convex14}, who cites \citet{Dekel:minibatch12} as the source. For completeness, the proof of the lemma is given in \cref{sec:lemub-proof}.
The lemma is somewhat more general than what we need (we will only need it for the case when $\beta_t=0$); 
the general form is presented because its proof is not significantly different than the simpler form and it may find
other applications in the future.
\begin{lemma}
\label{lem:ub}
Let $({\cS}_t)_{t}$ be a filtration such that $X_t$ is ${{\cS}}_t$-measurable.
Let $\overline G_t = \EE{G_t|{{\cS}}_t}$
and assume that the nonnegative real-valued deterministic sequence $(\beta_t)_{1\le t\le n}$ is such that
$\norm{\overline G_t - \nabla {f}(X_t)}_* \le \beta_t$ holds almost surely.
Further, assume that $\mathcal{R}$ is $\alpha$-strongly convex with respect to $\norm{\cdot}$, $D=\sup_{x,y\in \cK} D_{\mathcal{R}}(x,y) < \infty$,  and let $\eta_t = \frac{\alpha}{a_t+L}$ for some increasing
sequence $(a_t)_{t=1}^{n-1}$ of numbers. Then, the cumulative loss of \cref{alg} for a fixed convex and $L$-smooth  function $f$ can be bounded as
\begin{align*}
\EE{ \sum_{t=1}^n {f}(X_t) - {f}(x) }
\le 	 \EE{{f}(X_1)-{f}(x)}+
  \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t
 +\frac{D(a_{n-1}+L)}{\alpha} +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,,
\end{align*}
where $\sigma_t^2 = \EE{ \norm{G_t-\overline G_t}_*^2}$ is the ``variance'' of $G_t$.

If ${{f}}$ is also $\mu$-strongly convex with respect to $\mathcal{R}$ with $\mu > 2L/\alpha$, then letting $\eta_t = \dfrac{2}{\mu t}$ and $a_t = \alpha \mu t/2-L > 0$, the cumulative loss of  \cref{alg} can be bounded as
\begin{align*}
 \EE{ \sum_{t=1}^n {f}(X_t) - {f}(x) }
\le 	 \EE{{f}(X_1)-{f}(x)}+
 \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t
 +\sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,.
\end{align*}
\end{lemma}

Now we can easily prove the theorem.
%By the discussion in \cref{sec:problem}, we have $\Delta^*_{\F,n}(c_1,c_2) \le R^*_{\F,n}(c_1,c_2)/n$ by Jensen's inequality, so it is enough to bound the regret for the online case.
From the bias condition in \cref{def:oracle2} and using that the oracle is memoryless and uniform, we get
\begin{align*}
 \frac{1}{n} \EE{ \sum_{t=1}^n f(X_t) - \inf_{x \in \cK}\sum_{t=1}^n f(x) }
 \le \frac{1}{n}\EE{ \sum_{t=1}^n \tilde{f}(X_t) - \inf_{x \in \cK}\sum_{t=1}^n \tilde{f}(x) } +2C_1 \delta^p
 \,.
\end{align*}

Given $\overline{G}_t=\EE{G_t} = \nabla \tilde{f}(X_t)$, where $\tilde{f}\in \cF_{L,0}$ is convex and smooth,
the result immediately follows by applying \cref{lem:ub} to $\tilde{f}$. Choosing $$\eta_t = \alpha/(a_t+L)$$ as in the lemma with 
$a_t=a t^r$ for some $0<r<1$, and
substituting
 $\beta_t = 0$ (since we have a type-II oracle), $\sigma^2_t = C_2 \delta^{-q}$, respectively, we obtain
 \begin{align}
\MoveEqLeft
\frac{1}{n} \EE{ \sum_{t=1}^n f( X_t) - \inf_{x \in \cK} \sum_{t=1}^n f(x)} \nonumber \\
&\le \frac{1}{n}\left(\EE{f(X_1)-\inf_{x \in \cK}f(x)}+\frac{DL}{\alpha}  \right) %\nonumber
+\frac{Da}{\alpha} n^{r-1}+\dfrac{C_2 \delta^{-q}}{2a(1-r)}n^{-r}+ \left(2+\dfrac{2}{n}\right)C_1\delta^p \,.
\label{eq:ubToBeOpt}
 \end{align}
 Choosing $r = \frac{p+q}{2p+q}$,  
 $\delta = \left( \tfrac{C_2}{4aC_1}\tfrac{2p+q}{p}\tfrac{n}{n+1}\right)^{\frac{1}{p+q}}n^{-\frac{1}{2p+q}}$  and
 $a^{2p+q} =2^{q-p}\left( 2+\tfrac{q}{p} \right)^p\left(1+ \tfrac{1}{n} \right)^q \left( \tfrac{\alpha}{D} \right)^{p+q}C_1^q C_2^p $,
the last $3$ terms in \eqref{eq:ubToBeOpt} are optimized to
 \[
 K_1 C_1^{q/(2p+q)} C_2^{p/(2p+q)} n ^{-p/(2p+q)} \,,
 \]
 where
% $K_1 = 2^{\dfrac{3q}{2(p+q)}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{\dfrac{p+q^2+2pq}{(p+q)(2p+q)}}+2^{\dfrac{3q}{2(2p+q)}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{2-\dfrac{1}{2p+q}}$.
 $(K_1/3)^{2p+q} \le 2^{2q-p}\left(2+q/p \right)^p \left(D/\alpha\right)^{p}$. This implies \eqref{eq:MDbound1}.
% \begin{align*}
 
When $\tilde{f} \in \cF_{L,\mu, \cR}$ is $L$-smooth and $\mu$-strongly convex, for
$\eta_t = 2/(\mu t)$ and 
%\[
%\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n} \,,
%\]
$
\delta^{p+q} =  \tfrac{C_2\left( \log n+1+\tfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)} \,,
$
we similarly obtain 
 \begin{align*}
 \MoveEqLeft
\frac{1}{n} \EE{ \sum_{t=1}^n f( X_t) - \inf_{x \in \cK} \sum_{t=1}^n f(x)} -\frac{1}{n}\EE{f(X_1)-\inf_{x \in \cK}f(x)}\\
&\le (2+\dfrac{2}{n})C_1\delta^p+\dfrac{C_2 \delta^{-q}}{\alpha \mu n} \sum_{t=1}^{n-1}\dfrac{1}{t-\dfrac{2L}{\alpha \mu}}\\
&\le (2+\dfrac{2}{n})C_1\delta^p+\dfrac{C_2 }{\alpha \mu}\delta^{-q} \dfrac{\log n+1+\alpha \mu/(\alpha \mu-2L)}{n}\\
&\le K_2C_1^{\frac{q}{p+q}}C_2^{\frac{p}{p+q}} \left( \frac{\log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}}{n} \right)^{\frac{p}{p+q}}\,.
 \end{align*}
The last step optimizes the bound via the choice of $\delta$, and
%$\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n}$,
%$\delta^{p+q} =  \dfrac{C_2\left( \log n+1+\dfrac{\alpha \mu}{\alpha \mu -2L}\right)}{2\alpha \mu C_1 (n+1)}$,
%where
%$K_2^{p+q}=\sqrt{2}^{2p+3q}D^{q/2}\alpha^{-p-q/2}\mu^{-p}$.
$K_2^{p+q}=2^{q}\alpha^{-p}\mu^{-p}$.



\subsection{Online optimization}
The proof in this section follows closely the derivation of \citet{saha2011improved}.
Assume that the gradients of all $\tf_t \in \cF$ are bounded by $M$, that is $\norm{\nabla \tf_t(x)}_* \le M$ for all $x \in \cK$ (here, the subindex of $\tf_t$ signifies that the oracle is non-uniform).

Let $\cS_t$ denote the $\sigma$-algebra of all random events up until and including the selection of $X_t$. Since the oracle is unbiased, that is,  $\EE{Y_t|\cS_t}=X_t$, we have
\begin{equation}
\EE{\tf_t(Y_t)-\tf_t(X_t)|\cS_t} \le \EE{\ip{\nabla \tf_t(X_t),Y_t-X_t}+\tfrac{L}{2}\|X_t-Y_t\|^2|\cS_t} \le L\delta^2/2~.
\label{eq:YX}
\end{equation}
Instead of Lemma~\ref{lem:mdlinregret} used in the optimization proof, we will employ the prox-lemma \citep[see, e.g.,][]{Beck2003mirror, NeJuLaSh09}:
\begin{equation}
\label{eq:proxlemma}
\ip{G_t,X_t-x} \le \frac{1}{\eta_t}(D_\cR(x,X_t)-D_\cR(x,X_{t+1})) + \eta_t \frac{\|G_t\|_*^2}{2\alpha}~.
\end{equation}
Using the linearization $\tf(X_t) - \tf(x) \le \ip{\nabla \tf_t(X_t),X_t-x} = \EE{\ip{ G_t,X_t-x}| \cS_t}$, summing up the bounds for all $t$, and applying \eqref{eq:div-telescope}, we get the standard mirror descent bound for any $x \in \cK$
\begin{align}
\EE{\sum_{t=1}^n \tf_t(X_t)- \sum_{t=1}^n \tf_t(x)} 
& \le \EE{\sum_{t=1}^n \ip{G_t,X_t-x}} 
\le \frac{D}{\eta_{n-1}} + \sum_{t=1}^n \eta_t \frac{\EE{\|G_t\|_*^2}}{2\alpha} 
\label{eq:md-bound}
\end{align}
Combining the latter with the bound
\[
\EE{\norm{G_t}_*^2 | \cS_t} \le 2\EE{\norm{G_t - \nabla \tf_t(X_t)}_*^2 + \norm{\nabla \tf_t(X_t)}_*^2 \Big| \cS_t}
\le 2(M^2 + C_2 \delta^{-q}),
\]
the bias condition of the oracle, and \eqref{eq:YX}, we obtain
\begin{align*}
\EE{\sum_{t=1}^n f_t(Y_t)} - \inf_{x \in \cK} \sum_{t=1}^n f_t(x)
& \le \EE{\sum_{t=1}^n \tf_t(Y_t) }- \inf_{x \in \cK} \sum_{t=1}^n \tf_t(x) + 2 n C_1 \delta^{p} \\
& \le \EE{\sum_{t=1}^n \tf_t(X_t)} - \inf_{x \in \cK} \sum_{t=1}^n \tf_t(x) + 2 n C_1 \delta^{p} + \frac{n L \delta^2}{2} \\
& \le \frac{D}{\eta_{n-1}}  + \sum_{t=1}^n \eta_t \frac{M^2 + C_2 \delta^{-q}}{\alpha} +  2 n C_1 \delta^{p} + \frac{n L \delta^2}{2}~.
\end{align*}
Setting the parameters 
$\delta=(\frac{q}{2p})^{\frac{2}{2p+q}} (\frac{C_2 D}{\alpha B^2})^{\frac{1}{2p+q}} n^{-\frac{1}{2p+q}}$, 
where $B=C_1$ if $p<2$ and $B=C_1+L/4$ if $p=2$, and 
$\eta_t=D^{\frac{p+q}{2p+q}} (\frac{q}{2p})^{\frac{q}{2p+q}} (\frac{C_2}{\alpha})^{-\frac{p}{2p+q}} B^{-\frac{q}{2p+q}} n^{-\frac{p+q}{2p+q}}$ gives the desired bound for $\F_{L,0}$ with bounded gradients. When $p>2$, $p$ should be replaced with $2$ in the bound and $B=L/4$ (in this case the $\delta^2$ term dominates the one with $\delta^p$).
%For $q=0$, $C_2$ has to be replaced with $C_2+M^2$.


When the set of functions is also strongly convex, instead of the linearization we use strong convexity to get
\[
\tf(X_t) - \tf(x) \le \ip{\nabla \tf_t,X_t-x} - \frac{\mu}{2} D_\cR(x,X_t) = \EE{\ip{ G_t,X_t-x}| \cS_t} - \frac{\mu}{2} D_\cR(x,X_t) ~.
\]
Combining this with \eqref{eq:proxlemma} gives the well-known variant of \eqref{eq:md-bound} for strongly convex loss functions \citep{BaHaRa07} for the choice $\eta_t=2/(t\mu)$:
\begin{align*}
\EE{\sum_{t=1}^n \tf_t(X_t)- \sum_{t=1}^n \tf_t(x)} 
& \le \sum_{t=1}^n  \frac{\EE{\|G_t\|_*^2}}{ t \alpha \mu} \le \frac{\max_t \EE{\|G_t\|_*^2}}{\alpha \mu} (1+\log n).
\end{align*}
Similarly to the non-strongly convex case, this implies
\begin{align}
\EE{\sum_{t=1}^n f_t(Y_t)} - \inf_{x \in \cK} \sum_{t=1}^n f_t(x) \le 
\frac{M^2 + C_2 \delta^{-q}}{2 \alpha \mu} (1+\log n)+  2 n C_1 \delta^{p} + \frac{n L \delta^2}{2}~.
\end{align}
Setting $\delta=(\frac{C_2 q (1+\log n)}{4 \alpha \mu C_1 p n})^{\frac{1}{p+q}}$, we obtain the desired bound for $p<2$. For $p \ge 2$, $C_1$ should be replaced with $C_1+L/4$, while for $p>2$,  $C_1$ has to be replaced with $L/4$ and $p$ with $2$ (in this case the $\delta^2$ term dominates the one with $\delta^p$).



