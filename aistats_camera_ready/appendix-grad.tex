%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
\subsubsection*{Proof of Proposition \ref{prop:grad-onepoint}}

\if0
\begin{proposition}
\label{prop:grad-1spsa}
Given any $f$ that is three times continuously differentiable with bounded third derivative.
For any $x \in \cK$, and $\delta >0$, let oracle $\gamma$ return
\begin{align}
% Y = x+\delta U \,, \quad
G =  V \left(\dfrac{f(x+\delta U) + \xi}{\delta}\right),
 \label{eq:onesp}
\end{align}
where $V, U$ are random variables that satisfy $\E[V U\tr] = I_d$, $U_i, i=1,\ldots,d$ are i.i.d., $\E[ V U^2] = 0$, $\E[V_i]=0$, $|U_i|$ and $\E|V_i|$ have finite upper bounds.
Then, we have that $\gamma$ is a type-I oracle with $c_1(\delta) = C_1 \delta^2$ and $c_2(\delta) = C_2/\delta^2$.
\end{proposition}
\fi
\begin{proof}\ \\
% \todoc[inline]{This proof must be updated to follow the proof of Prop 1. Don't write $\le O(\cdot)$,for example.}
\textbf{Case 1 ($f \in \C^3$): }\ \\
We use the proof technique of \cite{spall1997one}.
We start by bounding the bias.
Since by assumption $\EE{ \xi|V}=0$, we have
\begin{align*}
\E\left[  V\left(\dfrac{\xi}{\delta}\right) \right]= 0\,,
\end{align*}
implying that
\begin{align*}
\E[G] =  \E\left[ V \left(\dfrac{f(x+\delta U) }{\delta}\right)\right] \,.
\end{align*}
By Taylor's theorem, we obtain, a.s.,
\begin{align*}
f(x + \delta U) =
 f(x)
 +\delta\,  U\tr\,\nabla f(x)
  + \frac{\delta^2}{2}\, U\tr \nabla^2 f(x) U
  +  \frac{\delta^3}{2} \, R^{+}(x,\delta,U) \,(U, U, U),
\end{align*}
where
\begin{align}
 R^{+}(x,\delta,U)= \int_0^1  \nabla^3 f(  x + s \, \delta U ) (1-s)^2 ds. \label{eq:taylor-r}
\end{align}
In the above, $\nabla^3 f(\cdot)$ is considered as a rank-3 tensor.
Letting $B_3 = \sup_{x\in D} \norm{ \nabla^3 f(x) }$,%
\footnote{Here, $\norm{\cdot}$ is the implied norm: For a rank-3 tensor $T$, $\norm{T} = \sup_{x,y,z\ne 0}
\frac{|T (x,y,z)|}{\norm{x}\norm{y}\norm{z}}$.
}
we have $\norm{ R^{+}(x,\delta,U)} \le B_3/3$ a.s.
Now,
\begin{align*}
\EE{V\, \dfrac{f(x+\delta U)}{\delta}}
&= \EE{V \frac{f(x)}{\delta}} +  \EE{VU^{\tr}
\, \nabla f(x)}  + \EE{\frac{\delta}{2}\, V U\tr \nabla^2 f(x) U} \\
&\qquad+   \EE{\frac{\delta^2}{2}  V \,R^{+}(x,\delta,U)(U \otimes U \otimes U)}
\\
&= \, \nabla f(x)  + \EE{\frac{\delta^2}{2}  V \,R^{+}(x,\delta,U)(U \otimes U \otimes U)}\,.
\end{align*}
The final equality above follows from the facts that $\EE{V} = 0$, $\EE{V U\tr} = I$ and for any $i,j=1,\ldots,d$, $E[V_i U_j^2] = 0$ since $V$ is a deterministic odd function of $U$, with $U$ having a symmetric distribution.
Using the fact that $|R^{+}(x,\delta,U) (U \otimes U \otimes U)| \le
\norm{R^{+}(x,\delta,U)} \norm{U}^3$,
we obtain
\begin{align*}
\norm{ \EE{ G } - \nabla f(x) }_*
\le C_1\,\, \delta^2 \,,
\end{align*}
where $C_1 = \frac{B_3 \EE{ \norm{V}_* \norm{U}^3 }}{6}$.

Let us now bound the variance of $G$:
Using the identity $\E\left\|X -  E[X]\right\|^2 \le 4 \E \left\|X\right\|^2$, which holds for any random variable $X$,%
\footnote{When $\norm{\cdot}$ is defined from an inner product,
$\E\left\|X -  E[X]\right\|^2 = \EE{\norm{X}^2} - \norm{\EE{X}}^2 \le \EE{\norm{X}^2}$ also holds, shaving off a factor of four from the inequality below.}
we bound $\E\left\| G - \E G\right\|_*^2$ as follows:
\begin{align}
\E\left\| G - \E G\right\|_*^2
 &\le 4 \E \left\|G\right\|_*^2 \nonumber \\
& =  4\E\left( \left\| V \right\|_*^2 \left(\left(\dfrac{\xi}{\delta}\right)^2  + 2 \left(\dfrac{\xi}{\delta}\right) \left(\dfrac{f(x+\delta U)}{\delta}\right)
+ \left( \dfrac{f(x+\delta U) }{\delta} \right)^2 \right)\right) \nonumber \\
&=  4\E\left( \left\| V \right\|_*^2 \left(\dfrac{\xi}{\delta}\right)^2\right)
+ 4 \E \left(\left\| V \right\|_*^2 \right)\left( \dfrac{f(x+\delta U) }{\delta} \right)^2  \label{eq:h31} \\
& \le  \frac{C_2}{\delta^2}\,, \nonumber \label{eq:h41}
\end{align}
where $C_2 = 4 \EE{\norm{V}_*^2}\left( \sigma_\xi^2+B_0^2\right)$, where
$\sigma_\xi^2 = \essup \EE{\xi^2|V}$ and $B_0 = \sup_{x\in \D} f(x)$.
The equality in \eqref{eq:h31} follows from $\EE{ \xi \,|\, V } = 0$.

Therefore, for $f \in \C^3$, $\gamma$ defined by \eqref{eq:one-point} is a $(C_1\delta^2, C_2/\delta^2)$ type-I oracle.

\paragraph{Case 2 ($f$ is convex and $L$-smooth):}\ \\
Since $f$ is convex and $L$-smooth, for any $0<\delta <1$,
\begin{align*}
 0 \le \frac{f(x + \delta u)-f(x)}{\delta}-\<\nabla f(x), u\> \le&   \frac{L \delta \norm{ u}^2}{2}.
\end{align*}
Denote
$\phi(x,\delta,u):=\frac{f(x + \delta u)-f(x)}{\delta}-\<\nabla f(x), u\>$, we have
$\left|\phi(x,\delta,u) \right| \le  \dfrac{L\delta}{2} \norm{u}^2$.
Then,  given $\EE{VU^\top}=I$, $\EE{V}=0$, we obtain
\begin{align}
\norm{ \EE{ G } - \nabla f(x) }_*
&= \norm{ \EE{\frac{f(x + \delta U)}{\delta}V}-\EE{VU^\top \nabla f(x)}  }_*\nonumber\\
&=\norm{ \EE{V\left( \frac{f(x + \delta U)}{\delta}- -U^\top \nabla f(x)\right)}  }_*\nonumber\\
&= \norm{ \EE{V\left( \phi(x,\delta,U)+\dfrac{f(x)}{\delta} \right)}  }_*\nonumber\\
&=\norm{ \EE{V\phi(x,\delta,U)} }_*\nonumber\\
&\le C_1 \,\, \delta\,, \label{eq:c1onepoint}
\end{align}
  where $C_1 = \dfrac{L}{2}\EE{\norm{V}_*\norm{U}^2}$.
The claim regarding the variance of $G$ follows in a similar manner as in Case 1, i.e., $f \in \C^3$.

Therefore, for $f$ convex and $L$-smooth, $\gamma$ defined by \eqref{eq:one-point} is a $(C_1\delta, C_2/\delta^2)$ type-I oracle, where $C_1$ is given by \eqref{eq:c1onepoint} and $C_2$ as defined in Case 1.
\end{proof}
% By Taylor's series expansions, we obtain, a.s.,
% \begin{align*}
% f(x + \delta U) = f(x) \pm \delta U\tr \nabla f(x) + \frac{\delta^2}{2} U\tr \nabla^2 f(x) U +  \frac{\delta^3}{6} \nabla^3 f(\tilde x^{+})(U \otimes U \otimes U),
% \end{align*}
% where $\otimes$ and $\tilde x^+$ are as in the proof of Proposition \ref{prop:grad-spsa}.
% Using suitable Taylor's series expansions, we have %the following for any $i=1,\ldots,d$:
% \begin{align}
% &\E\left[V\left(\dfrac{f(x+\delta U)}{\delta}\right) \right] \nonumber\\
% = & \E\left[V \dfrac{f(x)}{\delta} \right] + \E\left[V U\tr \left.\nabla f(x)\right| \F_n\right]  +   \E\left[\frac{\delta}{2}  \nabla^2 f(\tilde  x^+)(V \otimes U \otimes U)\right] + \O\left( \delta^2\right). \label{eq:l1}\\
% \le & \nabla f(x) + \O\left( \delta^2\right).\label{eq:l2}
% \end{align}
% The last inequality follows from the facts that $E[V U\tr] = I$, and for any $i=1,\ldots,d$, $E[V_i U_j^2] = 0$, $|U_i|$ and $\E|V_i|$ have finite upper bounds.
% \paragraph{Case 2: $f$ is convex with a $L$-Lipschitz gradient:}\ \\
% Using the tangent bound for convex functions and $L$-Lipschitz property of the gradient of $f$, we obtain the following for any $\delta>0$:
% \begin{align*}
% \frac{f(x)}{\delta} + \frac{\<\nabla f(x), \delta u\>}{\delta} \le \frac{f(x + \delta u)}{\delta} \le& \frac{f(x) + \<\nabla f(x), \delta u\> + (L / 2) \norm{\delta u}^2}{\delta}.
% \end{align*}
% Letting
% $\phi(x,\delta,u):=\frac1{\delta}\left(\frac{f(x + \delta u)}{\delta} - \<\nabla f(x),  u\>\right)$, we have
% \begin{align*}
% \left|\phi(x,\delta,u) \right| \le&  \frac{B_4}{\delta^2} + \dfrac{L}{2} \norm{u}^2\,,
% % \frac{f(x - \delta u) -  f(x)}{\delta} \le& -\frac{\<\nabla f(x), \delta u\> + (L / 2) \norm{\delta u}^2}{\delta}
% \end{align*}
% where $B_4 = \sup_{x\in\D} \norm{f(x)}$.
% From the foregoing,
% \begin{align*}
%  \norm{\E[G] - \nabla f(x)}
%  = \scnorm{\E\left[V\,  \left(\frac{f(x+\delta U)  -f(x-\delta U)}{2\delta}\right)-V U^\top\nabla f(x) \right]}
%  \le \delta \norm{\E[ V \phi(x,\delta, U)]}
%  \le \frac{\delta L}{2} \E[ \dnorm{V} \norm{U}^2],
% \end{align*}
% and the claim for the bias follows by setting $c_1(\delta)= \frac{\delta L}{2} \E[ \dnorm{V} \norm{U}^2]$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Proof of Proposition \ref{prop:flaxman}}
Before the proof, we introduce a fundamental theorem of vector calculus, which is commonly known as the Gauss-Ostrogradsky theorem or the divergence theorem . A special case of the theorem for real-valued functions in $\R^n$ can be stated as follows.
\begin{lemma}
\label{lem:gradientCalculus}
Suppose $W \subset \R^n$ is an open set with the boundary $\partial W$. At each point of $\partial W$ there is a normal vector $n_W$ such that $n_W$  (i) has unit norm, (ii) is orthogonal to $\partial W$, (iii) points outward from $W$. Suppose $f: \R^n\to \R$ is a function of class $C^1$ defined at least on the closure of $W$, then we have
\begin{align*}
\int_{ W} \nabla f\,d W = \int_{\partial W} f n_W \,d \partial W \,.
\end{align*}
\end{lemma}

\begin{proof}
Given that $\EE{\norm{V}_*^2}$ and $\EE{\xi^2}$ are bounded, the variance of $G$ remains the same as stated in \cref{prop:grad-onepoint}.

As to the bias, let $\tilde{f}$ be a smoothed version of $f$, i.e., $\forall x \in \cK$,
\begin{align*}
\tilde{f}(x) &= \EE{f(x+\delta V)} =\int_{v \in W} f(x+\delta v)\dfrac{\,d w}{\lvert W\rvert}\,,
\end{align*}
where the expectation is w.r.t. $V$, which is a random variable uniformly chosen from $W$. The second equality interprets the expectation as integral.
Now we want to prove that for any given $x\in \cK$, $G$ is an unbiased gradient estimate of $\tilde{f}$ at $x$.
Since $U$ is uniformly distributed over $\partial W$,  the expectation of $G$ can be written as
\begin{align*}
\EE{G} =\dfrac{\lvert \partial W\rvert}{\lvert W \rvert} \int_{\partial W} \dfrac{1}{\delta} f(x+\delta U)n_W(U)\dfrac{\,d U}{\lvert \partial W\rvert}
=  \int_W \nabla f(x+\delta U)\dfrac{\,d U}{\lvert W\rvert}\,,
\end{align*}
where the second equality follows from \cref{lem:gradientCalculus}, by replacing the gradient of $\hat{f}(u) =\dfrac{1}{\delta} f(x+\delta u) $ with $\nabla f(x+\delta u)$.
Then, the order of the gradient and the integral can be exchanged, because $\int_W f(x+\delta U)\,d U$ exists. Consequently, we obtain $\EE{G} = \nabla \tilde{f}(x)$.

Moreover, $\tilde{f}$ and $f$ are actually close. In particular, for any $x \in \cK$,
\begin{align}
\label{eq:f2tildef}
\tilde{f}(x)-f(x) =\int_{ W} f(x+\delta w)-f(x)\dfrac{\,d w}{\lvert W\rvert} \,.
\end{align}

When $f$ is $L_0$-Lipschitz, $\vert f(x+\delta w)-f(x)\vert \le L_0\delta \norm{w}$, which combined with \eqref{eq:f2tildef} gives that $\gamma$ is a type-II oracle with $c_1(\delta) = C_1 \delta$, where $C_1 = L_0 \sup_{w\in W}\norm{w}$.

When $f$ is convex and $L$-smooth,  $0\le f(x+\delta w)-f(x) - \ip{\nabla f(x), \delta w}\le\dfrac{L}{2}\delta^2 \norm{w}^2$. Given that $W$ is symmetric, $\int_{ W} \ip{\nabla f(x), \delta w} \,d w=0$. Hence, one can easily get that $\gamma$ is a type-II oracle with $c_1(\delta)=C'_1 \delta^2$, where $C'_1 = \dfrac{L}{2 |W|}\int_{ W}\norm{w}^2\,d w$.

Finally, if $f$ is $L$-smooth,
\begin{align*}
\norm{\nabla \tilde{f}(x)- \nabla f(x)}_*
&\le \int_W \norm{\nabla f(x+\delta w) - \nabla f(x)}_* \frac{dw}{|W|}
\le L\delta^2\int_W \norm{w}^2 \frac{dw}{|W|} =2 C'_1 \delta^2
\end{align*}
with the same value of $C'_1$ as before. So $\gamma$ is also a  type-I oracle with $c_1(\delta)=2C'_1 \delta^2$.
\end{proof}
%\begin{remark}
%Note that $f$ do not need to be differentiable here.
%$U$ and $V$ can be selected under other distributions, like Gaussian.
%For more examples about smoothing technique, see \cite{flaxman2005online}, \cite{nesterov2004introductory}, \cite{PoTsy90}, \cite{HaLe14:SOC}.
%\end{remark}


%
% \paragraph{One-point SPSA:}
%
%  Since $f$ is $3$-times continuously differentiable, using Taylor's expansion, we get for the the $i^{th}$ component of $G$,
% \begin{align*}
% &\EE{G_{\cdot i}}\\
% =&\EE{\dfrac{1}{\delta \Delta_{\cdot i}} \left( f(x+\delta \Delta)+\epsilon \right) }\\
% =& \EE{\dfrac{1}{\delta \Delta_{\cdot i}} \left( f(x)+\delta f'(x)^\top \Delta+\dfrac{1}{2}\delta^2 \Delta^\top f''(x)\Delta \right) } \numberthis \label{eq:spsaTaylorExp} \\
% &+\EE{\dfrac{1}{\delta \Delta_{\cdot i}} \left(O(\delta^3 \Delta\otimes\Delta\otimes\Delta) +\epsilon \right) }\\
% =& [f'(x)]_i +O(\delta^2) \,,
% \end{align*}
% where $[f'(x)]_i$ denotes the $i^{th}$ component of $f'(x)$. The last equality comes from the properties of symmetry, and bounded moment for $\Delta$.
% Hence, $G$ is a estimate of $f'(x)$ with bias $O(\delta^2)$.
%
% \paragraph{Two-point SPSA:}
%
% Under this situation, using Taylor expansion again, the $f(x)$ and $f''(x)$ terms in \eqref{eq:spsaTaylorExp} can be canceled and one can conclude that $G$ is only an order $O(\delta^2)$ term away from $f'(x)$. Note that the second-order term in one-point SPSA is zero-mean, while in the two-point SPSA it is zero.
% As a result, we only need $\Delta_{\cdot i}$ to be zero-mean instead of symmetry.

\subsubsection*{Proof of Proposition \ref{prop:grad-spsa}}
\begin{proof}\ \\
\textbf{Case 1 ($f \in \C^3$):}\ \\
We use the proof technique of \cite{spall1992multivariate}
  (in particular, Lemma 1 there).
We start by bounding the bias.
Since by assumption $\EE{ \xi^+-\xi^-|V}=0$, we have
\begin{align*}
\E\left[  V\left(\dfrac{\xi_n^+ - \xi_n^-}{2\delta}\right) \right]= 0\,,
\end{align*}
implying that
\begin{align*}
\E[G] =  \E\left[V\,  \dfrac{f(X^+)  -f(X^-)}{2\delta} \right]\,.
\end{align*}

By Taylor's theorem, using that $f\in C^3$, we obtain, a.s.,
% \todoc{I am using the integral form, because otherwise you would need to argue that the point picked on the line segment between $x$ and $x\pm \delta U$ is measurable.
% For the notation etc see \url{http://www.gold-saucer.org/math/taylor/taylor.pdf}}
\begin{align*}
f(x \pm \delta U) =
 f(x)
 \pm\delta\,  U\tr\,\nabla f(x)
  + \frac{\delta^2}{2}\, U\tr \nabla^2 f(x) U
  \pm  \frac{\delta^3}{2} \, R^{\pm}(x,\delta,U) \,(U, U, U),
\end{align*}
where, as in the proof of Proposition \ref{prop:grad-onepoint}, $R^{\pm}(x,\delta,U)$ is defined as follows:
\begin{align}
 R^{\pm}(x,\delta,U)= \int_0^1  \nabla^3 f(  x \pm s \, \delta U ) (1-s)^2 ds. \label{eq:taylor-r}
\end{align}
%In the above, $\nabla^3 f(\cdot)$ is considered as a rank-3 tensor.
Letting $B_3 = \sup_{x\in D} \norm{ \nabla^3 f(x) }$,%
%\footnote{Here, $\norm{\cdot}$ is the implied norm: For a rank-3 tensor $T$, $\norm{T} = \sup_{x,y,z\ne 0}
%\frac{|T (x,y,z)|}{\norm{x}\norm{y}\norm{z}}$.
%}
we have $\norm{ R^{\pm}(x,\delta,U)} \le B_3/3$ a.s.
Now,
\begin{align}
\begin{split}
\MoveEqLeft       V\, \dfrac{f(X^+)-f(X^-)}{2\delta}
  = V\, \dfrac{f(x+\delta U) - f(x-\delta U)}{2\delta} \\
&= VU^{\tr}
\, \nabla f(x)   +   \frac{\delta^2}{4}  V \,(R^{+}(x,\delta,U)+R^{-}(x,\delta,U))(U \otimes U \otimes U)\,.
\end{split}
\label{eq:l1}
\end{align}
and therefore,
by taking expectations of both sides,
using $\EE{V U\tr} = I$ and then $|R^{\pm}(x,\delta,U) (U \otimes U \otimes U)| \le
\norm{R^{\pm}(x,\delta,U)} \norm{U}^3$,
we get that
\begin{align*}
\norm{ \EE{ G } - \nabla f(x) }_*
\le C_1\,\, \delta^2 \,,
\end{align*}
where $C_1 = \frac{B_3 \EE{ \norm{V}_* \norm{U}^3 }}{6}$.

Using arguments similar to that in the proof of Proposition \ref{prop:grad-onepoint}, the variance of $G$ is bounded as follows:
\begin{align}
\MoveEqLeft \E\left\| G - \E G\right\|_*^2
 \le 4 \E \left\|G\right\|_*^2 \nonumber \\
& =  4\E\left( \left\| V \right\|_*^2 \left(\left(\dfrac{\xi^+ - \xi^-}{2\delta}\right)^2  + 2 \left(\dfrac{\xi^+ - \xi^-}{2\delta}\right) \left(\dfrac{f(X^+) - f(X^-)}{2\delta}\right)
+ \left( \dfrac{f(X^+) - f(X^-)}{2\delta} \right)^2 \right)\right) \nonumber \\
&=  4\E\left( \left\| V \right\|_*^2 \left(\dfrac{\xi^+ - \xi^-}{2\delta}\right)^2\right)
+ 4 \E \left(\left\| V \right\|_*^2 \right)\left( \dfrac{f(X^+) - f(X^-)}{2\delta} \right)^2  \label{eq:h3} \\
& \le  \frac{C_2}{\delta^2}\,, \nonumber \label{eq:h4}
\end{align}
where $C_2 = 4 \EE{\norm{V}_*^2}\left( \sigma_\xi^2+\fspan(f)\right)$
and $\fspan(f) = \sup_{x\in \D} f(x) - \inf_{x\in \D} f(x)$.
The equality in \eqref{eq:h3} follows from $\EE{ \xi^+-\xi^- \,|\, U,V } = 0$.

Therefore, for $f \in \C^3$, $\gamma$ defined by \eqref{eq:twosp} is a $(C_1\delta^2, C_2/\delta^2)$ type-I oracle.

\paragraph{Case 2 (Controlled noise and $F$ is convex and $L_{\psi}$-smooth):}\ \\
%\subsubsection*{Proof of Proposition \ref{prop:grad-spsa} for convex and smooth $f$}
The proof follows by parallel arguments to that used in the proof of Lemma 1 in \cite{duchi2015optimal} and we give it here for the sake of completeness.

For any convex function $f$ with an $L$-Lipschitz gradient, for any $\delta>0$ it holds that
\begin{align*}
\frac{\<\nabla f(x), \delta u\>}{2\delta} \le \frac{f(x + \delta u) -  f(x)}{2\delta} \le& \frac{\<\nabla f(x), \delta u\> + (L / 2) \norm{\delta u}^2}{2\delta}.
\end{align*}
Using similar inequalities for $f(x-\delta u)$, we obtain
\begin{align*}
\<\nabla f(x), u\> - \frac{L \delta \norm{ u}^2}{2} \le \frac{f(x + \delta u) -  f(x-\delta u)}{2\delta} \le& \<\nabla f(x), u\> + \frac{L \delta \norm{ u}^2}{2}.
\end{align*}
Letting
$\phi(x,\delta,u):=\frac1{\delta}\left(\frac{f(x + \delta u) -  f(x-\delta u)}{2\delta} - \<\nabla f(x),  u\>\right)$, we get
\begin{align*}
\left|\phi(x,\delta,u) \right| \le&  \dfrac{L}{2} \norm{u}^2\,.
% \frac{f(x - \delta u) -  f(x)}{\delta} \le& -\frac{\<\nabla f(x), \delta u\> + (L / 2) \norm{\delta u}^2}{\delta}
\end{align*}

Using $\EE{V U^\top}=I$, we obtain
\begin{align*}
\E\left[V\,  \left(\frac{f(x+\delta U)  -f(x-\delta U)}{2\delta}\right)\right]=&
\E\left[ V U^\top\nabla f(x) +  \delta\phi(x,\delta,U) V \right]\\
= &  \nabla f(x) + \delta \widehat\phi(x,\delta),
\end{align*}
where $\widehat\phi(x,\delta)$ satisfies $\scnorm{\widehat\phi(x,\delta)}_*  \le \, \dfrac{L}{2}\E[ \dnorm{V} \norm{U}^2]$.

Applying the above expression to $F(\cdot, \Psi)$ and recalling that $G=V\,  \left(\tfrac{F(X^+,\psi)  -F(X^-,\psi)}{2\delta}\right)$, we have, for $P$-almost every $\psi$, 
$$\E[G] = \nabla F(x,\psi) + \delta \widehat\phi(x,\delta),$$
where, as before, $\widehat\phi(x,\delta)$ satisfies $\scnorm{\widehat\phi(x,\delta)}_*  \le \, \dfrac{L_{\psi}}{2}\E[ \dnorm{V} \norm{U}^2]$.

Using the fact that $E[\nabla F(x,\Psi)] = \nabla f(x)$, we obtain
\begin{align*}
 \norm{\E[G] - \nabla f(x)}_*
 &= \scnorm{\E\left[V\,  \left(\frac{f(x+\delta U)  -f(x-\delta U)}{2\delta}\right)-V U^\top\nabla f(x) \right]}_*
 \le \, \delta \norm{\E[ V \phi(x,\delta, U)]}_*\\
 &\le \,\frac{\delta \overline{L}_{\Psi}}{2} \E[ \dnorm{V} \norm{U}^2],
\end{align*}
and the claim for the bias follows by setting $C_1= \frac{\overline{L}_{\Psi}}{2} \E[ \dnorm{V} \norm{U}^2]$.

We now bound $\EE{ \norm{G}_*^2}$ as follows:
%In the case of controlled noise, i.e., $\xi^+ = \xi^-$,
\begin{align*}
 \E \norm{G}^2
& = \mathbb{E}\norm{V\left(\delta \phi(x,\delta, U)+ U^\top\nabla f(x) \right)}^2
 \le  \E\left[ \left( \dnorm{ V U \tr \nabla f(x)} + \frac{\delta L}{2} \dnorm{V} \norm{U}^2 \right)^2\right]\\
& \le  2 \E\left[  \dnorm{ V U \tr \nabla f(x)}^2\right]  + \frac{\delta^2 \overline{L}_{\Psi}^2}{2}\E\left[ \dnorm{V}^2 \norm{U}^4 \right],
\end{align*}
and the claim for the variance follows by setting $C_2 =  2 B_1^2  + \frac{ \overline{L}_{\Psi}^2}{2}\E\left[ \dnorm{V}^2 \norm{U}^4 \right]$ with $B_1 = \sup_{x\in \K} \scnorm{\nabla f(x)}_*$.

Therefore, for the case of controlled noise with a convex and $L_{\psi}$-smooth $F$, we have that $\gamma$ defined by \eqref{eq:twosp} is a $(C_1\delta, C_2)$ type-I oracle.  
%For the case of uncontrolled noise, the claim for the variance follows in the same manner as that in the proof of Proposition \ref{prop:grad-spsa} and we obtain a $(C_1\delta, C_2/\delta^2)$ type-I oracle.
\end{proof}


