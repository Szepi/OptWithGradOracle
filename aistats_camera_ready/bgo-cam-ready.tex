\documentclass[twoside]{article}
\usepackage[accepted]{aistats2016}
\input{commands}

% If your paper is accepted, change the options for the package
% aistats2016 as follows:
%
%\usepackage[accepted]{aistats2016}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
\runningauthor{Xiaowei Hu, Prashanth L.A., Andr\'as Gy\"orgy, and Csaba Szepesv\'ari}

\twocolumn[

\aistatstitle{(Bandit) Convex Optimization with Biased Noisy Gradient Oracles}


\aistatsauthor{Xiaowei  Hu$^1$ \And Prashanth L.A.$^2$ \And Andr\'as Gy\"orgy$^3$ \And Csaba Szepesv\'ari$^1$ \vspace{1mm}}

\aistatsaddress{$^1$ Department of Computing Science, University of Alberta \\
$^2$ Institute for Systems Research, University of Maryland \\
$^3$ Department of Electrical and Electronic Engineering, Imperial College London} ]


\begin{abstract}
\vspace{-0.2cm}
We present a novel noisy gradient oracle model for convex optimization. The model allows to explicitly address the bias-variance tradeoff of gradient estimation methods typical in the literature, as well as to prove upper and lower bounds for the minimax error. As such the oracle model allows a clean way of addressing the limits of achievable performance
when biased, noisy gradient estimators with controllable bias-variance tradeoffs are employed.
When considering the model for online/stochastic convex optimization,
one consequence of our results is that the currently used gradient estimates and the proof techniques used to analyze them cannot 
achieve the optimal square-root regret rate for online/stochastic bandit convex optimization.
% when the function cannot be queried outside the domain.
\end{abstract}

\vspace{-0.3cm}

\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Problem Setup}
\label{sec:problem}
\input{problem}

\section{Main Results}
\label{sec:results}
\input{results}

\section{Applications to Stochastic BCO}
\label{sec:sbco}
\input{stochastic_bco}

\section{Applications to Online BCO}
\label{sec:obco}
\input{online_bco}

\section{Related Work}
\label{sec:related}
\input{related}

%\section{PROOFS}
%\label{sec:proofs}
%%\input{ub_proof}
%\input{lb_proof_sketch}
%%\input{red_proof}

\section{Conclusions}
\label{sec:conc}
\input{conc}

\subsubsection*{Acknowledgements}
This work was supported by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning, NSERC, the National Science Foundation (NSF) under Grants CMMI-1434419, CNS-1446665, and CMMI-1362303, and by the Air Force Office of Scientific Research (AFOSR) under Grant FA9550-15-10050.

\clearpage\newpage
%\begin{small}
\bibliographystyle{apalike}
\bibliography{main}
%\end{small}



%\subsubsection*{References}
\clearpage\newpage
\onecolumn
\appendix
\input{appendix}


\end{document}
