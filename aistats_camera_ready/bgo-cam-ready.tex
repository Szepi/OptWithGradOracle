\documentclass[twoside,11pt]{article}
%\usepackage[accepted]{aistats2016}
\input{commands}
% Compilation note:
% Don't forget to add --shell-escape (or -enable-write18 if you are using MikTeX) to the pdflatex commandline.
% see http://www.howtotex.com/tips-tricks/faster-latex-part-ii-external-tikz-library/

% If your paper is accepted, change the options for the package
% aistats2016 as follows:
%
%\usepackage[accepted]{aistats2016}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\title{(Bandit) Convex Optimization with Biased Noisy Gradient Oracles\thanks{An earlier version of this paper was published in AISTATS 2016 \citep{HuPrGySz16}.}}
\author{Xiaowei  Hu$^1$,  Prashanth L.A.$^2$,  Andr\'as Gy\"orgy$^3$, and Csaba Szepesv\'ari$^1$}
\date{$^1$ Department of Computing Science, University of Alberta \\
$^2$ Institute for Systems Research, University of Maryland \\
$^3$ Department of Electrical and Electronic Engineering, Imperial College London}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%

%\runningauthor{Xiaowei Hu, Prashanth L.A., Andr\'as Gy\"orgy, and Csaba Szepesv\'ari}
%
%\twocolumn[
%
%\aistatstitle{(Bandit) Convex Optimization with Biased Noisy Gradient Oracles}
%
%
%\aistatsauthor{Xiaowei  Hu$^1$ \And Prashanth L.A.$^2$ \And Andr\'as Gy\"orgy$^3$ \And Csaba Szepesv\'ari$^1$ \vspace{1mm}}
%
%\aistatsaddress{$^1$ Department of Computing Science, University of Alberta \\
%$^2$ Institute for Systems Research, University of Maryland \\
%$^3$ Department of Electrical and Electronic Engineering, Imperial College London} ]

\maketitle

\begin{abstract}
\vspace{-0.2cm}
Algorithms for bandit convex optimization and online learning often 
rely on constructing noisy gradient estimates, which are then used
 in appropriately adjusted first-order algorithms, replacing
 actual gradients.
Depending on the properties of the function to be optimized and the nature of ``noise'' in the bandit feedback,
the bias and variance of gradient estimates exhibit various tradeoffs.
% and, correspondingly,
%the algorithms were proven to achieve various convergence rates.
 In this paper we propose a novel framework that replaces the specific gradient estimation
 methods with an abstract oracle.
 % capturing the controlleable bias-variance tradeoff
 %of existing gradient estimators. 
 With the help of the new framework we unify previous works,
 reproducing  their results in a clean and concise fashion, 
 while, perhaps more importantly, the framework also allows us to formally show that to achieve the optimal 
  root-$n$ rate %in online/stochastic bandit convex optimization
  either the algorithms that use existing gradient estimators,
  or the proof techniques used to analyze them 
  have to go beyond what exists today.
\if0
We present a novel noisy gradient oracle model for convex optimization. The model allows to explicitly address the bias-variance tradeoff of gradient estimation methods typical in the literature, as well as to prove upper and lower bounds for the minimax error. As such the oracle model allows a clean way of addressing the limits of achievable performance
when biased, noisy gradient estimators with controllable bias-variance tradeoffs are employed.
When considering the model for online/stochastic convex optimization,
one consequence of our results is that the currently used gradient estimates and the proof techniques used to analyze them cannot 
achieve the optimal square-root regret rate for online/stochastic bandit convex optimization.
\fi
% when the function cannot be queried outside the domain.
\end{abstract}

\vspace{-0.3cm}

\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Problem Setup}
\label{sec:problem}
\input{problem}

\section{Main Results}
\label{sec:results}
\input{results}

\section{Applications to Stochastic BCO}
\label{sec:sbco}
\input{stochastic_bco}

\section{Applications to Online BCO}
\label{sec:obco}
\input{online_bco}

\section{Related Work}
\label{sec:related}
\input{related}


%\section{PROOFS}
%\label{sec:proofs}
%%\input{ub_proof}
%\input{lb_proof_sketch}
%%\input{red_proof}

\section{Proof of the Upper Bounds}
\label{sec:appendix-md}
\input{appendix-upperbds.tex}

\section{Proof of the Lower Bounds}
\label{sec:appendix-lb-proof}
\input{lb_proof}

\section{Gradient Estimation Proofs}
\label{sec:appendix-grad}
\input{appendix-grad.tex}


\section{Conclusions}
\label{sec:conc}
\input{conc}

\appendix


\input{red_proof}



\subsubsection*{Acknowledgements}
This work was supported by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and RLAI, NSERC, the National Science Foundation (NSF) under Grants CMMI-1434419, CNS-1446665, and CMMI-1362303, and by the Air Force Office of Scientific Research (AFOSR) under Grant FA9550-15-10050.

%\clearpage\newpage
%\begin{small}
\bibliographystyle{apalike}
\bibliography{main}
%\end{small}

%\end{document}

%\subsubsection*{References}
\clearpage\newpage
\onecolumn
\appendix
%\input{appendix}


\end{document}
