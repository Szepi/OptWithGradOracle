\documentclass[twoside, 11pt]{article}
%\usepackage[accepted]{aistats2016}
\input{commands}
% Compilation note:
% Don't forget to add --shell-escape (or -enable-write18 if you are using MikTeX) to the pdflatex commandline.
% see http://www.howtotex.com/tips-tricks/faster-latex-part-ii-external-tikz-library/

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ShortHeadings{(Bandit) Convex Optimization with Biased Noisy Gradient Oracles}{}

\title{(Bandit) Convex Optimization with Biased Noisy Gradient Oracles\thanks{An earlier version of this paper was published in AISTATS 2016 \citep{HuPrGySz16}.}}
\author{\name Xiaowei  Hu \email xhu3@ualberta.ca \\
  \addr Department of Computing Science,\\
  University of Alberta,\\
	Edmonton T6G 2E8, Canada
  \AND
  \name Prashanth L.A. \email prashla@isr.umd.edu\\
  \addr Institute for Systems Research, \\
  University of Maryland, \\
	College Park, MD 20742, US
  \AND
  \name Andr\'as Gy\"orgy \email  a.gyorgy@imperial.ac.uk\\
  \addr Department of Electrical and Electronic Engineering,\\
	Imperial College London,\\
  South Kensington Campus, London SW7 2BT, UK 
  \AND
  \name Csaba Szepesv\'ari \email szepesva@cs.ualberta.ca \\
  \addr Department of Computing Science,\\
  University of Alberta,\\
	Edmonton T6G 2E8, Canada
}

 \editor{}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{abstract}
Algorithms for bandit convex optimization and online learning often 
rely on constructing noisy gradient estimates, which are then used
 in appropriately adjusted first-order algorithms, replacing
 actual gradients.
Depending on the properties of the function to be optimized and the nature of ``noise'' in the bandit feedback,
the bias and variance of gradient estimates exhibit various tradeoffs.
% and, correspondingly,
%the algorithms were proven to achieve various convergence rates.
 In this paper we propose a novel framework that replaces the specific gradient estimation
 methods with an abstract oracle.
 % capturing the controlleable bias-variance tradeoff
 %of existing gradient estimators. 
 With the help of the new framework we unify previous works,
 reproducing  their results in a clean and concise fashion, 
 while, perhaps more importantly, the framework also allows us to formally show that to achieve the optimal 
  root-$n$ rate %in online/stochastic bandit convex optimization
  either the algorithms that use existing gradient estimators,
  or the proof techniques used to analyze them 
  have to go beyond what exists today.
\if0
We present a novel noisy gradient oracle model for convex optimization. The model allows to explicitly address the bias-variance tradeoff of gradient estimation methods typical in the literature, as well as to prove upper and lower bounds for the minimax error. As such the oracle model allows a clean way of addressing the limits of achievable performance
when biased, noisy gradient estimators with controllable bias-variance tradeoffs are employed.
When considering the model for online/stochastic convex optimization,
one consequence of our results is that the currently used gradient estimates and the proof techniques used to analyze them cannot 
achieve the optimal square-root regret rate for online/stochastic bandit convex optimization.
\fi
% when the function cannot be queried outside the domain.
\end{abstract}

%\vspace{-0.3cm}

\section{Introduction}
\label{sec:intro}
\input{intro}

\section{Problem Setup}
\label{sec:problem}
\input{problem}

\section{Main Results}
\label{sec:results}
\input{results}

\section{Applications to Stochastic BCO}
\label{sec:sbco}
\input{stochastic_bco}

\section{Applications to Online BCO}
\label{sec:obco}
\input{online_bco}

\section{Related Work}
\label{sec:related}
\input{related}


%\section{PROOFS}
%\label{sec:proofs}
%%\input{ub_proof}
%\input{lb_proof_sketch}
%%\input{red_proof}

\section{Proof of the Upper Bounds}
\label{sec:appendix-md}
\input{appendix-upperbds.tex}

\section{Proof of the Lower Bounds}
\label{sec:appendix-lb-proof}
\input{lb_proof}

\section{Gradient Estimation Proofs}
\label{sec:appendix-grad}
\input{appendix-grad.tex}


\section{Conclusions}
\label{sec:conc}
\input{conc}

\appendix


\input{red_proof}



\subsubsection*{Acknowledgements}
This work was supported by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and RLAI, NSERC, the National Science Foundation (NSF) under Grants CMMI-1434419, CNS-1446665, and CMMI-1362303, and by the Air Force Office of Scientific Research (AFOSR) under Grant FA9550-15-10050.

%\clearpage\newpage
%\begin{small}
%\bibliographystyle{apalike}
\bibliography{main}
%\end{small}

%\end{document}

\end{document}
