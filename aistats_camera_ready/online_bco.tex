%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
In the \emph{online BCO} setting a learner sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the losses $f_1(X_1),\dots,f_n(X_n)$. More specifically, in round $t$, having observed $f_1(X_1),\dots,f_{t-1}(X_{t-1})$ of the previous rounds, the learner chooses $X_t\in \cK$, after which it observes $f_t(X_t)$. The learner's goal is to minimize its expected regret $\EE{ \sum_{t=1}^n f_t(X_t) - \inf_{x\in \cK} \sum_{t=1}^n f_t(x) }$. 
This problem is also called online convex optimization with one-point feedback.
A slightly different problem is obtained if we allow the learner to choose multiple points in every round, at which points the function $f_t$ is observed. The loss is suffered at $X_t$. The points where the function is observed (``observation points'' for short) may or may not be tied to $X_t$. One possibility is that $X_t$ is one of the observation points.  
Another possibility is that $X_t$ is the average of the observation points (e.g., \citet{AgDeXi10}). Yet another possibility is that there is no relationship between them. 

The oracle constructions from the previous section also apply to the online BCO setting, except that one cannot employ two-point feedback as the functions change in each round.  \todoc{Here we should actually introduce $Y$ etc.}
This rules out the controlled noise case, where one can employ two function valuations and cancel out the noise. Thus, for the online BCO setting, one should consider type-I (and II) oracles with $c_1(\delta) = C_1 \delta^p$ and $c_2(\delta) = C_2\delta^{-q}$ with $p=q=2$.
For these type of oracles, the results from Theorem \ref{thm:lb-convex} give the following result: 
\begin{theorem}\label{thm:aaa}
Let $\cF_{L,0}$ be the space of convex, $L$-smooth functions over a convex non-empty domain $\K$.
No algorithm that relies on 
 $(\delta^2,\delta^{-2})$ type-I oracles
 can achieve better regret than $\Omega(n^{2/3})$.
\end{theorem}
\vspace{-0.2cm}
With a noisy gradient oracle of \cref{prop:flaxman}, Theorem~\ref{thm:aaa} implies that this regret rate is achievable, essentially recovering, and in some sense proving optimality of the result of \citet{saha2011improved}:
\begin{theorem}
For zeroth order noisy optimization with smooth convex functions, the gradient estimator of \cref{prop:flaxman} together with mirror descent (see Algorithm \ref{alg}) achieve $\O(n^{2/3})$ regret.
\end{theorem}
\vspace{-0.2cm}
This optimality result shows that with the usual analysis of the current gradient estimation techniques, no gradient method can achieve the optimal regret $O(n^{1/2})$ for online bandit convex optimization, established by \citet{BubeckDKP15,BuEl15}. Note that this shows a contradiction to the recent result of \citet{DeElKo15}, who claimed to achieve $\tilde{O}(n^{5/8})$ regret with the same $(\delta^2,\delta^{-2})$ type-II gradient oracle as \citet{saha2011improved}, but their proof only used the $(\delta^2,\delta^{-2})$ tradeoff in the bias and variance properties of the oracle.\todoa{Actually, all these proofs use Type-IIb oracles....}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bgo"
%%% End:
