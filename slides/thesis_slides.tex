\documentclass[12pt]{beamer}


%\usepackage{palatino}
%\usepackage[mathcal]{euscript}
%\usepackage[mathbf,mathcal]{euler}
%\usepackage{mathbbol}
\usepackage{multicol}
\usepackage[disable]{todonotes}

%\usefonttheme{serif}
%\usefonttheme[onlymath]{serif}

\input{BeamerCommands}
\input{Commands}

% tikz includes
\usepackage{tikz}
\usetikzlibrary{positioning,arrows,shapes,calc,patterns,snakes}
% For every picture that defines or uses external nodes, you'll have to
% apply the 'remember picture' style. To avoid some typing, we'll apply
% the style to all pictures.
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{na} = [baseline=-.5ex]
\everymath{\displaystyle}
% end of tikz includes %%%%%%

\title{Bandit Convex Optimization with\\
 Biased Noisy Gradient Oracles}
\institute{
 University of Alberta \\ 
 \href{mailto:xhu3@ualberta.ca}{\texttt{xhu3@ualberta.ca}}
\\ 
\vspace{0.5cm}
%\includegraphics[scale=0.9]{google-logo} 
%\hspace{2cm} \vspace{0.5cm} 
\includegraphics[scale=0.3]{figs/u-of-alberta-logo}
 }
\author{Xiaowei Hu}
\date{December 20, 2016}

\usepackage{natbib}
%\bibliographystyle{apalike}
\bibliographystyle{apalike}
%\bibliographystyle{alpha}

\DeclareMathOperator{\supp}{supp}
% number sets
\newcommand{\N}{{\mathbb N}}
%\newcommand{\R}{{\mathbb R}}
%\newcommand{\real}{{\mathbb R}}

% spaces, vectors, matrices, distributions
%\newcommand{\eps}{\varepsilon}
\renewcommand{\H}{{\mathcal H}}
%\newcommand{\X}{\mathbf{X}}
%\newcommand{\Y}{\mathbf{Y}}
%\newcommand{\E}{{\mathcal E}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\Z}{{\mathcal Z}}
\newcommand{\w}{{\mathbf w}}
\newcommand{\x}{{\mathbf x}}
\newcommand{\z}{{\mathbf z}}
\newcommand{\h}{{\mathbf h}}
\renewcommand{\P}{{\mathbf P}}
\renewcommand{\c}{{\mathbf c}}
\newcommand{\M}{{\mathbf M}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\D}{{\mathcal D}}
%\newcommand*{\newblock}{}
%\newcommand{\ip}[1]{\langle #1\rangle}
%\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\tnorm}[1]{\|#1\|}
\newcommand{\A}{\mathcal{A}}
%\newcommand{\one}[1]{\mathbb{I}_{\{#1\}}}
%\newcommand{\EE}[1]{\mathbb{E}[#1]}
%\newcommand{\EEgrow}[1]{\mathbb{E}\left[#1\right]}

% operators
%\DeclareMathOperator*{\Exp}{\mathbf{E}}   % expectation
%\DeclareMathOperator*{\Prob}{\mathbf{Pr}}   % expectation
%\DeclareMathOperator*{\Var}{\mathbf{Var}}   % expectation
\DeclareMathOperator*{\Err}{Err}   
%\DeclareMathOperator*{\argmin}{argmin}   
\DeclareMathOperator*{\VC}{VC-dim}   
\DeclareMathOperator*{\Ldim}{L-dim}   
\DeclareMathOperator{\Rademacher}{\mathcal{R}}   
\DeclareMathOperator{\SRademacher}{\mathcal{SR}}   
\DeclareMathOperator*{\sign}{sign}   
\DeclareMathOperator*{\indicator}{\mathbb{1}}   
\DeclareMathOperator*{\minimize}{minimize}   
\DeclareMathOperator*{\conv}{conv}   
%\DeclareMathOperator{\Regret}{Regret}   
\DeclareMathOperator{\rowspace}{rowspace}   
%\DeclareMathOperator*{\argmax}{argmax}   
\DeclareMathOperator*{\polylog}{polylog}

\DeclareMathOperator*{\rows}{\#rows}   
\DeclareMathOperator*{\shattered}{\#shattered}   

\newtheorem{conjecture}{Conjecture}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\maketitle
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Introduction}
\begin{frame}{Outline}
\tableofcontents %[currentsection]
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction: Bandit Convex Optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Convex Optimization with Noisy Bandit Feedback}
	\bcol[c]
	\col[0.5\textwidth]
	\bc
	\includegraphics[width=\textwidth]{figs/Interaction}
	\ec
	\col[0.45\textwidth]
%	\begin{block}<+->{Goal}
    Assume $f$ convex (and smooth etc).\\
	\alert{Goal}\\
	Find a near-minimizer of $f$ using $n>0$ queries!
%	\end{block}

	\ecol
	\bigskip
	\uncover<+->{
		Noisy Bandit Feedback $\equiv$ Noisy zeroth-order information\\
		Derivative-free Methods
	}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Problem Statement}
	\bi
	\item \alert{Optimization:} a single fixed objective function $f: \cK \to \R$, where $\cK \subset \R^d$
		\bi
		\item Round $t$: query at $X_t \subset \cK$
		\item The algorithm outputs $\hat{X}_n$ after $n$ rounds
		\item Optimization error: $\Delta_n = \EE{f(\hat{X}_n) }- \inf_{x\in \cK} f(x)$
		\ei
	\item \alert{Online learning:} a sequence of functions $f_1, \cdots, f_n$ chosen by the environment
		\bi
		\item Round $t$: query at $Y_t$ in a small vicinity of $\cK$
		\item The algorithm suffers the loss $f_t(Y_t)$
		\item expected regret: $R_n = \EE{\sum_{t=1}^n f_t(Y_t) }- \inf_{x\in \cK} \sum_{t=1}^n f_t(x)$
		\ei
	\ei
	\begin{block}<+->{Main Question}
	\bc
		How fast can/will/should the optimization error or learning regret
		 decrease with $n$?
	\ec		 
	\end{block}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Motivation: Why Bandit Feedback?}
	\setlength\itemsep{1em}
	\bi
	\item Explicit gradient is unavailable:
		\bi
		\setlength\itemsep{0.5em}
		\item Black-box problem: Controlling an unknown system
		\item Simulation-based optimization: 
			\bi
			\setlength\itemsep{0.5em}
			\item
				$f(x) = \EE{ F(x,\xi) }$ --- $\xi$: simulation noise
			\item $F(x,\xi)$ is the output of a simulation;
			\item \citep{spall2005introduction} Introduction to stochastic search and optimization 
			\ei
		\ei
	\ei
	\bigskip
	\bi
			\setlength\itemsep{0.5em}
	\item Gradient calculation is too expensive/complicated:
		\bi
			\setlength\itemsep{0.5em}
		\item Graphical model inference
			\bi
			\item Objective functions defined in a variational way
			\item \citep{wainwright2008graphical} Graphical models, exponential families, and variational inference
			\ei
		\ei
	\ei
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Example: Resource Allocation}
	\bcol[c]
	\col[0.45\textwidth]
	\bc
	\includegraphics[width=\textwidth]{figs/memory}
	\ec
	\col[0.55\textwidth]
	\bi
	\item Distribute memory between jobs
	\item Maximize success: $\max_{x\in \cK} f(x)$
	\item Linear constraints: $\cK = \cset{ x\in [0,1)^d}{\sum_i x_i = 1}$
	\item Concave objective $f$: 
	Find the best configuration quickly.
	\ei
	\ecol
	\bigskip
	\uncover<+->{The only way to learn about $f$ is to try different configurations.}
	\vspace{0.5cm}
	\uncover<+->{Feedback: $f(x) + \mathrm{noise}$ for each $x$ selected. \alert{``Bandit'' feedback}}\\
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Subclasses of Problems: Curviness}
	\bcol[c]
	\col
	\bc
	\uncover<+->{
	\begin{minipage}[t]{0.9\textwidth}
		\alert{Curviness} of functions:\\
		$\cK \subset \R^d$ \\
		convex, closed, non-empty.\\
	    Bregman divergence:
		\begin{align*}
		& D_f(w||v)\doteq\\
		& f(w)- \left\{ f(v) + \ip{\nabla f(v),w-v} \right\}.
		\end{align*}
	\end{minipage}}
	\ec
	\col
	\includegraphics[width=\textwidth]{figs/entropy-16-06338f2-1024}
	\ecol
	
	\bcol[t]
	\col[0.6\textwidth]
	\bi
	\item
	Smoothness: 
	$\qquad
	D_f(x||y) \le \frac{L}{2} \norm{x-y}^2\,.
	$
	\item
	Strong convexity:
	$\qquad
	D_f(x||y) \ge \frac{\mu}{2} \norm{x-y}^2\,.
	$
	\ei
	\col[0.4\textwidth]
	\figincol{
	\includegraphics[width=\textwidth]<4->{figs/Problems}}
	\ecol

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Subclasses of Problems: Noise}
	\alert{Noisy} bandit feedback:
	Recall $f(X) = \EE{ F(X,\xi) }$.

	\bigskip
	\bigskip

	\begin{block}{Assumptions}
	\medskip
	\bcol
	\col[0.015\textwidth]
	\mbox{}
	\col[0.995\textwidth]
	\bi
	\item[(A1)] \emph{``Uncontrolled noise''}: $F(X,\xi)$ can be obtained at any point.
	\item[(A2)] \emph{``Controlled noise''}: $\xi$  can be kept fixed between queries. 
	\ei
	\ecol
	\bigskip
	\end{block}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{The State-of-the-Art for Noisy Bandit Convex Optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{State of the Art}
	\bi
	\item For general convex functions
	\begin{align*}
	        \tikz[baseline]{
            \node[fill=red!20,anchor=base] (t1)
            {Optimal rate: $\Delta_n = \tilde{O}(\sqrt{1/n})$.};}	
	\end{align*}	
	\item But it is \textbf{non-constructive}!   \citep{BubeckDKP15}
	\item Two approaches to this problem:
		\bi
		\item \alert{``Gradient'' Method}: try to construct gradient information with noisy bandit feedback
		\item \alert{Ellipsoid Method}: avoid gradient estimation and rather use  geometric principles
		\ei
	\ei
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Controlled Noise: Simple Story}

	\bcol
	\col[0.1\textwidth]
	\col[0.9\textwidth]

	\uncover<+->{``Gradient'' method}
	\bigskip
	\bigskip
	
	\uncover<+->{$\Delta_n \le C \sqrt{d^2/n}$.}

	\bigskip
	\bigskip
	\uncover<+->{\citet{Ne11:TR}	,
	\citet{duchi2015optimal} }

	
	\bigskip
	\bigskip
	
	\uncover<+->{Optimal!}
	\ecol
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Uncontrolled Noise: Big Gaps}
			
%In their seminal work \citet{NeYu83} consider two approaches to this problem: Methods that try to construct gradient information and methods that avoid gradient estimation and rather use  geometric principles (the ellipsoid method, essentially). While methods in the second class make the error decay at the $O(1/\sqrt{n})$ rate,  the error scales extremely poorly with the number of optimization variables $d$.

\uncover<+->{Ellipsoid method {(\footnotesize \citet{NeYu83}, Section 9.4)}}
\bi
\item  \citet{AgFoHsuKaRa13:SIAM} --  $\sqrt{d^{33}/n}$
\item \citet{liang2014zeroth} -- $\sqrt{d^{14}/n}$
\ei 

\bigskip
 
\uncover<+->{``Gradient'' methods: {(\footnotesize \citet{NeYu83}, Section 9.3)}}
\bi
\item
	\textbf{Convex}: -- $\qquad (d^2/n)^{1/4}$\\
	{\footnotesize  \citep{NeYu83,flaxman2005online}}
\item
	\textbf{Smooth}:  -- $\qquad  (d^2/n)^{1/3}$ \\
	{\footnotesize \citep{NeYu83,saha2011improved}}
\item
	Smooth + SOC:  $\qquad \alert{\sqrt{d^2/n}}$\\
	{\footnotesize \citep{HaLe14:SOC} }
\ei

\bigskip
\uncover<+->{
Lower bound: $\alert{\sqrt{d^2/n}}$  \citep{shamir2012complexity}.
}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Can We Do Better?}
	\bi
	\item Ellipsoid method: suffer from the dimension $d$.
	\bigskip
	\bigskip
	\item ``Gradient'' method: 
		\bi
		\item Better gradient estimates?
		\bigskip
		\item {\color{darkgreen}\textbf{A ``clever'' gradient method?}} Make better use of gradient estimates?
		\ei
	\bigskip
	\bigskip
	\item $\cdots$
	\ei
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gradient Estimation Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\frame{
	\frametitle{Gradient Estimation: Two-sided Differences}	
			Explore in the direction $e_i$:
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$g_i = \frac{1}{2\delta} \left\{ f(x+\delta e_i) - f(x-\delta e_i) \right\},\quad i=1,\dots,d.$};}	
						\end{align*}		

	Taylor-series expansions:
	\begin{align*}
	f(x+ \delta e_i)  &= f(x) + \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).\\
	f(x\alert{-} \delta e_i)  &= f(x) \alert{-} \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).
	\end{align*}

	\bigskip

		\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {Accuracy: $\norm{ g - \nabla f(x) }_2 = O(\sqrt{d} \delta^2 )$.};}	
						\end{align*}		
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Gradient Estimation: Noisy Bandit Feedback}	
\begin{small}
	\uncover<+->{
	
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=magenta!20,anchor=base] (t1)
            {$\tilde{g}_i = \frac{1}{2\delta} \left\{ \left(f(x+\delta e_i) + \xi^+_i\right)- \left(f(x-\delta e_i)+\xi^-_i\right) \right\},\quad i=1,\dots,d.$};}	
						\end{align*}		
}

	\uncover<+->{ 
	Assumption: $\EE{ \xi^{\pm} } = 0$, $\EE{ (\xi^{\pm})^2 } \le \sigma^2 <+\infty$.}
		
	\uncover<+->{ Note: $\tilde{g}_i = g_i + \dfrac{\xi_i^+ - \xi_i^-}{2\delta}$, $\EE{ \tilde{g}_i } = g_i$.}
	
	\uncover<+->{ \alert{Bias}:
				\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$\norm{ \EE{\tilde{g}} - \nabla f(x) }_2=\norm{ g - \nabla f(x) }_2 = O(\sqrt{d} \alert{\delta^2} )\,.$ };}	
						\end{align*}}		
	
	\uncover<+->{ \alert{Second moment}: $\EE{\tilde{g}_i^2} = g_i^2 + \frac{2\sigma^2}{4 \delta^2} $}
	
	\bi
	\item \emph{Controlled:} $	\EE{ \norm{\tilde{g}}_2^2 } = \norm{g}_2^2\,.$
	\item \emph{Uncontrolled:} $	\EE{ \norm{\tilde{g}}_2^2 } = \norm{g}_2^2 + O\left( \frac{d}{\alert{\delta^2}} \right)\,.$
	\ei
\end{small}	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Random Exploration Direction}
	\uncover<+->{We need $2d$ queries to get $g$.}	
	
	\uncover<+->{Can we reduce the number of queries?}
	
	\uncover<+->{Idea: \alert{ Randomly choose direction! }
	
	$I \sim p(\cdot)$ a positive \emph{probability mass function} over $\{1,\dots,d\}$.}
	
	\uncover<+->{Choose
	\begin{align*}
	G_i &= \frac{1}{p(i)} \, \frac{f(x+\delta e_I)-f(x-\delta e_I)}{2\delta} \,e_{I,i}\\
	&=
	\begin{cases}
		\frac{1}{p(I)} \, \frac{f(x+\delta e_I)-f(x-\delta e_I)}{2\delta},  & I= i\,;\\
		0, & \text{otherwise}\,.
	\end{cases}
	\end{align*}}
	
	\uncover<+->{
	\alert{Only 2 queries, regardless of $d$!}}
	
	\uncover<+->{$\EE{G_i} = g_i$! }
	\uncover<+->{Hence, bias: $\norm{\EE{ G } - \nabla f(x)}_2 = O( \sqrt{d} \delta^2 )$.}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Second Moment from Randomization}
	\uncover<+->{
	\begin{align*}
	G_i &= \frac{1}{p(i)} \, \frac{f(x+\delta e_I)-f(x-\delta e_I)}{2\delta} \,e_{I,i}
	\end{align*}}
	
	\uncover<+->{
	Taylor-series expansion:	
	\begin{align*}
	f(x+ \delta e_i)  &= f(x) + \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3),\\
	f(x- \delta e_i)  &= f(x) - \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).
	\end{align*}	
	}
	\uncover<+->{
	\begin{align*}
	 G_i^2 & = \frac{\one{I=i}}{p^2(i)} 
		\frac{(2\delta f'_i(x) + O(\delta^3))^2}{4 \delta^2} 
		= %%GYA
	 \frac{\one{I=i}}{p^2(i)} 
		\frac{(\delta^2 (f_i'(x) )^2+ O(\delta^4))}{ \delta^2} 		\\
		&=
	 \frac{\one{I=i}}{p^2(i)} 
		\left\{ ( f'_i(x) )^2+ O(\delta^2) \right\}
	\end{align*}}
	
	\uncover<+->{
	Hence, $\EE{ G_i^2} = O(1/p(i))$, so at best $\EE{ \norm{G}_2^2 } = O( d^2 )$. 
	}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Noisy Bandit Feedback}

	\uncover<+->{
	\begin{align*}
	\tilde{G}_i &= \frac{1}{p(i)} \, \frac{(f(x+\delta e_I)+\xi^+)-(f(x-\delta e_I)+\xi^-)}{2\delta} \,e_{I,i}
	\end{align*}}
	
	\uncover<+->{
	Hence,
	$\tilde{G}_i= \frac{\one{I=i}}{p(i)} \frac{ \xi^+-\xi^- }{2 \delta} + G_i$}
	\uncover<+->{
	and
	\begin{align*}
	\EE{\tilde{G}_i} &= \EE{ G_i}\,,\\
	 \EE{ \tilde{G}_i^2 } &= \frac{1}{p(i)} \frac{ \EE{ (\xi^+-\xi^-)^2} }{ 4 \delta^2 } + \EE{ G_i^2 }\,,
	\end{align*}	}
	\uncover<+->{
	so
	\begin{align*}
	\norm{\EE{ \tilde{G} }_2 - \nabla f(x) }_2 &= O( \alert{\sqrt{d} \delta^2} )\,,\qquad \text{ \alert{reamin the same}}\\
	\EE{ \norm{\tilde{G} }_2^2 } & = O( d^2(1 + 1/\alert{\delta^{2}} ) )\,.  \qquad \text{ \alert{increase by d}}
	\end{align*}}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Gradient Estimation: One-Point}
	\textbf{Intuition:}
	\bigskip
	\bigskip
	\bi
	\item For one-dimension: let $u=\pm 1$ with equal probability
		\bi
		\item $\EE{\dfrac{f(x+\delta u)}{\delta} u} = \dfrac{f(x+\delta )-f(x-\delta )}{2\delta}$
		\ei
	\bigskip
	\bigskip
	\item For $d$-dimension: $\nabla f(x) = \dfrac{df}{dx_1}(x), \cdots,  \dfrac{df}{dx_d}(x)$
	\bigskip
	\bigskip
	\item Vector calculus: Gauss-Ostrogradsky theorem or the divergence theorem
	\ei
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{General Two-Point \& One-Point Estimates}
	\uncover<+->{Two-point estimate:
	
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$G = \frac{ (f(x+U)+\xi^+) - (f(x-U)+\xi^-)}{2\delta} V\,.$};}	
						\end{align*}		
	}

	\uncover<+->{
	One-point estimate!
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$G = \frac{ (f(x+U)+\xi^+) }{\delta} V\,.$};}	
						\end{align*}		
}
	\uncover<+->{
	Choose $U,V$ such that $\EE{ V U^\top  } = I$, $\EE{ V } = 0$.}\\
	\uncover<+->{How does this work??}\\
	\uncover<+->{$\EE{G} = \EE{ G - \frac{f(x)}{\delta} V } = \EE{\frac{ (f(x+U)+\xi^+)-f(x) }{\delta} V}$.}
	
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Examples of Gradient Estimation Methods}
	\bi
	\item $U \sim \delta\, \cN(0,I)$, 
	         $V = \delta^{-1}\, U$
	         \bi
	         \item Smoothed functional scheme by \cite{katkul};
	         \item Refined by   \citet{PoTsy90};
	         \item Further studied by \cite{Dip03:AoS,Ne11:TR}.
	         \ei
	\item $U \sim \delta\, \mathrm{Unif}(\mathbb{S}_d)$, 
			 $V =d \delta^{-1}\,  U$
			 \bi
			 \item RDSA by \cite{kushcla};
			 \item Rediscovered by \cite{flaxman2005online}
			 \ei
	\item $U_i \sim \delta\, \mathrm{Rademacher}(\pm 1)$, 
			 $V = \delta^{-1} \,U$
			 \bi
			 \item
			  SPSA by \cite{spall1992multivariate}.
			 \ei		 
	\ei
	
	\uncover<+->{
	\bc
	\textbf{Does it matter which of these we select? Not really:}\\
	when $ f\in \cC^3$,\\ 
	Bias: $O(\delta^2)$, second moment: $O(1)$ or $O(\delta^{-2})$.
	\ec
	}

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Oracle Model: Biased, Noisy Gradient Oracles}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Gradient Estimation Oracles}
	
	\bc
	\includegraphics[width=\textwidth]{figs/oracle0}

	\ec	
	

\begin{enumerate}[<+->]
\item Bias: $\norm{ \EE{G}  - \nabla f(x)  }_* \le c_1(\delta) $; and
\item Second moment: $\EE{\norm{ G  }_*^2} \le c_2(\delta)$.
\end{enumerate}


\bc
	\uncover<+->{
Polynomial oracle: $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q\ge 0$.}
	\bigskip

	\uncover<+->{
Controlled noise: $c_1(\delta) = C_1\delta^2$, $c_2(\delta) = C_2$.}

	\uncover<+->{
Uncontrolled noise:  $c_1(\delta) = C_1\delta^2$, $c_2(\delta) = C_2 \delta^{-2}$.}

%%%GYA: mention what matters is p/q

\ec



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Upper Bound: Algorithm}

\begin{block}{Mirror descent \citep{NeYu83}}
\begin{algorithmic}
    \State {\bf Input:}  Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$.
%     In round $t=1, 2, \cdots, n-1$:
\State Initialize $X_1\in \cK$ arbitrarily.
\For{$t=1, 2, \cdots, n-1$}	
	\State Query the oracle at $X_t$.
	\State Receive $G_t$.
	\State Update
	$$
	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
	$$
\EndFor
\State {\bf Return:} $\hat{X}_n = \frac{1}{n}\sum_{t=1}^n X_t \,.$
\end{algorithmic}
\end{block}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Upper Bound}

\begin{theorem}[\textit{\textbf{Upper bound}}]
Consider the Mirror Descent algorithm with a $(c_1, c_2)$, $(p,q)$ polynomial oracle, $\alpha$-strongly convex regularizer $\cR$.
Then:
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{2p+q} } )\\
\Delta_n(\F_{L,\mu},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{p+q} } )\,.
\end{align*}
\end{theorem}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Can We Get an Optimal Rate?}
\uncover<+->{
Recall:\vspace*{-0.2in}
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{2p+q} } )\\
% \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
\Delta_n(\F_{L,\mu},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{p+q} } )\,.
%\le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
}

\bigskip
\uncover<+->{
\center{\alert{\textbf{Yes!}}}\\ 
\bigskip
if $p/(2p+q)\ge 1/2$ for smooth, $p/(p+q)\ge 1/2$ for smooth+SOC.}\\
\bigskip
\uncover<+->{
First holds iff $q=0$. Second holds iff $p\ge q$.\\
}
}

\frame{
	\frametitle{Recover State-of-the-art Results}
\uncover<+->{
Recall:\vspace*{-0.2in}
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{2p+q} } )\\
% \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
\Delta_n(\F_{L,\mu},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{p+q} } )\,.
%\le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
}

\bigskip

\uncover<+->{
\alert{Uncontrolled noise}; under smoothness, $p=q=2$.} \\
\uncover<+->{
\tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {For $\F_{L,\mu}$ we get $O(\alert{n^{-1/2}})$ as \citet{HaLe14:SOC}.};}	
}\\[1ex]
\uncover<+->{
\tikz[baseline]{
            \node[fill=red!20,anchor=base] (t1)
            {For $\F_{L,0}$ we get $O(n^{-1/3})$ as \citet{saha2011improved}.};}	
}

\bigskip

\uncover<+->{
\alert{Controlled noise}: under smoothness, $p=2$, $q=0$.}\\
\uncover<+->{
\tikz[baseline]{
            \node[fill=blue!20,anchor=base] (t1)
            {For $\F_{L,\mu}$ we get $O(\alert{n^{-1}})$ as 
	\citet{Ne11:TR}.};}	
}\\
\uncover<+->{
\tikz[baseline]{
            \node[fill=brown!20,anchor=base] (t1)
            {For $\F_{L,0}$ we get $O(\alert{n^{-1/2}})$ as	
	\citet{duchi2015optimal}.};}	
}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Does the ``Clever'' Algorithm Exist?}

\uncover<+->{
\begin{theorem}[\textit{\textbf{Lower bound}}]
\label{thm:lb-convex}
$\cK\subset \R^d$ convex, closed, with  $\{+1,-1\}^d\subset \cK$.
For any algorithm $\mathrm{A}$ that observes $n$ random elements from a $(c_1, c_2)$,  $(p,q)$ polynomial oracle, we have
 \vspace*{-0.1in}
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{A},c_1,c_2 ) &= \Omega( n^{-\frac{p}{2p+q}}),\\
\Delta_n(\F_{L,1},\mathrm{A},c_1,c_2 ) & = \Omega(  n^{-\frac{\alert{2}p}{2p+q}})\,.
\end{align*}
\vspace*{-0.2in}
\end{theorem}}

\uncover<+->{
Compare with $O( n^{- \frac{p}{2p+q} } )$ and $O(n^{-\frac{2p}{2p+\alert{2}q}})$
%\begin{align*}
%\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
%= O( n^{- \frac{p}{2p+q} } )\\
%% \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
%\Delta_n(\F_{L,1},\mathrm{MD},c_1,c_2 ) &
%= O( n^{- \frac{p}{p+q} } )  = O(n^{-\frac{2p}{2p+\alert{2}q}})\,.
%%\le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
%\end{align*}
%\vspace*{-2cm}
\bi
\item The lower bound for $\F_{L,0}$ is tight, for $\F_{L,1}$ it is weak.
\item Rule out the improvement of ``gradient'' methods in \citep{DeKo15:BSCO} and \citep{yang2016optimistic}.
\ei
}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Lower Bound Idea}
	\bcol[c]
	\col[0.4\textwidth]
	\bc
	\includegraphics[width=\textwidth]{figs/lb_proof_graph}
	\ec
	\col[0.55\textwidth]
	\bi
	\item	Construct two loss functions: noisy gradient estimates are \alert{so close} that they can \textbf{NOT} be distinguished.

 \item Choose $\epsilon \left| x-v \right|$, where $v \in \{-1,+1\}$.

 \item Smooth approximation:
\begin{align*}
f_v(x) :=\epsilon\left( x-v\right)+2\epsilon^2 \ln\left(1+e^{-\frac{x-v}{\epsilon}}  \right)
\end{align*}
 is $0.5$-smooth.
 
 \item Oracle {\color{darkgreen}(dashed line)} shifted with a bias $\min\left(\epsilon,C_1\delta^p\right)$.
	\ei
	\ecol
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Proof Sketch}
	Denote $b=\min\left(\epsilon,C_1\delta^p\right)$.\\
	Define
\begin{align*}
\gamma_+(x,\delta)&=
			\begin{cases}
               f'_+(x)+b &x\leq 0\\
               \min \left( f'_+(x)+b, f'_-(x)-b \right) &x>0 
            \end{cases}\\            
\gamma_-(x,\delta)&=-\gamma_+(-x,\delta) \,.
\end{align*}
We get
$
\left| \gamma_+(x,\delta)-\gamma_-(x,\delta)\right|<2(\epsilon-C_1\delta^p)_+ 
$. 
\begin{align*}
\Delta_n^{*}  \ge & \inf_{\A} \dfrac{\epsilon}{2}\,  \P(\hat X_n V < 0), \\
  \ge &\dfrac{\epsilon}{2} \left(1 - \sqrt{
    n}  \dfrac{ \sup_{\delta>0} (\epsilon-C_1\delta^p)_+\delta^{q/2}}{C_2}
  \right)
\end{align*}
Choosing $\epsilon$ that maximizes the bound gives the result.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Conclusion}
\begin{Corollary}	
To get the optimal $O(n^{-1/2})$ rate for $\F_{L,0}$  with uncontrolled noise,
with a low-complexity algorithm,
one of the following must be done:
\begin{enumerate}
\item An oracle with $q=0$ (constant second moment bound) must be designed.
\item 
\sout{An algorithm that makes better use of the gradient estimates must be designed.}
\item Some extra properties of gradient estimates must be exploited beyond bias/variance.
\item Design a non-gradient algorithm. 
\end{enumerate}
\end{Corollary}	
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Big Picture: Noisy Bandit Optimization}
	\bi
	\item Linear case: Pretty well understood
	\bigskip
	\item Controlled noise or noise-free: Pretty well understood
	\bigskip
	\item Uncontrolled noise: Not much is known about low complexity, optimal algorithms!
		\bi
		\item Theoretical progress to bridge the gap
		\bigskip
		\item Current ``gradient'' methods are essentially sub-optimal
		\bigskip
		\item Go beyond bias-variance trade-off
		\ei
	\ei
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
\begin{center}
\LARGE{Thanks! Questions?}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%\frametitle{}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks]
	\frametitle{References}
%        \begin{center}
%          Thanks!
%        \end{center}
	%\bibliographystyle{acm}
%	\begin{multicols}{2}
	\scriptsize 
	%\scriptsize\tiny
%	\bibliography{biblio,allbib,shortconfs,../thebib}
%	\bibliography{allbib,shortconfs,../thebib}
%\bibliography{allbib,shortconfs}
\bibliography{allbib,main}
%	\bibliography{../thebib}
%	\end{multicols}
\end{frame}


\end{document}
