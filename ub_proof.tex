%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
\subsection{Proofs for Upper Bounds}
\label{sec:ub-proof}

Before giving the proof, we introduce a useful lemma, which is essentially Theorem~C.4 of \cite{MahdaviPhd:2014}.
\begin{lemma}
\label{lem:ub}
Let $({\cF})_{t}$ be a filtration such that $X_t$ is ${\cF}_t$-measurable.
Let $\overline G_t = \EE{G_t|{\cF}_t}$ 
and assume that the nonnegative real-valued deterministic sequence $(\beta_t)_{1\le t\le n}$ is such that 
$\norm{\overline G_t - \nabla {f}(X_t)}_* \le \beta_t$ holds almost surely. 
Further, assume that $D=\sup_{x,y\in \cK} D_{\mathcal{R}}(x,y)$ and let $\eta_t = \frac{\alpha}{a_t+L}$ for some increasing 
sequence $(a_t)_{t=1}^{n-1}$ of numbers. Then, if ${f}$ is $L$-smooth,
\begin{align*}
\MoveEqLeft \EE{ \sum_{t=1}^n {f}(X_t) - {f}(x) }  \\
\le& 	 \EE{{f}(X_1)-{f}(x)}+ \\
 & \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t 
 +\frac{D(a_{n-1}+L)}{\alpha} +
	  \sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,,
\end{align*}
where $\sigma_t^2 = \EE{ \norm{G_t-\overline G_t}_*^2}$ is the ``variance'' of $G_t$.

If ${f}$ is also $\mu$-strongly convex w.r.t. $\mathcal{R}$, let $\eta_t = \dfrac{2}{\mu t}$, $a_t = \dfrac{\alpha \mu}{2}t-L$,
\begin{align*}
\MoveEqLeft \EE{ \sum_{t=1}^n {f}(X_t) - {f}(x) }  \\
\le& 	 \EE{{f}(X_1)-{f}(x)}+ 
 \sqrt{\tfrac{2D}{\alpha}} \sum_{t=1}^{n-1} \beta_t 
 +\sum_{t=1}^{n-1}\frac{\sigma_t^2}{2a_t}\,.
\end{align*}
\end{lemma}
This is also identical to Theorem~6.3 of \cite{Bu:Convex14}, who cites \cite{Dekel:minibatch12} as the source. These results assume that $\beta_t=0$ above.
The proof of this lemma can be found in \cref{sec:appendix-md}.

Proof of Theorem \ref{thm:ub}:

Since $f$ is convex, by Jensen's inequality and the bias condition in \cref{def:oracle2}, we get
\begin{align*}
\MoveEqLeft
 \E[f(\hat X_n) - \inf_{x \in \cK} f(x)] \\
 \le&  
 \dfrac{1}{n}\EE{ \sum_{t=1}^n f(X_t) - \inf_{x \in \cK}f(x) } \\
 \le &\dfrac{1}{n}\EE{ \sum_{t=1}^n \tilde{f}(X_t) - \inf_{x \in \cK}\tilde{f}(x) } +2C_1 \delta^p
 \,.
\end{align*}
Given $\overline{G}_t=\EE{G_t} = \nabla \tilde{f}(X_t)$,
the result immediately follows from Lemma \ref{lem:ub}. 
When $f$ is only smooth,
\todox{$\tilde{f}$ should also be smooth, but this cannot be ensured with current oracle assumption...}
by replacing
 $\beta_t = 0$, $\sigma^2_t = C_2 \delta^{-q}$, $a_t=a t^r$ respectively, we obtain
 \begin{align}
 \MoveEqLeft
 \E[f(\hat X_n) - \inf_{x \in \cK} f(x)]  \nonumber\\
&\le \dfrac{1}{n}\left(\EE{f(X_1)-f(x)}+\dfrac{DL}{\alpha}  \right)\nonumber\\
&+\dfrac{D}{\alpha}a n^r+\dfrac{C_2 \delta^{-q}}{2a}n^{1-r}+ 4C_1\delta^p \,.
\label{eq:ubToBeOpt}
 \end{align}
 Choosing $r = \dfrac{p+q}{2p+q}$, and $a$, $\delta$ as stated in \cref{thm:ub}, the last $3$ terms in \eqref{eq:ubToBeOpt} are optimized to
 \[
 K_1 C_1^{q/(2p+q)} C_2^{p/(2p+q)} n ^{-p/(2p+q)} \,,
 \]
 where 
% $K_1 = 2^{\dfrac{3q}{2(p+q)}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{\dfrac{p+q^2+2pq}{(p+q)(2p+q)}}+2^{\dfrac{3q}{2(2p+q)}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{2-\dfrac{1}{2p+q}}$.
 $K_1 = 2^{\dfrac{3q(3p+2q)}{2(p+q)(2p+q)}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{\dfrac{p+pq}{(p+q)(2p+q)}}+2^{\dfrac{3q}{2p+q}} \left( \sqrt{\dfrac{D}{\alpha}} \right)^{2-\dfrac{1+q}{2p+q}}$.
 
 When $f$ is also $\mu$-strongly convex,
 \begin{align*}
 \MoveEqLeft
 \E[f(\hat X_n) - \inf_{x \in \cK} f(x)] -\dfrac{1}{n}\left(\EE{f(X_1)-f(x)}\right)\\
&\le 4C_1\delta^p+\dfrac{C_2 \delta^{-q}}{\alpha \mu n} \sum_{t=1}^{n-1}\dfrac{1}{t-\dfrac{2L}{\alpha \mu}}\\
&\le 4C_1\delta^p+\dfrac{C_2 }{\alpha \mu}\delta^{-q} \dfrac{\log n}{n}\\
&= K_2C_1^{\dfrac{q}{p+q}}C_2^{\dfrac{p}{p+q}} \left( \dfrac{\log n}{n} \right)^{\dfrac{p}{p+q}}
 \end{align*}
The last step optimizes the bound via letting 
%$\delta^{p+q} =  \sqrt{\dfrac{\alpha}{2D}}\dfrac{C_2 \log n}{\alpha \mu C_1 n}$, 
$\delta^{p+q} =  \dfrac{C_2 \log n}{4\alpha \mu C_1 n}$, 
where
%$K_2^{p+q}=\sqrt{2}^{2p+3q}D^{q/2}\alpha^{-p-q/2}\mu^{-p}$.
$K_2^{p+q}=2^{p+3q}\alpha^{-p}\mu^{-p}$.