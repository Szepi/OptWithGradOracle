%!TEX root =  bgo-cam-ready.tex
% please do not delete or change the first line (needed by Csaba's editor)
In the \emph{online BCO} setting a learner sequentially chooses the points $X_1,\dots,X_n\in \cK$ while observing the losses $f_1(X_1),\dots,f_n(X_n)$. More specifically, in round $t$, having observed $f_1(X_1),\dots,f_{t-1}(X_{t-1})$ of the previous rounds, the learner chooses $X_t\in \cK$, after which it observes $f_t(X_t)$. The learner's goal is to minimize its expected regret $\EE{ \sum_{t=1}^n f_t(X_t) - \inf_{x\in \cK} \sum_{t=1}^n f_t(x) }$. 
This problem is also called online convex optimization with one-point feedback.
A slightly different problem is obtained if we allow the learner to choose multiple points in every round, at which points the function $f_t$ is observed. The loss is suffered at $X_t$. The points where the function is observed (``observation points'' for short) may or may not be tied to $X_t$. One possibility is that $X_t$ is one of the observation points.  
Another possibility is that $X_t$ is the average of the observation points (e.g., \citet{AgDeXi10}). Yet another possibility is that there is no relationship between them. 

The oracle constructions from the previous section also apply to the online BCO setting
where the algorithm is evaluated at $Y_t$, though in this case 
one cannot employ two-point feedback as the functions change between rounds. 
This also rules out the controlled noise case. 
Thus, for the online BCO setting, one should consider type-I (and II) oracles with $c_1(\delta) = C_1 \delta^p$ and $c_2(\delta) = C_2\delta^{-q}$ with $p=q=2$.
For these type of oracles, the results from \cref{thm:lb-convex} give the following result: 
\begin{theorem}\label{thm:aaa}
Let $\cF_{L,0}$ be the space of convex, $L$-smooth functions over a convex body $\K$.
No algorithm that relies on 
 $(\delta^2,\delta^{-2})$ type-I oracles
 can achieve better regret than $\Omega(n^{2/3})$.
\end{theorem}
%\vspace{-0.2cm}
With a noisy gradient oracle of \cref{prop:flaxman}, \cref{thm:aaa} implies that this regret rate is achievable, essentially recovering, and in some sense proving optimality of the result of \citet{saha2011improved}:
\begin{theorem}
For zeroth order noisy optimization with smooth convex functions, the gradient estimator of \cref{prop:flaxman} together with mirror descent (see \cref{alg}) achieve $\O(n^{2/3})$ regret.
\end{theorem}
%\vspace{-0.2cm}
This ``optimality result'' shows that with the usual analysis of the current gradient estimation techniques, no gradient method can achieve the optimal regret $O(n^{1/2})$ for online bandit convex optimization, established by \citet{BubeckDKP15,BuEl15}. Note that \cref{thm:aaa} contradicts the recent result of \citet{DeElKo15}, who claimed to achieve $\tilde{O}(n^{5/8})$ regret with the same $(\delta^2,\delta^{-2})$ type-II gradient oracle as \citet{saha2011improved}, but their proof only used the $(\delta^2,\delta^{-2})$ tradeoff in the bias and variance properties of the oracle.\todoa{Actually, all these proofs use Type-IIb oracles....}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bgo"
%%% End:
