%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
\subsection{Proofs for the Lower Bound}
\label{sec:lb-proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Define the probability space $(\Omega, \B, P_{\A,\gamma,\sigma^2})$, where $\Omega = (\R\times \{-1,1\})^n$, $\B$ is the associated Borel sigma algebra. Further, the probability measure $P_{\A,\gamma,\sigma^2} := p_{\A,\gamma,\sigma^2} d(\lambda \times m)$, where $\gamma$ is the oracle in the family of oracles $\Gamma(f)$ for some given $f$, $\lambda$ is the Lebesgue measure on $\B$, $m$ is the counting measure on $\{-1,1\}$ and $p_{\A,\gamma,\sigma^2}$ is the density function defined as\footnote{We use the shorthand $g_{1:n}$ to denote $g_1,\ldots,g_n$.}
\begin{align*}
&p_{\A,\gamma,\sigma^2}(g_{1:n}, v) = \dfrac{1}{2} \bigg(p_{\A,\gamma,\sigma^2}(g_n \mid g_{1:n-1}) \\
&\times p_{\A,\gamma,\sigma^2}(g_{n-1} \mid g_{1:n-2}) \ldots p_{\A,\gamma,\sigma^2}(g_1)\bigg)\\
 = & \dfrac{1}{2} \bigg( p_{\N}(g_n - \gamma(\A_n(g_{1:n-1})))
%  &\times p_{\normal(0,\sigma^2)}(g_{n-1} - \gamma(\A_n(g_{1:n-2}))) \times \ldots \\
 \ldots \times  p_{\N}(g_1 - \gamma(\A_1))\bigg),
\end{align*}
where $p_{\N}$ is the density of a $\normal(0,\sigma^2)$ random variable.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

We consider a class of functions $\F = \{f_+, f_-\}$, with 
\begin{align*}
  f_+(x) := \dfrac{\epsilon}{2} (x - 1)^2 \text{ and } f_-(x) := \dfrac{\epsilon}{2} (x + 1)^2, \forall x \in \R.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

It is easy to see that $f_+$ (resp, $f_-$) is minimized at $x^*_+ = 1$ (resp. $x^*_- = -1$). 
Using the fact that $f_+$ and $f_-$ are strongly convex with associated constant $\left(\dfrac{\epsilon}{2}\right)$, we obtain
\begin{align}
  f_+(x) - f_+(x^*_+)
  \ge &  \dfrac{\epsilon}{2} (x - 1)^2 \ge  \dfrac{\epsilon}{2}  \indic{x  < 0}. \label{eq:fv-lb}
\end{align}
Similarly,   $f_-(x) - f_-(x^*_-) \ge  \dfrac{\epsilon}{2}  \indic{x  >0}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

The minimax error \eqref{eq:minimax-err} can be lower bounded as follows\footnote{$f_{+1} \equiv f_+$ and $f_{-1}\equiv f_-$.}:
\begin{align}
 &\Delta_n(\F, \sigma^2)\nonumber\\
  \ge & \inf_{\A} \sup_{v \in \{+1,-1\}} \sup_{\gamma \in \Gamma(f_v)} \E[f_V(\hat x_n) - \inf_{x \in X}
  f_V(x)],\label{eq:avg-bd}
  \end{align}
where the expectation is w.r.t. the distribution $\P:= \dfrac{1}{2} \left(P_{\A, \gamma_+, \sigma^2} \indic{v=+1} + P_{\A, \gamma_-, \sigma^2}\indic{v=-1}\right)$. Here $\gamma_v$  is the oracle for $f_v$, for $v=+1,-1$ and is defined by $\gamma_v(x) = \epsilon(x-v) + v C_1 \delta^p$. 
Using \eqref{eq:fv-lb}, we obtain
\begin{align}
\Delta_n(\F, \sigma^2)  \ge & \inf_{\A} \dfrac{\epsilon}{2}  \P(\hat x V < 0), \label{eq:strong-convex-bd}\\
  = & \inf_{\A} \dfrac{\epsilon }{2}  \left(\P_{+}(\hat x_n < 0) + \P_{-}(\hat x_n > 0)\right), \label{eq:Pplus}\\
  \ge &\inf_{\A} \dfrac{\epsilon }{2} \left(1 - \tvnorm{\P_{+}- \P_{-}}\right), \label{eq:lecam}\\
  \ge &\inf_{\A} \dfrac{\epsilon }{2}  \left( 1 - \left(\dkl{P_{+}}{P_{-}}\right)^{\frac{1}{2}}\right), \label{eq:pinsker}
\end{align}
where 
the equality in \eqref{eq:Pplus} uses the definitions $\P_{+}(\cdot) := \P(\cdot\mid V=1)$, $\P_{-}(\cdot) := \P(\cdot\mid V=-1)$. Further, the inequality in \eqref{eq:lecam} follows from the definition of total variation distance, while \eqref{eq:pinsker} follows from Pinksker's inequality.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

% \paragraph{Upper-bounding the difference in gradient estimates:}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Let $P_{+}^t(\cdot\mid g_1,\ldots,g_{t-1})$ denote the distribution of the $t$th observation $G_t$ conditional on $V=+1$ and $G_1=g_1,\ldots,G_{t-1}=g_{t-1}$. Define  $P_{-j}^t(\cdot\mid g_1,\ldots,g_{t-1})$ in a similar fashion.
Then, by the chain rule for KL-divergences, we have
\begin{scriptsize}
\begin{align}
&\dkl{P_{+}}{P_{-}}\label{eq:dklchain}\\ 
&= \sum_{t=1}^n \int_{\R^{t-1}} \dkl{P_{+}^t(\cdot\mid g_{1:t-1})}{P_{-}^t(\cdot\mid g_{1:t-1})} d P_{+}^t(\cdot\mid g_{1:t-1}).\nonumber
\end{align}
\end{scriptsize}
Since the noise $\xi \sim \normal(0,\sigma^2)$, it is easy to see that $P_{+}^t(\cdot\mid g_{1:t-1}) \sim \normal(\gamma_{+}(A(g_{1:t-1})),\sigma^2)$, where $A(g_{1:t-1})$ denotes the point chosen by the algorithm given observations $g_1,\ldots, g_{t-1}$ and $\gamma_{+}$ is the gradient oracle defined earlier. 
Observe that, for any $x\in \R$, $
 |f'_+(x) - f'_-(x)| \le 2 \epsilon.$ This implies 
 \begin{align}
 |\gamma_+(x) - \gamma_-(x)| \le 2(\epsilon-C_1\delta^p).  \label{eq:gdiff-ub}
 \end{align}
From the foregoing, 
\begin{align}
 &\dkl{P_{+}^t(\cdot\mid g_{1:t-1})}{P_{-}^t(\cdot\mid g_{1:t-1})} d P_{+}^t(\cdot\mid g_{1:t-1}) \nonumber\\
 \le&\dfrac{(g_{+}(A(g_{1:t-1})) - g_{-}(A(g_{1:t-1})))^2}{2 \sigma^2}\label{eq:dkgauss1}\\
 \le& \dfrac{4(\epsilon-C_1\delta^p)^2\delta^q}{C_2}.\label{eq:dkgauss}
\end{align}
The inequality \eqref{eq:dkgauss1} follows from the fact that the KL-divergence between normal distributions $\normal(\mu_1,\sigma^2)$ and $\normal(\mu_2,\sigma^2)$ is upper-bounded by $\dfrac{(\mu_1 - \mu_2)^2}{2 \sigma^2}$, while the inequality \eqref{eq:dkgauss} follows from \eqref{eq:gdiff-ub} and the variance constraint that governs the observations output by the oracles $\gamma_+$ and $\gamma_-$.

Plugging \eqref{eq:dkgauss} into \eqref{eq:dklchain}, we obtain
\begin{align}
\dkl{P_{+}}{P_{-}} \le \dfrac{4n(\epsilon-C_1\delta^p)^2 \delta^q}{C_2}.\label{eq:dkbd}
\end{align}
Note that the above bound holds uniformly over all algorithms $\A$. 
Substituting the above bound into \eqref{eq:pinsker}, we obtain
\begin{align}
 \Delta_n(\F, \sigma^2) 
  \ge & \dfrac{\epsilon}{2} \left(1 - 2\sqrt{
    n}  \dfrac{(\epsilon-C_1\delta^p)\delta^{q/2}}{C_2}
  \right)\label{eq:final-lower-bd}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Derivations of rates:}
 Optimizing over $\epsilon$ in \eqref{eq:final-lower-bd}, we get
 $$\epsilon^* = \left(\dfrac{\delta^{-q/2}C_2}{4\sqrt{n}} + \dfrac{C_1\delta^p}{2}\right).$$

 Plugging in $\epsilon^*$, we obtain
 $$\Delta_n(\F, \sigma^2) 
 \ge \left(\dfrac{\delta^{-q/2}C_2}{8\sqrt{n}} + \dfrac{C_1\delta^p}{4}\right) \left( \dfrac{1}{2} + \dfrac{\sqrt{n}C_1 \delta^{p + q/2}}{C_2} \right).$$

Substituting $p=1$ and $q=2$  and optimizing over $\delta$, we obtain
$$\Delta_n(\F, \sigma^2) \ge \dfrac{C_3}{ n^{1/4}} \text{ for some } C_3>0.$$

On the other hand, substituting $p=q=2$, we obtain
$$\Delta_n(\F, \sigma^2) \ge \dfrac{C_4}{ n^{1/3}} \text{ for some } C_4>0.$$
