%!TEX root =  bgo.tex
% please do not delete or change the first line (needed by Csaba's editor)
\subsection{Proofs for the Lower Bound}
\label{sec:lb-proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

For brevity, let $\Delta_n^*$ denote $\Delta_n^*(\F, c_1,c_2)$.
The proof uses only $(c_1,c_2)$ Type-I oracles $\gamma$ that map $\cK$ to $\R$ (i.e., the oracles do not have memory).
Fix a  $(c_1,c_2)$ Type-I oracle $\gamma$ and an algorithm $\A$.
We restrict the class of oracles to those that return a random gradient estimate $G = m(x) + \xi$ with some map $m: \cK \to \R$,
where $\xi$ is standard normal with variance $\sigma^2 = C_2 \delta^q$, satisfying the variance requirement. 
The map $m$, which, by slightly abusing notation, we will also denote by $\gamma$ in what follows, will be chosen based on $f$ to satisfy the requirement on the bias. The $Y$ value returned by the oracles is made equal to $x$.

Let $\delta$ denote the tolerance parameter chosen by $\A$.
Define the probability space $(\Omega, \B, P_{\A,\gamma})$, 
where $\Omega = \R^n\times \{-1,1\}$, $\B$ is the associated Borel sigma algebra. 
Further, the probability measure $P_{\A,\gamma} := p_{\A,\gamma} d(\lambda \times m)$, 
	$\lambda$ is the Lebesgue measure on $\B$, 
	$m$ is the counting measure on $\{-1,1\}$ and 
	$p_{\A,\gamma}$ is the density function defined as
\begin{align*}
&p_{\A,\gamma}(g_{1:n}, v) = \dfrac{1}{2} \bigg(p_{\A,\gamma}(g_n \mid g_{1:n-1})\times \dots \\
&\times p_{\A,\gamma }(g_{n-1} \mid g_{1:n-2}) \ldots p_{\A,\gamma}(g_1)\bigg)\\
 = & \dfrac{1}{2} \bigg( p_{\N}(g_n - \gamma(\A_n(g_{1:n-1}))) \cdot
%  &\times p_{\normal(0,\sigma^2)}(g_{n-1} - \gamma(\A_n(g_{1:n-2}))) \times \ldots \\
 \ldots \cdot  p_{\N}(g_1 - \gamma(\A_1))\bigg),
\end{align*}
where $p_{\N}$ is the density of a $\normal(0,\sigma^2)$ random variable,
and $\cA_t$ denotes the map from the algorithm's past observations
that picks the point that is sent to the oracle in round $t$.
\todoc{Enough to consider deterministic algorithms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

By assumption,  $\{f_+, f_-\}\subset \F$, with 
\begin{align*}
  f_+(x) := \dfrac{\epsilon}{2} (x - 1)^2 \text{ and } f_-(x) := \dfrac{\epsilon}{2} (x + 1)^2, \,\, x \in \cK
\end{align*}
(we will slightly abuse notation by using $f_+$ ($f_-$) in place of $f_{+1}$ (resp., $f_{-1}$).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Clearly, $f_+$ (resp, $f_-$) is minimized at $x^*_+ = 1$ (resp. $x^*_- = -1$) with the minimum value being zero.
Hence, \todoc{No need for strong convexity..}
%Using the fact that $f_+$ and $f_-$ are strongly convex with associated constant $\left(\dfrac{\epsilon}{2}\right)$, we obtain
\begin{align}
  f_+(x) - f_+(x^*_+)
  = &  \dfrac{\epsilon}{2} (x - 1)^2 \ge  \dfrac{\epsilon}{2}  \indic{x  < 0}. \label{eq:fv-lb}
\end{align}
Similarly,   $f_-(x) - f_-(x^*_-) \ge  \dfrac{\epsilon}{2}  \indic{x  >0}$.
We will consider the oracles $\gamma_v$ defined using 
by $\gamma_v(x) = \epsilon(x-v) + v\, C_1 \delta^p$ (as with $f_v$, we will also use $\gamma_{+}$ ($\gamma_-$) 
to denote $\gamma_{+1}$ (resp., $\gamma_{-1}$).
The oracle is indeed a $(c_1,c_2)$ Type-I oracle.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

The minimax error \eqref{eq:minimax-err} is lower bounded by
%\footnote{$f_{+1} \equiv f_+$ and $f_{-1}\equiv f_-$.}:
\begin{align}
\MoveEqLeft 
\Delta_n^* %\nonumber\\
  \ge  \inf_{\A} \,  \E[f_V(\hat X_n) - \inf_{x \in X}
  f_V(x)],\label{eq:avg-bd}
  \end{align}
where the expectation is w.r.t. the distribution $\P:= \dfrac{1}{2} \left(P_{\A, \gamma_+} \indic{v=+1} + P_{\A, \gamma_-}\indic{v=-1}\right)$ and $V: \Omega \to \{\pm 1 \}$ is defined by $V(g_{1:n},v) = v$.%
\footnote{Here, we are slightly abusing the notation as $\P$ depends on $\A$, but the dependence is suppressed.
In what follows, we will define several other distributions derived from $\P$, which will all depend on $\A$, but
for brevity this dependence will be also suppressed.}
%Here $\gamma_v$  is the oracle for $f_v$, for $v=+1,-1$ and is defined by $\gamma_v(x) = \epsilon(x-v) + v C_1 \delta^p$. 
Using \eqref{eq:fv-lb}, we obtain
\begin{align}
\Delta_n^*  \ge & \inf_{\A} \dfrac{\epsilon}{2}\,  \P(\hat X_n V < 0), \label{eq:strong-convex-bd}\\
  = & \inf_{\A} \dfrac{\epsilon }{2} \, \left(\P_{+}(\hat X_n < 0) + \P_{-}(\hat X_n > 0)\right), \label{eq:Pplus}\\
  \ge &\inf_{\A} \dfrac{\epsilon }{2} \,\left(1 - \tvnorm{\P_{+}- \P_{-}}\right), \label{eq:lecam}\\
  \ge &\inf_{\A} \dfrac{\epsilon }{2}  \,\left( 1 - \left(\dkl{P_{+}}{P_{-}}\right)^{\frac{1}{2}}\right), \label{eq:pinsker}
\end{align}
where 
the equality in \eqref{eq:Pplus} uses the definitions $\P_{+}(\cdot) := \P(\cdot\mid V=1)$, $\P_{-}(\cdot) := \P(\cdot\mid V=-1)$. Further, the inequality in \eqref{eq:lecam} follows from the definition of total variation distance, while \eqref{eq:pinsker} follows from Pinsker's inequality. \todoc{You lost a factor of two here!}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

% \paragraph{Upper-bounding the difference in gradient estimates:}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
Define $G_t$ to be the $t$th observation of $\A$: $G_t:\Omega \to \R$, with $G_t( g_{1:n}, v) = g_t$.
Let $P_+^t(g_1,\dots,g_t)$ denote the joint distribution of $G_1,\dots,G_t$ conditioned on $V=+1$.
Let $P_{+}^t(\cdot\mid g_1,\ldots,g_{t-1})$ denote the distribution of $G_t$ conditional on $V=+1$ and $G_1=g_1,\ldots,G_{t-1}=g_{t-1}$. Define  $P_{-j}^t(\cdot\mid g_1,\ldots,g_{t-1})$ in a similar fashion.
Then, by the chain rule for KL-divergences, we have
\begin{scriptsize}
\begin{align}
&\dkl{P_{+}}{P_{-}}\label{eq:dklchain}\\ 
&= \sum_{t=1}^n \int_{\R^{t-1}} \dkl{P_{+}^t(\cdot\mid g_{1:t-1})}{P_{-}^t(\cdot\mid g_{1:t-1})} d P_{+}^t( g_{1:t-1}).\nonumber
\end{align}
\end{scriptsize}
By the oracle's definition, $G_t \sim  \normal(\gamma_{+}(\cA_t(G_{1:t-1})),\sigma^2)$. Hence, 
$P_{+}^t(\cdot\mid g_{1:t-1})$ is the normal distribution with the mean $\cA_t(g_{1:t-1})$ and variance $\sigma^2$.
%, where $A(g_{1:t-1})$ denotes the point chosen by the algorithm given observations $g_1,\ldots, g_{t-1}$ and $\gamma_{+}$ is the gradient oracle defined earlier. 
Observe that, for any $x\in \R$, $f_+'(x) - f_-'(x) = 2\epsilon$ and hence
\begin{align}
 |\gamma_+(x) - \gamma_-(x)| 
& = | f'_+(x) + C_1 \delta^p - (f'_-(x)-C_1 \delta^p | \nonumber \\
& = 2| \epsilon - C_1 \delta^p |\,.
 \label{eq:gdiff-ub}
\end{align}
From the foregoing, 
\begin{align}
 \MoveEqLeft \dkl{P_{+}^t(\cdot\mid g_{1:t-1})}{P_{-}^t(\cdot\mid g_{1:t-1})}\nonumber\\
 =&\dfrac{(g_{+}(A(g_{1:t-1})) - g_{-}(A(g_{1:t-1})))^2}{2 \sigma^2}\label{eq:dkgauss1}\\
 =& \dfrac{4(\epsilon-C_1\delta^p)^2\delta^q}{C_2}.\label{eq:dkgauss}
\end{align}
The equality \eqref{eq:dkgauss1} follows from the fact that the KL-divergence between normal distributions $\normal(\mu_1,\sigma^2)$ and $\normal(\mu_2,\sigma^2)$ is equal to $\dfrac{(\mu_1 - \mu_2)^2}{2 \sigma^2}$, while the equality \eqref{eq:dkgauss} follows from \eqref{eq:gdiff-ub} and the choice $\sigma^2 = C_2 \delta^{-q}$.

Plugging \eqref{eq:dkgauss} into \eqref{eq:dklchain}, we obtain
\begin{align}
\dkl{P_{+}}{P_{-}} \le \dfrac{4n(\epsilon-C_1\delta^p)^2 \delta^q}{C_2}.\label{eq:dkbd}
\end{align}
Note that the above bound holds uniformly over all algorithms $\A$. 
Substituting the above bound into \eqref{eq:pinsker}, we obtain 
\todoc[inline]{This seems to assume that $\sqrt{ (\epsilon - C_1 \delta^p )^2 } = \epsilon - C_1 \delta^p$, which will only hold if $\epsilon$ is big compared to $C_1 \delta^p$. We will need to do a case analysis.}
\begin{align}
 \Delta_n^*
  \ge & \dfrac{\epsilon}{2} \left(1 - 2\sqrt{
    n}  \dfrac{|\epsilon-C_1\delta^p|\delta^{q/2}}{C_2}
  \right)\label{eq:final-lower-bd}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Derivations of rates:}
 Optimizing over $\epsilon$ in \eqref{eq:final-lower-bd}, 
\todoc{As said, this only works for $\epsilon$ large enough.} we get
\[
\epsilon^* = \left(\dfrac{\delta^{-q/2}C_2}{4\sqrt{n}} + \dfrac{C_1\delta^p}{2}\right).
\]

 Plugging in $\epsilon^*$, we obtain
 \[
 \Delta_n^*
 \ge \left(\dfrac{\delta^{-q/2}C_2}{8\sqrt{n}} + \dfrac{C_1\delta^p}{4}\right) \left( \dfrac{1}{2} + \dfrac{\sqrt{n}C_1 \delta^{p + q/2}}{C_2} \right).
 \]

Substituting $p=1$ and $q=2$  and optimizing over $\delta$, we obtain
\[
\Delta_n^* \ge \dfrac{C_3}{ n^{1/4}} \text{ for some } C_3>0.
\]

On the other hand, substituting $p=q=2$, we obtain
\[
\Delta_n^* \ge \dfrac{C_4}{ n^{1/3}} \text{ for some } C_4>0.
\]
