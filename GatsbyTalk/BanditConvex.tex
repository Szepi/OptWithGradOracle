%\documentclass[handout,12pt]{beamer}
\documentclass[12pt]{beamer}


%\usepackage{palatino}
%\usepackage[mathcal]{euscript}
%\usepackage[mathbf,mathcal]{euler}
%\usepackage{mathbbol}
\usepackage{multicol}
\usepackage[disable]{todonotes}

%\usefonttheme{serif}
%\usefonttheme[onlymath]{serif}

\input{BeamerCommands}
\input{Commands}

% tikz includes
\usepackage{tikz}
\usetikzlibrary{positioning,arrows,shapes,calc,patterns,snakes}
% For every picture that defines or uses external nodes, you'll have to
% apply the 'remember picture' style. To avoid some typing, we'll apply
% the style to all pictures.
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{na} = [baseline=-.5ex]
\everymath{\displaystyle}
% end of tikz includes %%%%%%

\title{(Bandit) Convex Optimization with\\
 Biased Noisy Gradient Oracles}
\institute{
Reinforcement Learning and Artificial Intelligence (RLAI) Group\\
 Department of Computing Science \& AICML\\ 
 University of Alberta \\ 
 \href{mailto:csaba.szepesvari@ualberta.ca}{\texttt{csaba.szepesvari@ualberta.ca}}
% \href{mailto:jneufeld@ualberta.ca}{\texttt{jneufeld@ualberta.ca}}
\\ 
\vspace{0.5cm}
%\includegraphics[scale=0.9]{google-logo} 
%\hspace{2cm} \vspace{0.5cm} 
\includegraphics[scale=0.3]{figs/u-of-alberta-logo}
 }
%\author{James Neufeld}
\author{Csaba Szepesv\'ari\\
Joint work with
Xiaowei Hu, L.A. Prashanth, and Andr\'as Gy\"orgy
}
%\date{February 13, 2014}
%\date{May 23, 2014}
%\date{June 7, 2014}
%\date{June 22, 2014}
%\date{April 2, 2015}
\date{November 10, 2015}

\usepackage{natbib}
%\bibliographystyle{apalike}
\bibliographystyle{apalike}
%\bibliographystyle{alpha}

\DeclareMathOperator{\supp}{supp}
% number sets
\newcommand{\N}{{\mathbb N}}
%\newcommand{\R}{{\mathbb R}}
%\newcommand{\real}{{\mathbb R}}

% spaces, vectors, matrices, distributions
%\newcommand{\eps}{\varepsilon}
\renewcommand{\H}{{\mathcal H}}
%\newcommand{\X}{\mathbf{X}}
%\newcommand{\Y}{\mathbf{Y}}
%\newcommand{\E}{{\mathcal E}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\Z}{{\mathcal Z}}
\newcommand{\w}{{\mathbf w}}
\newcommand{\x}{{\mathbf x}}
\newcommand{\z}{{\mathbf z}}
\newcommand{\h}{{\mathbf h}}
\renewcommand{\P}{{\mathbf P}}
\renewcommand{\c}{{\mathbf c}}
\newcommand{\M}{{\mathbf M}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\D}{{\mathcal D}}
%\newcommand*{\newblock}{}
%\newcommand{\ip}[1]{\langle #1\rangle}
%\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\tnorm}[1]{\|#1\|}
\newcommand{\A}{\mathcal{A}}
%\newcommand{\one}[1]{\mathbb{I}_{\{#1\}}}
%\newcommand{\EE}[1]{\mathbb{E}[#1]}
%\newcommand{\EEgrow}[1]{\mathbb{E}\left[#1\right]}

% operators
%\DeclareMathOperator*{\Exp}{\mathbf{E}}   % expectation
%\DeclareMathOperator*{\Prob}{\mathbf{Pr}}   % expectation
%\DeclareMathOperator*{\Var}{\mathbf{Var}}   % expectation
\DeclareMathOperator*{\Err}{Err}   
%\DeclareMathOperator*{\argmin}{argmin}   
\DeclareMathOperator*{\VC}{VC-dim}   
\DeclareMathOperator*{\Ldim}{L-dim}   
\DeclareMathOperator{\Rademacher}{\mathcal{R}}   
\DeclareMathOperator{\SRademacher}{\mathcal{SR}}   
\DeclareMathOperator*{\sign}{sign}   
\DeclareMathOperator*{\indicator}{\mathbb{1}}   
\DeclareMathOperator*{\minimize}{minimize}   
\DeclareMathOperator*{\conv}{conv}   
%\DeclareMathOperator{\Regret}{Regret}   
\DeclareMathOperator{\rowspace}{rowspace}   
%\DeclareMathOperator*{\argmax}{argmax}   
\DeclareMathOperator*{\polylog}{polylog}

\DeclareMathOperator*{\rows}{\#rows}   
\DeclareMathOperator*{\shattered}{\#shattered}   

\newtheorem{conjecture}{Conjecture}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\maketitle
	\vspace*{-0.2in}
	\begin{center}
          Gatsby Seminar 
	\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0
\begin{frame}
\frametitle{Summary}

    \bi
    \item \alert{Why} should we care? 
    \item \alert{What} are the unique challenges? 
    \item \alert{How} do we address them?
    \ei
    \bigskip
    
\end{frame}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Introduction}
\begin{frame}{Outline}
\tableofcontents %[currentsection]
\end{frame}


\frame{
	\frametitle{Expectation Management}
	\bi
	\item \Large
	80\% review (long history!)
	\bigskip
	\item \Large
	20\% new
	\ei
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convex Bandit Optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Bandit problems:
% Get value of an option
% 
% Robbins
% Bandit linear optimization
% Bandit nonlinear optimization
% Bandit convex optimization

\frame{
	\frametitle{Convex Optimization with Noisy Bandit Feedback}
	\bcol[c]
	\col[0.5\textwidth]
	\bc
	\includegraphics[width=\textwidth]{figs/Interaction}
	\ec
	\col[0.45\textwidth]
%	\begin{block}<+->{Goal}
	\alert{Goal}\\
	Assume $f$ convex (smooth etc).\\
	Find a near-minimizer of $f$ using $n>0$ queries!
%	\end{block}
	\bigskip
	
	\uncover<+->{
		Noisy Bandit Feedback $\equiv$ Noisy zeroth-order information\\
	}

	\ecol
	\bigskip
%	Why ``bandit''? ``Bandit theory'' \\
	% Why bandit?
	\begin{block}<+->{Main Question}
	\bc
		How fast can/will/should the optimization error
		$
		\Delta_n = \EE{f(X_n) }- \inf_{x\in \cK} f(x)
		$
		 decrease with $n$?
	\ec		 
	\end{block}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Example: Resource Allocation}
	\bcol[c]
	\col[0.45\textwidth]
	\bc
	\includegraphics[width=\textwidth]{figs/memory}
	\ec
	\col[0.55\textwidth]
	\bi
	\item Distribute memory in between jobs
	\item Maximize success: $\max_{x\in \cK} f(x)$
	\item Linear constraints: $\cK = \cset{ x\in [0,\infty)^d}{\sum_i x_i = 1}$
	\item Concave objective $f$: 
	Convex combination of resources not worse than what randomization gives.
	\ei
	\ecol
	\uncover<+->{The only way to learn about $f$ is by trying different configurations.}
	\uncover<+->{Feedback: $f(x) + \mathrm{noise}$ for $x$ tried. ``Bandit'' feedback.}\\
	\uncover<+->{Goal is to find the best configuration quickly.}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Why Bandit Feedback?}
	\setlength\itemsep{1em}
	\bi
	\item No other choice
		\bi
		\setlength\itemsep{0.5em}
		\item Controlling an unknown system
		\item Simulation optimization: 
			\bi
			\setlength\itemsep{0.5em}
			\item
				$f(x) = \EE{ F(x,\xi) }$ --- $\xi$: simulation noise
			\item $F(x,\xi)$ is the output of a simulation;
			\item Gradients are unavailable
			\ei
		\ei
	\ei
	\bigskip
	\bi
			\setlength\itemsep{0.5em}
	\item By choice: 
		\bi
			\setlength\itemsep{0.5em}
		\item Gradient is too expensive/complicated to compute
	  \item (Can this be justified?)  %%%%%%%%GYA: ???????
		\ei
	\ei
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Subclasses of Convex Problems}
	\bcol[c]
	\col
	\bc
	\uncover<+->{
	\begin{minipage}[t]{0.9\textwidth}
		Deviation from linearity, or\\
		``curviness'':
		\begin{align*}
		& D_f(w||v)\doteq\\
		& f(w)- \left\{ f(v) + \ip{\nabla f(v),w-v} \right\}.
		\end{align*}
	\end{minipage}}
	\ec
	\col
	\includegraphics[width=\textwidth]{figs/entropy-16-06338f2-1024}
	\ecol
	
	\bcol[t]
	\col[0.6\textwidth]
	\bi
	\item
	Smoothness: 
	$\qquad
	D_f(x||y) \le \frac{L}{2} \norm{x-y}^2\,.
	$
	\item
	Strong convexity:
	$\qquad
	D_f(x||y) \ge \frac{\mu}{2} \norm{x-y}^2\,.
	$
	\ei
	\col[0.4\textwidth]
	\figincol{
	\includegraphics[width=\textwidth]<4->{figs/Problems}}
	\ecol

	\putatUR{40mm}{126mm}{-1mm}{
	\begin{block}<5->{}
		$\cK \subset \R^d$ convex, closed, non-empty.
	\end{block}
	}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Convex Optimization 101: Ellipsoid Method}
	\begin{block}{Assumption} First-order information,
	$(\nabla f(\cdot), f(\cdot))$, can be obtained at any point.
	\end{block}
	
	\bcol[b]
	\col[0.65\textwidth]
	\begin{block}<2->{Ellipsoid Method \citep{Shor70}}
	\bi
	\item<3->   %%%%%%GYA: Delta nincs definialva
	$\Delta_n^{\mathcal{E},d} = C\exp(-\frac{n}{d^2})$ {\footnotesize \citep{NeYu83}}	
	\item<4->
	Matching lower bound when $d$ small and $n$ large.\\
	\uncover<5->{$\ldots$ but}
			\item<6-> Each update takes $O(d^2)$ MADDs (prohibitive when $d$ large).
			\item<7-> Suboptimal for $d/n^2 \gg 1$:
                          %%%GYA: d/n^2 javitva
			 $\lim_{d\to\infty} \Delta_n^{\mathcal{E},d} = \Omega(1)$, while
			  $\Delta_n^* = \Theta(1/\sqrt{n})$.
	\ei
	\end{block}
	\col[0.35\textwidth]
	\includegraphics<2->[width=\textwidth]{figs/Ellipsoid_2}
	\ecol
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Convex Optimization 101: Gradient Methods}

\centerline{
	        \tikz[baseline]{
            \node[fill=blue!20,anchor=base] (t1)
            {Think: ``$d$ large''. Update complexity is $O(d)$.};}	}


{\color{darkgreen}Subgradient method: Optimal for convex problems:}
\begin{align*}
	        \tikz[baseline]{
            \node[fill=red!20,anchor=base] (t1)
            {$\Delta_n = O(\sqrt{1/n})$.};}	
\end{align*}	
	\centerline{ No dependence on $d$!}  %%%GYA: tenyleg?

\vspace{1ex}
	
{\color{darkgreen}Gradient method  + momentum term}\\ 
	$\equiv$ \alert{accelerated} gradient method
	\citep{nesterov2004introductory}.
	\todoc[inline]{Requires knowledge of $L$, $\mu$?}
	\bi
	\item Optimal for \alert{smooth convex problems}, $\cF_{L,0}$: 
	\begin{align*}
		        \tikz[baseline]{
            \node[fill=brown!20,anchor=base] (t1)
            {$\Delta_n = O(L/n^2)$.};}	
						\end{align*}
	\item Optimal for \alert{strongly convex, smooth problems}, $\cF_{L,\mu}$:
	\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$\Delta_n = \exp(- \tfrac{n}{ \sqrt{L/\mu} } )$.};}	
						 %%%GYA: O() missing?
						\end{align*}
						\ei
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The State-of-the-Art for Noisy Bandit Convex Optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Results under Noisy Bandit Feedback}
	Recall: $f(x) = \EE{ F(x,\xi) }$.

	\bigskip
	\bigskip

	\begin{block}{Assumptions}
	\medskip
	\bcol
	\col[0.015\textwidth]
	\mbox{}
	\col[0.995\textwidth]
	\bi
	\item[(A1)] $F(x,\xi)$ can be obtained at any point. ``Uncontrolled noise''
	\item[(A2)] $\xi$  can be kept fixed between queries. ``Controlled noise''
	\ei
	\ecol
	\bigskip
	\end{block}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Controlled Noise -- Smooth Case}

	\bcol
	\col[0.1\textwidth]
	\col[0.9\textwidth]
	\uncover<+->{Simple story!}
	\bigskip
	\bigskip

	\uncover<+->{Gradient method}
	\bigskip
	\bigskip
	
	\uncover<+->{$\Delta_n \le C \sqrt{d^2/n}$.}

	\bigskip
	\bigskip
	\uncover<+->{\citet{Ne11:TR}	,
	\citet{duchi2015optimal} }

	
	\bigskip
	\bigskip
	
	\uncover<+->{Optimal!}
	\ecol
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Uncontrolled Noise: Big Gaps}
			
%In their seminal work \citet{NeYu83} consider two approaches to this problem: Methods that try to construct gradient information and methods that avoid gradient estimation and rather use  geometric principles (the ellipsoid method, essentially). While methods in the second class make the error decay at the $O(1/\sqrt{n})$ rate,  the error scales extremely poorly with the number of optimization variables $d$.

\uncover<+->{Ellipsoid method and relatives {(\footnotesize \citet{NeYu83}, Section 9.4)}}
\bi
\item  \citet{AgFoHsuKaRa13:SIAM} --  $\sqrt{\alert{d^{33}}/n}$
\item \citet{liang2014zeroth} -- $\sqrt{\alert{d^{14}}/n}$
\ei 

\bigskip
 
\uncover<+->{Gradient methods: {(\footnotesize \citet{NeYu83}, Section 9.3)}}
\bi
\item
	Convex: -- $\qquad \alert{(d^2/n)^{1/4}}$\\
	{\footnotesize  \citep{NeYu83,flaxman2005online}}
\item
	Smooth:  -- $\qquad \alert{ (d^2/n)^{1/3}}$ \\
	{\footnotesize \citep{NeYu83,saha2011improved}}
\item
	Smooth + SOC:  $\qquad \sqrt{d^2/n}$\\
	{\footnotesize \citep{HaLe14:SOC} }
\ei

\bigskip
\uncover<+->{
Lower bound: $\sqrt{d^2/n}$  \citep{shamir2012complexity}.
}

\putatBR{40mm}{124mm}{92mm}{
\begin{block}<+->{}
\uncover<.->{Can we do better?}
\uncover<+->{$\ldots$ using a ``clever'' gradient method maybe?}
\end{block}
}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{How are Gradients Estimated?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Gradient Estimation -- Noise-free Feedback}
	
	Finite-differences {\footnotesize \citep{KiWo52}}:
		\begin{align*}
			        \tikz[baseline]{
            \node[fill=red!20,anchor=base] (t1)
            {$g_i = \frac{1}{\delta} \left( f(x+\delta e_i) - f(x) \right)\,,\quad i=1,\dots,d\,.$};}	
						\end{align*}		
	
	\bigskip
	
	Taylor-series expansion:
	\begin{align*}
	f(x+ \delta e_i)  &= f(x) + \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).
	%f(x\alert{-} \delta e_i)  &= f(x) \alert{-} \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).
	\end{align*}

	\bigskip
	
		\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {Accuracy: $\norm{ g - \nabla f(x) }_2 = O( \sqrt{d}\, \delta )$.};}	
						\end{align*}		
	
%					$\norm{ g - \nabla f(x) }_\infty = O(  \delta )$.
	
	\bigskip
	\bigskip
	
\onslide<2->{\color{violet} Needs $d+1$ queries.} \tikz[na]\node [coordinate] (n1) {};	

        \begin{tikzpicture}[overlay]
        \path[->]<2-> (n1) edge [bend right] ($(t1)+(2,-0.4)$);
			\end{tikzpicture} 
	
	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\frame{
	\frametitle{Two-sided Differences}	

	Improved estimate: 
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$g_i = \frac{1}{2\delta} \left( f(x+\delta e_i) - f(x-\delta e_i) \right),\quad i=1,\dots,d.$};}	
						\end{align*}		

	Taylor-series expansions:
	\begin{align*}
	f(x+ \delta e_i)  &= f(x) + \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).\\
	f(x\alert{-} \delta e_i)  &= f(x) \alert{-} \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).
	\end{align*}

	\bigskip

		\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {Accuracy: $\norm{ g - \nabla f(x) }_2 = O(\sqrt{d} \delta^2 )$.};}	
						\end{align*}		
	
%					$\norm{ g - \nabla f(x) }_\infty = O(  \delta )$.
	
	\bigskip
	\bigskip
	
\onslide<2->{\color{violet} Needs $2d$ queries.} \tikz[na]\node [coordinate] (n1) {};	

        \begin{tikzpicture}[overlay]
        \path[->]<2-> (n1) edge [bend right] ($(t1)+(2,-0.4)$);
			\end{tikzpicture} 
		
	\todoc[inline]{Noisy feedback case}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Two-sided Differences -- Noisy Feedback}	
\begin{small}
	\uncover<+->{
	
	Improved estimate: 
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=magenta!20,anchor=base] (t1)
            {$G_i = \frac{1}{2\delta} \left\{ f(x+\delta e_i) + \xi^+_i- (f(x-\delta e_i)+\xi^-_i) \right\},\quad i=1,\dots,d.$};}	
						\end{align*}		
}

%	Taylor-series expansions:
%	\begin{align*}
%	f(x+ \delta e_i)  &= f(x) + \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).\\
%	f(x\alert{-} \delta e_i)  &= f(x) \alert{-} \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).
%	\end{align*}
	
	\uncover<+->{ 
	Assumption: $\EE{ \xi^{\pm} } = 0$, $\EE{ (\xi^{\pm}) } \le \sigma^2 <+\infty$.}
		
	\uncover<+->{ $\EE{ G_i } = g_i$.}
	\uncover<+->{  Hence
				\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$\norm{ \EE{G} - \nabla f(x) }_2 = O(\sqrt{d} \delta^2 )\,.$  $\longleftarrow$ \textbf{bias}};}	
						\end{align*}}		
	
	\uncover<+->{ \alert{Second moment}: $\EE{ \norm{G}_2^2 } = ?$}
	
	\uncover<+->{ $G_i = g_i + \frac{\xi^+_i-\xi^-_i}{2\delta}$}
	\uncover<+->{, hence
							$\EE{G_i^2} = g_i^2 + \frac{2\sigma^2}{4 \delta^2} = g_i^2 + \frac{\sigma^2}{2\delta^2}$
	} 
	\uncover<+->{ and
					\begin{align*}
			        \tikz[baseline]{
            \node[fill=blue!20,anchor=base] (t1)
            {$	\EE{ \norm{G}_2^2 } = \norm{g}_2^2 + O\left( \frac{d}{\alert{\delta^2}} \right)\,.$};}	
						\end{align*}		
	}
\end{small}	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Randomization (Noise-Free Bandit Feedback)}
	
	\uncover<+->{Can we reduce the number of queries?}
	
	\uncover<+->{Idea: \alert{ Randomize! }
	$I \sim p(\cdot)$ a positive pmf on $\{1,\dots,d\}$.}
	
	\uncover<+->{Choose
	\begin{align*}
	G_i &= \frac{1}{p(i)} \, \frac{f(x+\delta e_I)-f(x-\delta e_I)}{2\delta} \,e_{I,i}\\
	&=
	\begin{cases}
		\frac{1}{p(I)} \, \frac{f(x+\delta e_I)-f(x-\delta e_I)}{2\delta},  & I= i\,;\\
		0, & \text{otherwise}\,.
	\end{cases}
	\end{align*}}
	
	\uncover<+->{
	\alert{Only 2-queries, regardless of $d$!}}
	
	\uncover<+->{$\EE{G_i} = g_i$! }
	\uncover<+->{Hence, $\norm{\EE{ G } - \nabla f(x)}_2 = O( \sqrt{d} \delta^2 )$.}
	
	\bigskip
	\uncover<+->{Second moment?}
%	\alert{``Variance''}? $\Var{G} := \EE{ \norm{ G - \EE{G} }_2^2 }$.  $\quad \Rightarrow \quad$
%	$\Var{G} = \EE{ \norm{G}_2^2 } - \norm{ \EE{G} }_2^2$.\\
%	Second term is controlled as $\delta\to 0$. 
%	How about $\EE{ \norm{G}_2^2 }?$
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Second Moment from Randomization}
	\uncover<+->{
	\begin{align*}
	G_i &= \frac{1}{p(i)} \, \frac{f(x+\delta e_I)-f(x-\delta e_I)}{2\delta} \,e_{I,i}
	\end{align*}}
	
	\uncover<+->{
	Taylor-series expansion:	
	\begin{align*}
	f(x+ \delta e_i)  &= f(x) + \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3),\\
	f(x- \delta e_i)  &= f(x) - \delta\, \nabla f(x) e_i + \delta^2\, e_i^\top \nabla^2 f(x) e_i +  O(\delta^3).
	\end{align*}	
	}
	\uncover<+->{
	\begin{align*}
	 G_i^2 & = \frac{\one{I=i}}{p^2(i)} 
		\frac{(\delta f'_i(x) + c_3(\delta))^2}{4 \delta^2} 
		= %%GYA
	 \frac{\one{I=i}}{p^2(i)} 
		\frac{(\delta^2 (f_i'(x) )^2+ c_4(\delta))}{ \delta^2} 		\\
		&=
	 \frac{\one{I=i}}{p^2(i)} 
		\left\{ ( f'_i(x) )^2+ O(\delta^2) \right\}
	\end{align*}}
	
	\uncover<+->{
	Hence, $\EE{ G_i^2} = O(1/p(i))$, so at best $\EE{ \norm{G}_2^2 } = O( d^2 )$. 
	Hmm..}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Noisy Bandit Feedback}

	\uncover<+->{
	\begin{align*}
	\tilde{G}_i &= \frac{1}{p(i)} \, \frac{(f(x+\delta e_I)+\xi^+)-(f(x-\delta e_I)+\xi^-)}{2\delta} \,e_{I,i}
	\end{align*}}
	
	\uncover<+->{
	Hence,
	$\tilde{G}_i= \frac{\one{I=i}}{p(i)} \frac{ \xi^+-\xi^- }{2 \delta} + G_i$}
	\uncover<+->{
	and
	\begin{align*}
	\EE{\tilde{G}_i} &= \EE{ G_i}\,,\\
	 \EE{ \tilde{G}_i^2 } &= \frac{1}{p(i)} \frac{ \EE{ (\xi^+-\xi^-)^2} }{ 4 \delta^2 } + \EE{ G_i^2 }\,,
	\end{align*}	}
	\uncover<+->{
	so
	\begin{align*}
	\norm{\EE{ \tilde{G} }_2 - \nabla f(x) }_2 &= O( \alert{\sqrt{d} \delta^2} )\,,\\
	\EE{ \norm{\tilde{G} }_2^2 } & = O( d^2(1 + 1/\alert{\delta^{2}} ) )\,.  \qquad \text{ \alert{harsh!}}
	\end{align*}}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Summary}
	
	\bi
	\item Noise-free observations (or controlled noise):
	\bigskip		
		\bi
		\item Bias:	 $\qquad \qquad \quad O(\delta^2)$
	\bigskip		
		\item Second-moment: $O(1)$
		\ei
	\bigskip		
	\item Noisy observations (a.k.a. uncontrolled noise):
		\bi
	\bigskip		
		\item Bias: $\qquad \qquad \quad O(\delta^2)$
	\bigskip		
		\item Second moment: $O(\delta^{\alert{-2}})$
		\ei
	\ei
	
	\bigskip
	\uncover<+->{
	This assumed $f\in \cC^3$. Holds also for $f$ convex, smooth.}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Other Methods}
	\uncover<+->{General two-point estimate:
	
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$G = \frac{ (f(x+U)+\xi^+) - (f(x-U)+\xi^-)}{2\delta} V\,.$};}	
						\end{align*}		
	}
	\uncover<+->{
	Choose $U,V$ such that $\EE{ V U^\top  } = I$, $\EE{ V } = 0$.}

	\uncover<+->{
	One-point estimate!
			\begin{align*}
			        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {$G = \frac{ (f(x+U)+\xi^+) }{\delta} V\,.$};}	
						\end{align*}		
}
	\uncover<+->{
	Choose $U,V$ such that $\EE{ V U^\top  } = I$, $\EE{ V } = 0$.}
	\uncover<+->{Works??}\\
	\uncover<+->{$\EE{G} = \EE{ G - \frac{f(x)}{\delta} V } = \EE{\frac{ (f(x+U)+\xi^+)-f(x) }{\delta} V}$.}
	
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{``Zoo'' of Gradient Estimation Methods}
	\bi
	\item $U \sim \delta\, \cN(0,I)$, 
	         $V = \delta^{-1}\, U$
	         \bi
	         \item Smoothed functional scheme by \cite{katkul};
	         \item Refined by   \citet{PoTsy90};
	         \item Further studied by \cite{Dip03:AoS,Ne11:TR}.
	         \ei
	\item $U \sim \delta\, \mathrm{Unif}(\mathbb{S}_d)$, 
			 $V =d \delta^{-1}\,  U$
			 \bi
			 \item RDSA by \cite{kushcla};
			 \item Rediscovered by \cite{flaxman2005online}
			 \ei
	\item $U_i \sim \delta\, \mathrm{Rademacher}(\pm 1)$, 
			 $V = \delta^{-1} \,U$
			 \bi
			 \item
			  SPSA by \cite{spall1992multivariate}.
			 \ei
	\item $\dots$			 
	\ei
	
	\uncover<+->{
	\bc
	Does it matter which of these we select? Not really:\\
	Bias: $O(\delta^2)$, second moment: $O(1)$ or $O(\delta^{-2})$.
	\ec
	}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{New Oracle Model: Noisy, Biased Oracles}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Gradient Estimation Oracles}
	
	\bc
	\includegraphics[width=\textwidth]{figs/oracle0}

	\ec	
	

\begin{enumerate}[<+->]
\item Bias: $\norm{ \EE{G}  - \nabla f(x)  }_* \le c_1(\delta) $; and
\item Second moment: $\EE{\norm{ G  }_*^2} \le c_2(\delta)$.
\end{enumerate}


\bc
	\uncover<+->{
Controlled noise: $c_1(\delta) = C_1\delta^2$, $c_2(\delta) = C_2$.}

	\uncover<+->{
Uncontrolled noise:  $c_1(\delta) = C_1\delta^2$, $c_2(\delta) = C_2 \delta^{-2}$.}

\bigskip 
	\uncover<+->{
Polynomial oracle: $c_1(\delta) = C_1 \delta^p$, $c_2(\delta) = C_2 \delta^{-q}$, $p,q>0$.}
%%%GYA: mention what matters is p/q

\ec



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Upper Bound: Algorithm}

\begin{block}{Mirror descent \citep{NeYu83}}
\begin{algorithmic}
    \State {\bf Input:}  Closed convex set $\cK$, regularization function $\mathcal{R}:\mathbb{R}^d\to \mathbb{R}$, tolerance parameter $\delta$, learning rates $\{\eta_t\}_{t=1}^{n-1}$.
%     In round $t=1, 2, \cdots, n-1$:
\State Initialize $X_1\in \cK$ arbitrarily.
\For{$t=1, 2, \cdots, n-1$}	
	\State Query the oracle at $X_t$.
	\State Receive $G_t$.
	\State Update
	$$
	X_{t+1}=\argmin_{x\in \mathcal{K}}\left[ \eta_{t} \ip{G_t,x}+D_{\mathcal{R}}(x,X_t) \right] \,.
	$$
\EndFor
\State {\bf Return:} $\hat{X}_n = \frac{1}{n}\sum_{t=1}^n X_t \,.$
\end{algorithmic}
\end{block}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Upper Bound}

\begin{theorem}[\textit{\textbf{Upper bound}}]
Consider MD with a $(p,q)$-order polynomial oracle, $\alpha$ SOC regularizer $\cR$.
Then:
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{2p+q} } )\\
% \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
\Delta_n(\F_{L,\mu},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{p+q} } )\,.
%\le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
%where $D:=\sup_{x,y\in \cK} \DR(x,y)$, $K_1$ is a constant that depends on oracle parameters $p, q$ and strong convexity constant $\alpha$ of $\cal R$ and where $K_2$ is a constant that depends on $p, q, \alpha$ and $\mu$.
\end{theorem}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
	\frametitle{Is This Any Good?}
\uncover<+->{
Recall:\vspace*{-0.2in}
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{2p+q} } )\\
% \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
\Delta_n(\F_{L,\mu},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{p+q} } )\,.
%\le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
}

\bigskip

\uncover<+->{
Can get an $n^{-1/2}$ rate?} \\
\uncover<+->{
Yes, if $p/(2p+q)\ge 1/2$ vs. $p/(p+q)\ge 1/2$.}\\
\uncover<+->{
First holds iff $q=0$. Second holds iff $p\ge q$.\\
}
}
\frame{
	\frametitle{Is This Any Good?}
\uncover<+->{
Recall:\vspace*{-0.2in}
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{2p+q} } )\\
% \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
\Delta_n(\F_{L,\mu},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{p+q} } )\,.
%\le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
}

\bigskip

\uncover<+->{
\alert{Uncontrolled noise}; under smoothness, $p=q=2$.} \\
\uncover<+->{
\tikz[baseline]{
            \node[fill=green!20,anchor=base] (t1)
            {For $\F_{L,\mu}$ we get $O(\alert{n^{-1/2}})$ as \citet{HaLe14:SOC}.};}	
}\\[1ex]
\uncover<+->{
\tikz[baseline]{
            \node[fill=red!20,anchor=base] (t1)
            {For $\F_{L,0}$ we get $O(n^{-1/3})$ as \citet{saha2011improved}.};}	
}

\bigskip

\uncover<+->{
\alert{Controlled noise}: under smoothness, $p=2$, $q=0$.}\\
\uncover<+->{
\tikz[baseline]{
            \node[fill=blue!20,anchor=base] (t1)
            {For $\F_{L,\mu}$ we get $O(\alert{n^{-1}})$ as 
	\citet{Ne11:TR}.};}	
}\\
\uncover<+->{
\tikz[baseline]{
            \node[fill=brown!20,anchor=base] (t1)
            {For $\F_{L,0}$ we get $O(\alert{n^{-1/2}})$ as	
	\citet{duchi2015optimal}.};}	
}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Could We Have Done Better?}

\uncover<+->{
\begin{theorem}[\textit{\textbf{Lower bound}}]
\label{thm:lb-convex}
$\cK\subset \R^d$ convex, closed, with  $\{+1,-1\}^d\subset \cK$, $n$ large enough.
For any algorithm $\mathrm{A}$ that observes $n$ random elements from a  $(p,q)$ polynomial oracle, we have
 \vspace*{-0.1in}
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{A},c_1,c_2 ) &= \Omega( n^{-\frac{p}{2p+q}}),\\
\Delta_n(\F_{L,1},\mathrm{A},c_1,c_2 ) & = \Omega(  n^{-\frac{\alert{2}p}{2p+q}})\,.
\end{align*}
\vspace*{-0.2in}
\end{theorem}}

\uncover<+->{
Compare with 
\begin{align*}
\Delta_n(\F_{L,0},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{2p+q} } )\\
% \le K_1\left(\dfrac{D {C_1^{\frac{q}{p}} C_2}}{ n}\right)^{\frac{p}{2p+q}},\\
\Delta_n(\F_{L,1},\mathrm{MD},c_1,c_2 ) &
= O( n^{- \frac{p}{p+q} } )  = O(n^{-\frac{2p}{2p+\alert{2}q}})\,.
%\le K_2\left(\dfrac{C_1^{\frac{q}{p}} C_2}{ n}\right)^{\frac{p}{p+q}},
\end{align*}
(The lower bound for $\F_{L,0}$ is tight, for $\F_{L,1}$ it is weak.)
}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Lower Bound Idea}
	\bc
	\vspace*{-0.3in}
	\includegraphics[height=\textheight]{figs/lb1}
	\ec
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Conclusion}
\begin{Corollary}	
To get the optimal $O(n^{-1/2})$ rate for $\F_{L,0}$  with uncontrolled noise,
with a low-complexity algorithm,
one of the following must be done:
\begin{enumerate}
\item An oracle with $q=0$ (constant second moment bound) must be designed.
\item 
\sout{An algorithm that makes better use of the gradient estimates must be designed.}
\item Some extra properties of gradient estimates must be exploited beyond bias/variance.
\item Design a non-gradient algorithm. 
\end{enumerate}
\end{Corollary}	
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
	\frametitle{Big Picture}
	\bi
	\item Noisy Bandit Convex 	Optimization
	\bigskip
	\item Controlled noise: Pretty well understood
	\bigskip
	\item Linear case: Pretty well understood
	\bigskip
	\item Uncontrolled noise: Not much is known about low complexity, optimal algorithms!
	\bigskip
	\item Fascinating results on regret minimization in the online setting
	\bigskip
	\ei
}
\if0
\frame{
	\frametitle{}	
	\bc
	\begin{tabular}[t]{r|c|c|c|}
	 & Lower b. & Upper b. & Alg. \\
	 \hline
	 \hline
	 Convex & $\sqrt{d^2/n}$ & $\sqrt{d^2/n}$ & AAA \\
	 \hline
	 Smooth C. & $\sqrt{d^2/n}$ & $\sqrt{d^2/n}$ & AAA \\
	 \hline
	 Smooth + SOC & $\sqrt{d^2/n}$ & $\sqrt{d^2/n}$ & AAA \\
	 \hline
	\end{tabular}
	\ec
}
\fi

\if0
Convex opt 101:
Ellipsoid method
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
\begin{center}
\LARGE{Thanks! Questions?}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%\frametitle{}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks]
	\frametitle{References}
%        \begin{center}
%          Thanks!
%        \end{center}
	%\bibliographystyle{acm}
%	\begin{multicols}{2}
	\scriptsize 
	%\scriptsize\tiny
%	\bibliography{biblio,allbib,shortconfs,../thebib}
%	\bibliography{allbib,shortconfs,../thebib}
%\bibliography{allbib,shortconfs}
\bibliography{allbib,main}
%	\bibliography{../thebib}
%	\end{multicols}
\end{frame}


\end{document}
