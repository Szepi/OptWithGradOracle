\documentclass[11pt,letterpaper,english]{article}

\input{commands}


\begin{document}
\section*{Bias-Variance upper bound proof:}
paper: bandit smooth convex optimization: improving the bias-variance tradeoff

\subsection*{Conditions: } 

$f_t: \mathbb{R}^d \to \mathbb{R}$, convex, $H$-smooth.
$ 0 \leq \hat{f}_t(x_t) \leq B$, $f_t(y_t)\leq B$.

$\norm{\nabla \hat{f}_t(x_t)} \leq L$, $\norm{x_t - x^*} \leq D$, where $\norm{\cdot}$ is the Euclidean norm, $x^* = \arg \min_x \sum_{t=1}^T f_t(x)$.

\subsection*{Algorithm: }
$x_1 \in \mathbb{R}^d$ arbitrarily.\\
$y_t = x_t + \delta u_t$, $u_t$ is uniformly drawn from unit sphere.\\
$\hat{g}_t = (d/\delta) f_t(y_t) u_t$.\\
$x_{t+1} = \arg \min_x x \sum_{s=1}^t \alpha_{s,t} \hat{g}_s + R(x)$

\subsection*{Proof: }

Denote $\hat{f}_t(x) = \EE{f_t(x+\delta v)}$, then $\EE{\hat{g}_t| \mathcal{F}_t} = \nabla \hat{f}_t(x_t)$.
Denote $\bar{f}_t = \dfrac{1}{k+1} \sum_{i=0}^k \hat{f}_{t-i}$, 
$\bar{g}_t = \dfrac{1}{k+1} \sum_{i=0}^k \hat{g}_{t-i}$. 
Then, $\sum_{s=1}^t \alpha_{s,t} \hat{g}_s = \eta \sum_{s=1}^t \bar{g}_s$.

Using the algorithm, let $\eta>0$ be such that $\eta \norm{\bar{g}_t} \leq \dfrac{1}{2}$, we have that
\begin{align*}
\sum_{t=1}^T \bar{g}_t (x_t - x^*) &\leq \dfrac{1}{\eta} R(x^*) + 2\eta \sum_{t=1}^T \norm{\bar{g}_t}^2 \\ 
\norm{x_t-x_{t+1}} &\leq 2\eta \norm{\bar{g}_t}
\end{align*}

$H$-smooth $\Rightarrow$ (6a) + (6c) $\leq H\delta^2T + (H/2)\delta^2T$ 

convex, $\norm{\nabla \hat{f}_t(x_t)} \leq L$, $ 0 \leq \hat{f}_t(x_t) \leq B$   $\Rightarrow$
(8a) $\leq \dfrac{1}{k+1} \sum_{t=1}^{T-k}\sum_{i=1}^k L\EE{\norm{x_{t+i} -x_t}} + kB$

\begin{align*}
(8b) 
&\leq  \EE{\sum_{t=1}^T \nabla \bar{f}_t (x_t) (x_t-x^*)}\\
& = \EE{\sum_{t=1}^T \bar{g}_t (x_t - x^*)} + \EE{\sum_{t=1}^T (\nabla \bar{f}_t (x_t) -\bar{g}_t) (x_t-x^*)}\\
&\leq \dfrac{1}{\eta} R(x^*) + 2\eta \sum_{t=1}^T\EE{ \norm{\bar{g}_t}^2} + \dfrac{1}{k+1} \sum_{t=1}^{T}\sum_{i=0}^k \EE{ (\nabla \hat{f}_{t-i}(x_t) - \EE{\hat{g}_{t-i}}) (x_t-x^*) } \\
&\leq \dfrac{1}{\eta} R(x^*) + 2\eta \sum_{t=1}^T\EE{ \norm{\bar{g}_t}^2} + \dfrac{1}{k+1} \sum_{t=1}^{T}\sum_{i=0}^k \EE{ \norm{\nabla \hat{f}_{t-i}(x_t) - \nabla \hat{f}_{t-i}(x_{t-i})} \norm{x_t-x^*} }\\
& \leq \dfrac{1}{\eta} R(x^*) + 2\eta \sum_{t=1}^T\EE{ \norm{\bar{g}_t}^2} + \dfrac{1}{k+1} \sum_{t=1}^{T}\sum_{i=0}^k \EE{H \norm{x_t-x_{t-i}} D }
\end{align*}
The last inequality comes from smoothness and $\norm{x_t - x^*} \leq D$.

Now it remains to bound $\EE{ \norm{\bar{g}_t}^2}$ and $\EE{\norm{x_{t+1}-x_t}}$.

\begin{align*}
\norm{\bar{g}_t} &\leq \dfrac{1}{k+1} \norm{ \sum_{i=0}^k \EE{\hat{g}_{t-i}}  } + \dfrac{1}{k+1} \norm{ \sum_{i=0}^k \hat{g}_{t-i} - \EE{\hat{g}_{t-i}}  } \\
&\leq L + \dfrac{1}{k+1} \norm{ \sum_{i=0}^k \hat{g}_{t-i} - \EE{\hat{g}_{t-i}}  }
\end{align*}
So 
$\EE{\norm{\bar{g}_t}^2} \leq 2L^2+\dfrac{2}{(k+1)^2} \EE{\norm{ \sum_{i=0}^k h_{t-i}  }^2}$, where $h_s = \hat{g}_s-\EE{\hat{g}_s}$.

Since $\left\lbrace h_s \right\rbrace$ is a martingale difference sequence, 
$\EE{\norm{ \sum_{i=0}^k h_{t-i}  }^2} = \EE{\sum_{i=0}^k \norm{h_{t-i}}^2  } \leq  (k+1)\EE{\norm{\hat{g}_{t-i}}^2} $. We also have
\begin{align*}
\EE{\norm{\hat{g}_{t}}^2}  = \EE{ \norm{(d/\delta) f_t(y_t) u_t}^2 }
\leq \dfrac{d^2}{\delta^2} B^2 \,.
\end{align*}
Therefore,
\begin{align*}
\EE{\norm{\bar{g}_t}^2} \leq 2L^2+ \dfrac{2d^2B^2}{(k+1)\delta^2}\,.
\end{align*}

According to the algorithm property,
\begin{align*}
\EE{\norm{x_{t+1}-x_t}} &\leq 2\eta \EE{\norm{\bar{g}_t}}
\leq 2\eta \sqrt{\EE{\norm{\bar{g}_t}^2}}
\leq 2\eta ( \sqrt{2}L+\dfrac{\sqrt{2}dB}{\delta\sqrt{k+1}} ) \,.
\end{align*}

Combine all above bounds, we get
\begin{align*}
Regret(T) \leq& \dfrac{3}{2} H\delta^2T+ 
Bk + k(T-k)L \eta ( \sqrt{2}L+\dfrac{\sqrt{2}dB}{\delta\sqrt{k+1}} ) \\
&+ \dfrac{1}{\eta} R(x^*) + 2\eta T (2L^2+ \dfrac{2d^2B^2}{(k+1)\delta^2}) +HD kT \eta ( \sqrt{2}L+\dfrac{\sqrt{2}dB}{\delta\sqrt{k+1}} ) 
\end{align*}

\section*{For the counterexample: }
\subsection*{Conditions: }
$f: \mathcal{K}\to \mathbb{R}, f(x) = \dfrac{\epsilon}{2}(x-1)^2$, $\forall x \in \mathcal{K}, \norm{x} \leq D$, $\epsilon \leq H$.

\subsection*{Algorithm: }
Query at $y_t = x_t + \delta u_t$, $x_t \in \mathcal{K}$. \\
The oracle gives $\hat{g}_t = \epsilon (y_t - 1)-\min\left( \epsilon, C_1\delta^2  \right)+\xi_t $, where $\xi_t$ can be $\sqrt{C_2}/\delta$ or $-\sqrt{C_2}/\delta$ with equal probability. $\xi_t$ is $0$ mean, $\dfrac{C_2}{\delta^2}$ variance.

\subsection*{Proof: }
Define $\hat{f}_t(x) = \dfrac{\epsilon}{2}(x-1)^2-\min\left( \epsilon, C_1\delta^2   \right)x$, then 
\begin{align*}
\EE{\hat{g}_t | \mathcal{F}_t }= \epsilon (x_t - 1)-\min\left( \epsilon, C_1\delta^2  \right) = \nabla \hat{f}_t(x_t)
\end{align*}
Define $\bar{f}_t $, $\bar{g}_t $ same as the previous section. 

Since $f_t$ is $H$-smooth,
\begin{align*}
\EE{f_t(y_t)-f_t(x_t)} \leq \EE{ \ip{\nabla f_t(x_t), \delta u_t} +\dfrac{H}{2}\delta^2}=\dfrac{H}{2}\delta^2 \,.
\end{align*}
We also have
\begin{align*}
\EE{ \hat{f}_t(x)-f_t(x) }, \EE{ f_t(x)-\hat{f}_t(x) } = \min\left( \epsilon, C_1\delta^2   \right)x \leq D C_1\delta^2 \,.
\end{align*}
Therefore, 
\begin{align*}
(6a) + (6c) : \EE{ \sum_{t=1}^T f_t(y_t)-\hat{f}_t(x_t) } + \EE{\sum_{t=1}^T \hat{f}_t(x^*)-{f}_t(x^*)} \leq (\dfrac{H}{2}+2D C_1)\delta^2 T
\end{align*}
For $t\geq k+1$, $\hat{f}_t=\bar{f}_t$.
\begin{align*}
\sum_{t=1}^k \hat{f}_t(x)-\bar{f}_t(x) = \dfrac{1}{k+1}\sum_{t=1}^k \sum_{i=0}^k \left( \hat{f}_t(x)-\hat{f}_{t-i}(x) \right)=\dfrac{1}{k+1}\sum_{t=1}^k t \hat{f}_t(x)
\end{align*}
Denote $B = \dfrac{\epsilon}{2}(D+1)^2+D\min\left( \epsilon, C_1\delta^2   \right)$, then $\hat{f}_t(x_t) \leq B$,
\begin{align*}
(8a): \EE{\sum_{t=1}^T \hat{f}_t(x_t)-\bar{f}_t(x_t) } = \EE{\sum_{t=1}^k \hat{f}_t(x_t)-\bar{f}_t(x_t) }  \leq \dfrac{1}{2} Bk \,,
\end{align*}
\begin{align*}
(8c): \EE{\sum_{t=1}^T \bar{f}_t(x^*)-\hat{f}_t(x^*) } = \EE{\sum_{t=1}^k \bar{f}_t(x^*)-\hat{f}_t(x^*) } \leq \dfrac{1}{2} Bk \,.
\end{align*}
Given $x_t, x^* \in \mathcal{K}$, $\hat{f}_t$ is $H$-smooth,  let $\eta>0$ be such that $\eta \norm{\bar{g}_t} \leq \dfrac{1}{2}$, we have that
\begin{align*}
(8b): \EE{\sum_{t=1}^T \bar{f}_t(x_t)-\bar{f}_t(x^*) } \leq \dfrac{1}{\eta} R(x^*) + 2\eta \sum_{t=1}^T\EE{ \norm{\bar{g}_t}^2} + \dfrac{1}{k+1} \sum_{t=1}^{T}\sum_{i=0}^k \EE{H \norm{x_t-x_{t-i}} 2D }\,.
\end{align*}
Now it remains to bound $\EE{ \norm{\bar{g}_t}^2}$, $\norm{\bar{g}_t}$,   and $\EE{\norm{x_{t+1}-x_t}}$.
Given that
\begin{align*}
\hat{g}_t = \epsilon (x_t-1)-\min\left( \epsilon, C_1\delta^2  \right) + \epsilon \delta u_t +\xi_t \,,
\end{align*}
we have
\begin{align*}
\norm{\bar{g}_t} &= \dfrac{1}{k+1} \norm{ \sum_{i=0}^k \hat{g}_{t-i} }\\
&\leq \dfrac{1}{k+1}\sum_{i=0}^k \norm{\epsilon (x_{t-i}-1)} + \min\left( \epsilon, C_1\delta^2  \right) +\dfrac{1}{k+1} \norm{\sum_{i=0}^k \epsilon \delta u_{t-i}  }+\dfrac{1}{k+1} \norm{\sum_{i=0}^k \xi_{t-i}  }\\
&\leq \epsilon (D+1)+\min\left( \epsilon, C_1\delta^2  \right)+\epsilon \delta + \dfrac{\sqrt{C_2}}{\delta}
\end{align*}
Since $\left\lbrace u_t \right\rbrace$ and $\left\lbrace \xi_t \right\rbrace$ are independent random sequences, $\EE{\norm{u_t}^2}\leq 1$, $\EE{\norm{\xi_t}^2}\leq \dfrac{C_2}{\delta^2}$, we also have
\begin{align*}
\EE{ \norm{\bar{g}_t}^2 } 
&\leq \EE{ \left[ \epsilon (D+1)+\min\left( \epsilon, C_1\delta^2  \right) +\dfrac{\epsilon \delta}{k+1} \norm{\sum_{i=0}^k  u_{t-i}  }+\dfrac{1}{k+1} \norm{\sum_{i=0}^k \xi_{t-i}  } \right]^2 }\\
&\leq 4\epsilon^2 (D+1)^2+ 4\min\left( \epsilon^2, C_1^2\delta^4 \right)+4 \dfrac{\epsilon^2 \delta^2}{(k+1)^2}(k+1)+4\dfrac{1}{(k+1)^2}(k+1)\dfrac{C_2}{\delta^2}\\
&= 4\epsilon^2 (D+1)^2+ 4\min\left( \epsilon^2, C_1^2\delta^4 \right)+4 \dfrac{\epsilon^2 \delta^2}{k+1}+4\dfrac{C_2}{\delta^2(k+1)} \,,
\end{align*}
\begin{align*}
\EE{\norm{x_{t+1}-x_t}} &\leq 2\eta \EE{\norm{\bar{g}_t}}
\leq 2\eta \sqrt{\EE{\norm{\bar{g}_t}^2}}\\
&\leq 2\eta \left( 2\epsilon (D+1)+2\min\left( \epsilon, C_1\delta^2  \right)+\dfrac{2\epsilon \delta}{\sqrt{k+1}}+\dfrac{2\sqrt{C_2}}{\delta \sqrt{k+1}} \right) \,.
\end{align*}
Choose $\eta >0$ such that $\eta \left( \epsilon (D+1)+\min\left( \epsilon, C_1\delta^2  \right)+\epsilon \delta + \dfrac{\sqrt{C_2}}{\delta} \right) \leq \dfrac{1}{2}$, we get
\begin{align*}
Regret(T) \leq&  (\dfrac{H}{2}+2D C_1)\delta^2 T+k\dfrac{\epsilon}{2}(D+1)^2+kD\min\left( \epsilon, C_1\delta^2   \right)+\dfrac{1}{\eta} R(x^*) \\
&+ 2\eta T \left( 4\epsilon^2 (D+1)^2+ 4\min\left( \epsilon^2, C_1^2\delta^4 \right)+4 \dfrac{\epsilon^2 \delta^2}{k+1}+4\dfrac{C_2}{\delta^2(k+1)} \right)\\
&+ 2HD kT \eta \left( 2\epsilon (D+1)+2\min\left( \epsilon, C_1\delta^2  \right)+\dfrac{2\epsilon \delta}{\sqrt{k+1}}+\dfrac{2\sqrt{C_2}}{\delta \sqrt{k+1}} \right)\,.
\end{align*}
Choose $\delta = T^{-3/16}$, $\eta = T^{-5/8}$, $k=T^{1/8}$, 
\begin{align*}
Regret(T) \leq C_1 T^{5/8} + \epsilon T^{1/8}+\epsilon^2 T^{3/8} +C_2 T^{5/8}+\epsilon T^{1/2} +\sqrt{C_2}T^{5/8}
\end{align*}

\section*{lower bound:}
\begin{align*}
R(\delta, \epsilon) = \dfrac{\epsilon}{2} \left(1 - 2 \sqrt{n}  \dfrac{(\epsilon-C_1\delta^p)_+\delta^{q/2}}{\sqrt{C_2}}\right)
\end{align*}

The goal: for any given $\delta$, find $\epsilon$ that maximize $R$. Find $\max_{\epsilon} \min_{\delta} R(\delta, \epsilon)$. 

$p=2, q=2$.

When $\epsilon \leq C_1 \delta^2$, $R = \dfrac{\epsilon}{2}$.

When $\epsilon > C_1\delta^2$:

If $\delta < n^{-\frac{1}{6}}$, 
choose $\epsilon = \dfrac{\sqrt{C_2}}{4\delta \sqrt{n}}+\dfrac{1}{2}C_1\delta^2$, 
then
$R = \dfrac{\sqrt{C_2}}{16}n^{-\frac{1}{2}}\delta^{-1}+\dfrac{1}{8}C_1\delta^2+\dfrac{C_1^2}{4\sqrt{C_2}}n^{\frac{1}{2}}\delta^5$.
\begin{align*}
\dfrac{dR}{d\delta} = \dfrac{5C_1^2}{4\sqrt{C_2}}n^{-\frac{1}{2}}\delta^{-2}\left( n^{\frac{1}{2}}\delta^{3}-\dfrac{-1+\sqrt{6}}{10}\dfrac{\sqrt{C_2}}{C_1} \right)\left( n^{\frac{1}{2}}\delta^{3}-\dfrac{-1-\sqrt{6}}{10}\dfrac{\sqrt{C_2}}{C_1} \right)
\end{align*}

Choose $\delta = n^{-\frac{1}{6}}$, $R= n^{-\frac{1}{3}}$.

Choose $\delta = n^{-\frac{3}{16}}<n^{-\frac{1}{6}}$, $R= n^{-\frac{5}{16}}>n^{-\frac{1}{3}}$.

\end{document}
