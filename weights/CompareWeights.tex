\documentclass[11pt,letterpaper,english]{article}
%\usepackage{bbold}
\usepackage[round]{natbib}
\input{commands}

\author{Xiaowei Hu, L. A. Prashanth, Andr\'as Gy\"orgy, and Csaba Szepesv\'ari}
\title{Lower bound for averaging algorithms in bandit convex optimization}
\begin{document}
\maketitle

\section{Expression of the estimate}


We consider the problem of iterative optimization of a convex function $f:\R \to [0,\infty)$ using a gradient oracle.
In every round, the optimizer can query the gradient oracle $g_t$ at some point $x_t$, and the goal of the algorithm is to find a point $x^*_T$ after $T$ steps such that $x^*_T$ is a function of $x_1,g_t(x_1),\ldots, x_{T},g_T$ and $\EE{f(x^*_T)-f(x^*)}$ is small where $x^*$ is the minimizer of $f$, that is, $f(x^*)=\min_x f(x)$. Assume $f(x) = \dfrac{\epsilon}{2} (x-1)^2$ and 
$g_t(x) = \epsilon (x-1) + C_1 \delta^2 + \xi_t$ where $\xi_t$ are zero-mean iid random variables with variance $C_2 /\delta^2$.
Note that 
$g_t(x)-f'(x)=C_1 \delta^2 + \xi_t$, therefore, the bias of the gradient oracle is 
\[
\EE{g_t(x)-f'(x)}=C_1 \delta^2,
\] 
while the variance of the oracle is
\[
\EE{g_t(x)-f'(x)-C_1 \delta^2} = \E{\xi^2} = C_2 /\delta^2.
\]
Note that the above bias-variance bounds (with inequalities instead of equalities) are used in bandit convex optimization in papers that consider gradient methods using estimated gradients, including \citep{DekelEK15}, and they do not use anything else about the gradient estimates.

Next we consider two algorithms, SGD with gradient estimate $g_t$ and the method of \citet{DekelEK15}. With a slight abuse of notation, we will write $g_t=g_t(x_t)$, and we define $k^+=\max\{k,1\}$.
\begin{description}
\item[Algorithm 1:] $ x_{t+1} = x_t - \eta  g_t$;

\item[Algorithm 2:] $\bar{g}_t = \dfrac{1}{K+1}  \sum_{s=(t-K)^+}^t g_s$ and
$x_{t+1} = x_t - \eta  \bar{g}_t$.
\end{description}

\begin{proposition}
\label{prop1}
Assume Algorithm 1 or 2 is run to produce $x_t$ for $t=1,2,\ldots$. Then
\begin{align}
\label{eq:expression_x_t}
x_{t} = w^{(t)}_0 x_1 + 1-w^{(t)}_0 -\frac{C_1 \delta^2}{\epsilon}(1-w^{(t)}_0) -w^{(t)}_1 \xi_1 - w^{(t)}_2 \xi_2 - \cdots - w^{(t)}_{t-1} \xi_{t-1} \,,
\end{align}
for some weights $w^{(t)}_0,\ldots,w^{(t)}_{t-1}$ satisfying
$w^{(t)}_0 = 1-\epsilon (w^{(t)}_1+\cdots+w^{(t)}_{t-1})$.
\end{proposition}
\begin{proof}
Assume Algorithm~1 is used. Then
\begin{align*}
x_{t+1} =& (1-\eta \epsilon) x_{t} + \eta \epsilon - \eta C_1 \delta^2-\eta \xi_{t}\\
=&  \left(1-\eta \epsilon\right)^t x_1+ \eta \epsilon+\eta \epsilon\left(1-\eta \epsilon\right)+\cdots + \eta \epsilon\left(1-\eta \epsilon\right)^{t-1}\\
&-C_1 \delta^2\left( \eta + \eta\left( 1-\eta \epsilon \right)+\cdots + \eta \left(1-\eta \epsilon\right)^{t-1} \right) \\
&-\eta \xi_t -\eta \left(1-\eta \epsilon\right)\xi_{t-1}- \cdots -\eta \left(1-\eta \epsilon\right)^{t-1}\xi_{1}\\
=& \left(1-\eta \epsilon\right)^t x_1+ 1-\left(1-\eta \epsilon \right)^t
-C_1 \delta^2 \dfrac{1}{\epsilon}\left( 1-\left(1-\eta \epsilon \right)^t \right)\\
&- \eta \xi_t -\eta \left(1-\eta \epsilon\right)\xi_{t-1}- \cdots -\eta \left(1-\eta \epsilon\right)^{t-1}\xi_{1},
\end{align*}
showing that the proposition holds with $w^{(t+1)}_0=(1-\eta \epsilon)^t$ and $w^{(t+1)}_s= \eta (1-\eta\epsilon)^{t-s}$ for $s=1,\ldots,t$.

\medskip

To prove the proposition for Algorithm~2, we use induction.
It is obvious that \eqref{eq:expression_x_t} holds for $t=1$ with $w^{(1)}_0=1$.
Assume that \eqref{eq:expression_x_t} holds for $t=1,2,\ldots,n$, we will prove that it is also true for $t=n+1$.
Given the expression of $x_t$, we have
\begin{align*}
g_t &= \epsilon (x_t-1) + C_1 \delta^2 + \xi_t \\
&= \epsilon w^{(t)}_0 x_1 - \epsilon w^{(t)}_0 + C_1 \delta^2w^{(t)}_0 -\epsilon w^{(t)}_1 \xi_1-\cdots-\epsilon w^{(t)}_{t-1} \xi_{t-1}+\xi_t \,.
\end{align*}
Therefore, 
\begin{align*}
\bar{g}_n &= \dfrac{1}{K+1}\sum_{i=(n-K)^+}^n g_i \\
& = \frac{\epsilon x_1-\epsilon +C_1\delta^2}{K+1} \sum_{j=(n-K)^+}^n w^{(j)}_0 
% - \frac{\epsilon}{K+1} \sum_{j=(n-K)^+}^n w^{(j)}_0 +\frac{C_1 \delta^2}{K+1} \sum_{j=(n-K)^+}^n w^{(j)}_0 \\
 - \frac{\epsilon}{K+1} \sum_{i=1}^{n-1} \sum_{j=\max\left\{i+1,(n-K)^+\right\}}^n w^{(j)}_{i} \xi_{i} 
%&\quad -\epsilon \dfrac{\sum_{j=\max\left\{2,(n-K)^+\right\}}^n w^{(j)}_1}{K+1} \xi_1
%-\epsilon \dfrac{\sum_{j=\max\left\{3,(n-K)^+\right\}}^n w^{(j)}_2}{K+1} \xi_2-\cdots
%-\epsilon \dfrac{w^{(n)}_{n-1}}{K+1} \xi_{n-1}  
+\frac{1}{K+1}\sum_{i=(n-K)^+}^n \xi_i \,.
\end{align*}
Then, following the algorithm, by the induction hypothesis we have
\begin{align*}
x_{n+1} &= x_n -\eta \bar{g}_n \\
&= \left( w^{(n)}_0-\dfrac{\eta \epsilon}{K+1} \sum_{j=(n-K)^+}^n w^{(j)}_0  \right)x_1
+1- w^{(n)}_0+\dfrac{\eta \epsilon}{K+1} \sum_{j=(n-K)^+}^n w^{(j)}_0 \\
&\quad - \frac{C_1 \delta^2}{\epsilon} \left(1- w^{(n)}_0+\dfrac{\eta \epsilon}{K+1} \sum_{j=(n-K)^+}^n w^{(j)}_0    \right) \\
&\quad -  \sum_{i=1}^{n-1} \left( w^{(n)}_i- \dfrac{\eta \epsilon}{K+1} \sum_{j=\max \left\{i+1,(n-K)^+\right\}}^n w^{(j)}_i +\dfrac{\eta}{K+1} \mathbb{I}\left\lbrace i \geq n-K \right\rbrace \right) \xi_i  -\dfrac{\eta}{K+1}\xi_n.
\end{align*}
Letting
\begin{align*}
w^{(n+1)}_0 &= w^{(n)}_0-\dfrac{\eta \epsilon}{K+1} \sum_{j=(n-K)^+}^n w^{(j)}_0  \,,\\
w^{(n+1)}_i &= w^{(n)}_i- \dfrac{\eta \epsilon}{K+1} \sum_{j=\max \left\{i+1,(n-K)^+\right\}}^n w^{(j)}_i +\dfrac{\eta}{K+1} \mathbb{I}\left\lbrace i \geq n-K \right\rbrace, \quad \,i=1,2,\cdots,n-1 \,, \\
w^{(n+1)}_n &=\dfrac{\eta}{K+1} \,,
\end{align*}
we get 
\begin{align*}
x_{n+1} = w^{(n+1)}_0 x_1 + 1-w^{(n+1)}_0 -C_1 \delta^2 \frac{1}{\epsilon}(1-w^{(n+1)}_0) -w^{(n+1)}_1 \xi_1 - w^{(n+1)}_2 \xi_2 - \cdots - w^{(n+1)}_{n} \xi_{n} \,.
\end{align*}
Now we only need to prove that $w^{(n+1)}_0=1-\epsilon \sum_{i=1}^n w^{(n+1)}_i$. Given that $w^{(j)}_0=1-\epsilon \sum_{i=1}^{j-1} w^{(j)}_i$ for $j=1,2,\cdots,n$, we have
\begin{align*}
 \sum_{i=1}^n w^{(n+1)}_i
&=  \sum_{i=1}^{n-1} w^{(n)}_i +\dfrac{\eta}{K+1} \left( n+1-(n-K)^+ -\epsilon  \sum_{i=1}^{n-1} \sum_{j=\max \left\{i+1,(n-K)^+\right\}}^n w^{(j)}_i \right) \\
&= \dfrac{1}{\epsilon} \left( 1-w^{(n)}_0 \right) + \dfrac{\eta}{K+1}  \sum_{j=(n-K)^+}^n w^{(j)}_0  \\
&=  \dfrac{1}{\epsilon} \left( 1-w^{(n+1)}_0 \right) .
\end{align*}
Thereby, \eqref{eq:expression_x_t} also holds for $t=n+1$, finishing the proof.
\end{proof}



\section{Regret lower bounds}

Now we assume that the sequence of estimates $x_t$ satisfies Proposition~\ref{prop1}. Then, letting $w_i=w^{(T+1)}_i$, the final estimate $x_{T+1}$ has the form
\begin{align*}
x_{T+1} = w_0 x_1 + 1-w_0 -\frac{C_1 \delta^2 }{\epsilon}(1-w_0) -w_1 \xi_1 - w_2 \xi_2 - \cdots - w_T \xi_T
\end{align*}
where
\[w_0 = 1-\epsilon (w_1+\cdots+w_T).\]
%We should also have 
%\[w_0 \to 0 \text{ as } T \to +\infty.\]
%For Algorithm 1, $w_0 =  \left(1-\eta \epsilon\right)^T = (1-T^{-\frac{23}{24}})^T = (\frac{1}{e})^{T^{1/24}} \to 0$.
Since $\{\xi_t\}$ is independent, $\EE{\xi_t}=0$, $\EE{\xi_t^2}=\dfrac{C_2}{\delta^2}$, 
the regret is
\begin{align}
\EE{R} &=\EE{ \frac{\epsilon}{2} (x_{T+1}-1)^2} \nonumber \\
&= \EE{\frac{\epsilon}{2} \left(w_0 x_1 -w_0 -\frac{C_1 \delta^2 }{\epsilon}(1-w_0) -w_1 \xi_1 - w_2 \xi_2 - \cdots - w_T \xi_T  \right)^2} \nonumber \\
&= \frac{\epsilon}{2} \left( (x_1-1)-(\epsilon x_1 -\epsilon +C_1\delta^2)(w_1+\cdots+w_T) \right)^2 + \dfrac{\epsilon}{2} \dfrac{C_2}{\delta^2} \left(w_1^2+\cdots+w_T^2  \right) \label{eq:lb2}\\
&\geq \dfrac{\epsilon}{2} \left( (x_1-1)-(\epsilon x_1 -\epsilon +C_1\delta^2)(w_1+\cdots+w_T) \right)^2 + \dfrac{\epsilon}{2} \dfrac{C_2}{\delta^2}\dfrac{1}{T} \left(w_1+\cdots+w_T  \right)^2 \nonumber \\
&= \frac{\epsilon}{2} \Bigl( (x_1-1)^2-2(x_1-1)(\epsilon x_1-\epsilon +C_1 \delta^2)W + [(\epsilon x_1-\epsilon +C_1 \delta^2)^2+C_2 \delta^{-2}T^{-1}]W^2  \Bigr)  \nonumber
\end{align}
where we introduced the shorthand notation $W = w_1+\cdots+w_T$.
%\begin{align*}
%\EE{R} \geq  \dfrac{\epsilon}{2} \Bigl( (x_1-1)^2-2(x_1-1)(\epsilon x_1-\epsilon +C_1 \delta^2)W + [(\epsilon x_1-\epsilon +C_1 \delta^2)^2+C_2 \delta^{-2}T^{-1}]W^2  \Bigr) \,.
%\end{align*}
Using that $a W^2 + bW +c \ge c-b^2/(4a)$, we get 
\begin{align*}
\EE{R}  &\ge \frac{\epsilon (x_1-1)^2}{2} \frac{C_2}{C_2 + \delta^2 T(\epsilon x_1 - \epsilon +C_1 \delta^2)^2} \\
& \ge \frac{\epsilon (x_1-1)^2}{2} \frac{C_2}{C_2 + 2(x_1-1)^2 T \delta^2 \epsilon^2+ 2 C_1^2 T \delta^6}.
\end{align*}

Now, for Algorithm~2, the parameter choices are $\epsilon = T^{-\frac{1}{3}}$, $\eta = T^{-\frac{5}{8}}$, $K = T^{\frac{1}{8}}$, $\delta=T^{-\frac{3}{16}}$, leading to the lower bound $\EE{R} = \Omega(T^{-1/3})$, contradicting the upper bound $O(T^{-3/8})$ of \citet{DekelEK15}.


\bibliographystyle{plainnat}
\bibliography{../main}


\end{document}



--------------------------------

 
The lower bound is minimized when $W=\dfrac{(x_1-1)(\epsilon x_1-\epsilon +C_1 \delta^2)}{\left(\epsilon x_1-\epsilon +C_1 \delta^2+C_2 \delta^{-2}T^{-1}\right)^2} $. Therefore, 
\begin{align*}
\EE{R} = O(\dfrac{\epsilon}{2}) = O(T^{-\frac{1}{3}})\,.
\end{align*}

Specially, for Algorithm 1, 
\begin{align*}
w_1+\cdots+w_T&= \eta \left( 1+(1-\eta \epsilon)+\cdots+(1-\eta \epsilon)^{T-1} \right)\to \dfrac{1}{\epsilon} =T^{\frac{1}{3}}\,,\\
w_1^2+\cdots+w_T^2&=\eta^2 \left( 1+(1-\eta \epsilon)^2+\cdots+(1-\eta \epsilon)^{2(T-1)} \right) \to \dfrac{\eta^2}{1-(1-\eta \epsilon)^2}= T^{-\frac{7}{24}}\,.
\end{align*}
Therefore, $\EE{R} = O(T^{-\frac{5}{12}})+O(T^{-\frac{1}{4}})>O(T^{-\frac{1}{3}})$.

\bibliographystyle{plainnat}
\bibliography{../main}


\end{document}
